[{"body":"","categories":"","description":"Execute the following exerciss in order.","excerpt":"Execute the following exerciss in order.","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_single_host/","tags":"","title":"APM for a Single Host"},{"body":"Identify your token and realm from the Splunk Observability Cloud Portal:\nOrganization Settings-\u003eAccess Tokens and Your Name-\u003eAccount Settings\n If using your own k8s cluster on an Ubuntu host\nRemove the Otel Collector if its running on the same host as your k8s cluster:\nsudo sh /tmp/splunk-otel-collector.sh --uninstall Use this setup script to bootstrap your Debian based k8s environment with everything needed for the k8s workshop:\n bash \u003c(curl -s https://raw.githubusercontent.com/signalfx/otelworkshop/master/setup-tools/k8s-env-only.sh)  Ensure you have helm and lynx installed.\nSkip to: 2: Deploy APM for containerized apps: Python and Java\nIf you are using k8s anywhere else you can still do this workshop but will need to ensure helm, lynx are available.\n  1: Use Data Setup Wizard for Splunk Otel Collector Pod on k3s  IMPORTANT: If you have the Otel Collector and prior lab examples running on a host, them at this time:\nStop all the prior labs apps by using ctrl-c in each terminal window and then closing the window.\nRemove the host based otel collector: sudo sh /tmp/splunk-otel-collector.sh --uninstall\n 1a: Splunk Observability Cloud Portal In Splunk Observability Cloud: Data Setup-\u003eKubernetes-\u003eAdd Connection\nChoose the following:\n   Key Value     Access Token Select from list   Cluster Name Your initials-cluster i.e. SL-cluster   Provider Other   Distribution Other   Add Gateway No   Log Collection True    And then select Next\nFollow the steps on the Install Integration page.\nA result will look like this:\nNAME: splunk-otel-collector-1620505665 LAST DEPLOYED: Sat May 8 20:27:46 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None Note the name of the deployment when the install completes i.e.: splunk-otel-collector-1620505665\n1b: Update k3s For Splunk Log Observer (Ignore if you are using k8s) k3s has a different format that standard k8s for logging and we need to update our deployment for this.\nYou’ll need the Collector deployment from the Data Setup Wizard install.\nYou can also dervice this from using helm list i.e.:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION splunk-otel-collector-1620504591 default 1 2021-05-08 20:09:51.625419479 +0000 UTC deployed splunk-otel-collector-0.25.0 The deployment name would be: splunk-otel-collector-1620504591\nPrepare values for Collector update If you run into any errors from helm, fix with:\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml sudo chmod 755 /etc/rancher/k3s/k3s.yaml Prep values for collector update:\nhelm list\nhelm get values NAME\ni.e. helm get values splunk-otel-collector-1620609739\nmake note of:\nclusterNAME\nsplunkAccessToken\nsplunkRealm\nPrepare values.yaml file for updating the Helm chart Start in k8s directory:\ncd ~/otelworkshop/k8s Edit k3slogs.yaml with thes values above.\nUpdate the Collector Install the Collector configuration chart:\nhelm upgrade \\ YOURCOLLECTORHERE \\ --values k3slogs.yaml \\ splunk-otel-collector-chart/splunk-otel-collector i.e.\nhelm upgrade \\ splunk-otel-collector-1620609739 \\ --values k3slogs.yaml \\ splunk-otel-collector-chart/splunk-otel-collector  2: Deploy APM For Containerized Apps: Python and Java !!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the py-deployment.yaml below and add your initials to the environment i.e. change all instances:\ndeployment.environment=apm-workshop\nto deployment.environment=sjl-apm-workshop\nDeploy the Flask server deployment/service and the python-requests (makes requests of Flask server) pod:\ncd ~/otelworkshop/k8s kubectl apply -f py-deployment.yaml !!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the java-deployment.yaml below and add your initials to the environment i.e. change all instances:\ndeployment.environment=apm-workshop\nto deployment.environment=sjl-apm-workshop\nDeploy the Java OKHTTP requests pod (makes requests of Flask server):\nkubectl apply -f java-deployment.yaml Study the results:\nThe APM Dashboard will show the instrumented Python-Requests and Java OKHTTP clients posting to the Flask Server.\nMake sure you select the apm-workshop ENVIRONMENT to monitor.\nStudy the deployment.yaml files:\nExample in Github or:\n~/otelworkshop/k8s/py-deployment.yaml ~/otelworkshop/k8s/java-deployment.yaml The .yaml files show the environment variables telling the instrumentation to send spans to the OpenTelemetry Collector.\nNormally we use an environment variable pointing to localhost on a single host application where the Collector is running. In k8s we have separate pods in a cluster for apps and the Collector.\nThe Collector pod is running with node wide visibility, so to tell each application pod where to send spans:\n- name: SPLUNK_OTEL_AGENT valueFrom: fieldRef: fieldPath: status.hostIP - name: OTEL_EXPORTER_OTLP_ENDPOINT value: \"http://$(SPLUNK_OTEL_AGENT):4317\"  3: Monitor JVM Metrics For a Java Container JVM Metrics are emitted by the Splunk OpenTelemetry Java instrumentation and send to the Collector.\nDownload this file to your local machine: JVM Metrics Dashboard Template\nSelect the + Icon on top right and create a new Dashboard Group called test\nClick the + Icon again and select Import-\u003eDahsboard\nand select the downloaded dashboard_JVMMetrics.json file.\nFilter by Application by adding service:SERVICENAMEHERE\nComplete JVM metrics available at this link\n 4: Manually instrument a Java App And Add Custom Attributres (Tags) Let’s say you have an app that has your own functions and doesn’t only use auto-instrumented frameworks- or doesn’t have any of them!\nYou can easily manually instrument your functions and have them appear as part of a service, or as an entire service.\nExample is here:\ncd ~/otelworkshop/k8s/java/manual-inst\nDeploy an app with ONLY manual instrumentation:\n!!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the java-reqs-manual-inst.yaml below and add your initials to the environment i.e. change all instances:\ndeployment.environment=apm-workshop\nto deployment.environment=sjl-apm-workshop\nkubectl apply -f java-reqs-manual-inst.yaml When this app deploys, it appears as an isolated bubble in the map. It has all metrics and tracing just like an auto-instrumented app does.\nTake a look at the traces and their spans to see the manually added values of Message, Logs etc.\nYou will see the function called ExampleSpan with custom Logging messages and a message:myevent span/tag.\nSee the custom attribute my.key and value myvalue.\nThis could be a transaction ID, user ID, or any custom value that you want to correlate and even metricize.\nStudy the manual instrumentation code example here.\nThere are two methods shown- the decorator @WithSpan method (easiest), and using the GlobalTracer method (more complicated/powerful)…\nNote that this is the most minimal example of manual instrumentation- there is a vast amount of power available in OpenTelemetry- please see the documentation and in depth details\n 5: Process Spans with the Otel Collector The Otel Collector has many powerful configuration options ranging from splitting telemetry to multiple destinations to sampling to span processing.\nProcessor documentation\nCollector config examples\nFull documentation\nPrepare values for Collector update helm list helm get values NAME i.e. helm get values splunk-otel-collector-1620609739\nmake note of:\nclusterNAME\nsplunkAccessToken\nsplunkRealm\nSpan Processing Example: Redacting Data from a Span Attribute Change to the example directory:\ncd ~/otelworkshop/k8s/collectorconfig Prepare values.yaml file for updating the Helm chart Edit spanprocessor.yaml with thes values from Step 1.\nUpdate the Collector Install the Collector configuration chart:\nhelm upgrade --install \\ YOURCOLLECTORHERE \\ --values spanprocessor.yaml \\ splunk-otel-collector-chart/splunk-otel-collector i.e.\nhelm upgrade --install \\ splunk-otel-collector-1620609739 \\ --values spanprocessor.yaml \\ splunk-otel-collector-chart/splunk-otel-collector Study the results:\nSplunk Observability Portal -\u003e APM -\u003e Explore -\u003e java-otel-manual-inst -\u003e Traces\nExample my.key and you’ll see that the value is redacted after applying the spanprocessor.yaml example\nIf you want to make changes and update the spanprocessor.yaml or add more configurations, use:\nhelm upgrade --reuse-values\n 6: Receive Prometheus Metrics at the Otel Collector Add a Prometheus endpoint pod Change to the k8s Collector Config directory:\ncd ~/otelworkshop/k8s/collectorconfig Add the Prometheus pod (source code is in the k8s/python directory):\nkubectl apply -f prometheus-deployment.yaml Update Otel Collector to Scrape the Prometheus Pod Update realm/token/cluster in the otel-prometheus.yaml\nVerify your helm deployment of the collector:\nhelm list Upgrade the Collector deployment with the values required for scraping Prometheus metrics from the Prometheus pod deployed in the previous step:\nhelm upgrade --reuse-values splunk-otel-collector-YOURCOLLECTORVALUE --values otel-prometheus.yaml splunk-otel-collector-chart/splunk-otel-collector Find Prometheus Metric and Generate Chart Splunk Observabilty -\u003e Menu -\u003e Metrics -\u003e Metric Finder\nSearch for: customgauge\nClick CustomGauge\nChart appears with value 17\nExamine the collector update otel-prometheus.yaml to see how this works.\n 7: Configure Otel Collector to Transform a Metric Name This example uses the Metrics Transform Processor\nChange to the k8s Collector Config directory:\ncd ~/otelworkshop/k8s/collectorconfig Update realm/token/cluster in the metricstransform.yaml with your token/realm/cluster\nUpgrade the Collector deployment with the values required for scraping Prometheus metrics from the Prometheus pod deployed in the previous step:\nhelm upgrade --reuse-values splunk-otel-collector-YOURCOLLECTORVALUE --values metricstransform.yaml splunk-otel-collector-chart/splunk-otel-collector Find Transformed Prometheus Metric and Generate Chart Splunk Observabilty -\u003e Menu -\u003e Metrics -\u003e Metric Finder\nSearch for: transformedgauge\nClick TransformedGauge\nYou’ll now see the new chart for the metric formerly known as CustomGauge that has been transformed using the metrics transform processor.\nExamine the collector update metricstransform.yaml to see how this works.\n Monitoring and Troubleshooting View Otel Collector POD stats kubectl get pods Note the pod name of the OpenTelemetry Collector pod i.e.:\nsplunk-otel-collector-1620505665-agent-sw45w\nSend the Zpages stats to the lynx browser:\nkubectl exec -it YOURAGENTPODHERE -- curl localhost:55679/debug/tracez | lynx -stdin i.e.\nkubectl exec -it splunk-otel-collector-1620505665-agent-sw45w -- curl localhost:55679/debug/tracez | lynx -stdin Examine Otel Collector Config get your Collector agent pod name via:\nkubectl get pods i.e.\nsplunk-otel-collector-1626453714-agent-vfr7s\nShow current Collector config:\nkubectl exec -it YOURAGENTPODHERE -- curl localhost:55554/debug/configz/effective Show initial Collector config:\nkubectl exec -it YOURAGENTPODHERE -- curl localhost:55554/debug/configz/initial  Bonus Instrumentation Examples: Istio and .NET .NET: containerized example is located here Istio: service mesh lab here  Clean up deployments and services To delete all k8s lab work:\nin ~/otelworkshop/k8s/\nsource delete-all-k8s.sh source delete-prometheus.sh To delete the Collector from k8s:\nhelm list helm delete YOURCOLLECTORHERE i.e.\nhelm delete splunk-otel-collector-1620505665 k3s:\n/usr/local/bin/k3s-uninstall.sh ","categories":"","description":"","excerpt":"Identify your token and realm from the Splunk Observability Cloud …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_k8s/k8s/","tags":"","title":"APM for K8s"},{"body":"We deeply believe that the best way for you to familiarize yourself with the Splunk IT Service Intelligence (ITSI) Add-On is to get your hands dirty. Therefore, we provide individual sandbox environments in the form of Splunk instances for you. The first task of this workshop for you is to connect to those instances.\nA successful connection to your instance can be established via executing the following steps:\nAccess the Instance List by clicking HERE. You should be able to see a Google spreadsheet that looks similar to the screenshotted example below:\nIn the first column with the title Name of Attendee locate your name. Find your personal access link to the instance on the right of your name and use it to reach the login page of Splunk Enterprise. It looks like this:\nTo log in, use the username admin. Use the password is provided for you in the Instance List. Click the Sign In-Button.\nOn a successful login, you might get greeted by pop-up windows showing tips, tutorials, and/or recommendations. These are not important for us right now. Feel free to ignore them by clicking the Got it!-Button, or respectively, the Don’t show me this again-Button. Other than that, you should be able now to see Splunk Enterprise Home view, which initially looks like this:\nIf you fail to see this home view, most likely something went wrong. Please do not hesitate to raise your hand in Zoom, or shoot us a short message in the Zoom channel. An assistent will be with you shortly.\nIf that is not the case, we want to congratulate you! You successfully connected to your instance, and thus completed the first task!\n","categories":"","description":"","excerpt":"We deeply believe that the best way for you to familiarize yourself …","ref":"/observability-workshop/v4.22/itsi/docs/connect/","tags":"","title":"Connect to ITSI instance"},{"body":" Use the Splunk Helm chart to install the OpenTelemetry Collector in K3s Explore your cluster in the Kubernetes Navigator   1. Obtain Access Token You will need to obtain your Access Token1 from the Splunk UI. You can find the workshop Access Token by clicking » bottom left and then selecting Settings → Access Tokens.\nExpand the workshop token that your host has instructed you to use e.g. O11y-Workshop-ACCESS, then click on Show Token to expose your token. Click the Copy    button to copy to clipboard. Please do not use the Default token!\nPlease do not attempt to create your own token\nWe have created a Token specifically for this workshop with the appropriate settings for the exercises you will be performing so have allocated it both Ingest and API Permissions. Best practice in production is to only allocate a single permission to a Token such as Ingest OR API OR RUM and use multiple Tokens where required.   You will also need to obtain the name of the Realm2 for your Splunk account. At the top of the side menu, click on your name and select Account Settings. The Realm can be found in the middle of the page within the Organizations section. In this example it is us0.\n2. Installation using Helm Create the ACCESS_TOKEN and REALM environment variables to use in the proceeding Helm install command. For instance, if your realm is us1, you would type export REALM=us1 and for eu0 type export REALM=eu0.\nExport Variables   export ACCESS_TOKEN=\u003creplace_with_O11y-Workshop-ACCESS_token\u003e export REALM=\u003creplace_with_splunk_realm\u003e  Install the OpenTelemetry Collector using the Splunk Helm chart. First, add the Splunk Helm chart repository to Helm and update. Helm Repo Add  Helm Repo Add Output   helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 \"splunk-otel-collector-chart\" has been added to your repositories Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. ⎈Happy Helming!⎈  Install the OpenTelemetry Collector Helm chart with the following commands, do NOT edit this:\nHelm Install  Helm Install Output   helm install splunk-otel-collector \\ --set=\"splunkObservability.realm=$REALM\" \\ --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=$(hostname)-k3s-cluster\" \\ --set=\"splunkObservability.logsEnabled=true\" \\ --set=\"environment=$(hostname)-apm-env\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 NAME: splunk-otel-collector LAST DEPLOYED: Fri May 7 11:19:01 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None  You can monitor the progress of the deployment by running kubectl get pods which should typically report a new pod is up and running after about 30 seconds.\nEnsure the status is reported as Running before continuing.\nKubectl Get Pods  Kubectl Get Pods Output   kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-2sk6k 0/1 Running 0 10s splunk-otel-collector-k8s-cluster-receiver-6956d4446f-gwnd7 0/1 Running 0 10s  Ensure there are no errors by tailing the logs from the OpenTelemetry Collector pod. Output should look similar to the log output shown in the Output tab below.\nUse the label set by the helm install to tail logs (You will need to press ctrl+c to exit). Or use the installed k9s terminal UI for bonus points!\nKubectl Logs  Kubectl Logs Output   kubectl logs -l app=splunk-otel-collector -f --container otel-collector 2021-03-21T16:11:10.900Z INFO service/service.go:364 Starting receivers... 2021-03-21T16:11:10.900Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/watcher.go:195 Configured Kubernetes MetadataExporter {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\", \"exporter_name\": \"signalfx\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO healthcheck/handler.go:128 Health Check state change {\"component_kind\": \"extension\", \"component_type\": \"health_check\", \"component_name\": \"health_check\", \"status\": \"ready\"} 2021-03-21T16:11:11.009Z INFO service/service.go:267 Everything is ready. Begin running and processing data. 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/receiver.go:59 Starting shared informers and wait for initial cache sync. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.281Z INFO k8sclusterreceiver@v0.21.0/receiver.go:75 Completed syncing shared informer caches. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"}  Deleting a failed installation\nIf you make an error installing the OpenTelemetry Collector you can start over by deleting the installation using: helm delete splunk-otel-collector    3. Validate metrics in the UI In the Splunk UI, click the » bottom left and click on Infrastructure.\nUnder Containers click on Kubernetes to open the Kubernetes Navigator Cluster Map to ensure metrics are being sent in.\nValidate that your cluster is discovered and reporting by finding your cluster (in the workshop you will see many other clusters). To find your cluster name run the following command and copy the output to your clipboard:\nEcho Cluster Name   echo $(hostname)-k3s-cluster  Then in the UI, click on the “Cluster: - \" menu just below the Splunk Logo, and paste the Cluster name you just copied into the search box, click the box to select your cluster, and finally click off the menu into white space to apply the filter.\nTo examine the health of your node, hover over the pale blue background of your cluster, then click on the blue magnifying glass that appears in the top left hand corner.\nThis will drill down to the node level. Next, open the side bar by clicking on the side bar button to open the Metrics side bar.\nOnce it is open, you can use the slider on the side to explore the various charts relevant to your cluster/node: CPU, Memory, Network, Events etc.\n  Access Tokens (sometimes called Org Tokens) are long-lived organization-level tokens. By default, these tokens persist for 5 years, and thus are suitable for embedding into emitters that send data points over long periods of time, or for any long-running scripts that call the Splunk API. ↩︎\n A realm is a self-contained deployment of Splunk in which your Organization is hosted. Different realms have different API endpoints (e.g. the endpoint for sending data is ingest.us1.signalfx.com for the us1 realm, and ingest.eu0.signalfx.com for the eu0 realm). This realm name is shown on your profile page in the Splunk UI. If you do not include the realm name when specifying an endpoint, Splunk will interpret it as pointing to the us0 realm. ↩︎\n   ","categories":["IMT"],"description":"","excerpt":" Use the Splunk Helm chart to install the OpenTelemetry Collector in …","ref":"/observability-workshop/v4.22/imt/docs/gdi/k3s/","tags":["k3s"],"title":"Deploying the OpenTelemetry Collector in Kubernetes"},{"body":"A collection of the common questions and their answers associated with Observability, DevOps, Incident Response and Splunk On-Call.\nQ: Alerts v. Incident Response v. Incident Management A: Alerts, Incident Response and Incident Management are related functions. Together they comprise the incident response and resolution process.\nMonitoring and Observability tools send alerts to incident response platforms. Those platforms take a collection of alerts and correlate them into incidents.\nThose incidents are recorded into incident management (ITSM) platforms for record. Alerts are the trigger that something has happened, and provide context to an incident.\nIncidents consist of the alert payload, all activity associated with the incident from the time it was created, and the on-call policies to be followed. ITSM is the system of record for incidents that are active and after they have been resolved.\nAll these components are necessary for successful incident response and management practices.\nOn-Call    Q: Is Observability Monitoring A: The key difference between Monitoring and Observability is the difference between “known knowns” and “unknown knowns” respectively.\nIn monitoring the operator generally has prior knowledge of the architecture and elements in their system. They can reliably predict the relationship between elements, and their associated metadata. Monitoring is good for stateful infrastructure that is not frequently changed.\nObservability is for systems where the operators ability to predict and trace all elements in the system and their relationships is limited.\nObservability is a set of practices and technology, which include traditional monitoring metrics.\nThese practices and technologies combined give the operator the ability to understand ephemeral and highly complex environments without prior knowledge of all elements of a system. Observability technology can also account for fluctuations in the environment, and variation in metadata (cardinality) better than traditional monitoring which is more static.\nObservability    Q: What are Traces and Spans A: Traces and spans, combined with metrics and logs, make up the core types of data that feed modern Observability tools. They all have specific elements and functions, but work well together.\nBecause microservices based architectures are distributed, transactions in the system touch multiple services before completing. This makes accurately pinpointing the location of an issue difficult. Traces are a method for tracking the full path of a request through all the services in a distributed system. Spans are the timed operations in each service. Traces are the connective tissue for the spans and together they give more detail on individual service processes. While metrics give a good snapshot of the health of a system, and logs give depth when investigating issues, traces and spans help navigate operators to the source of issues with greater context. This saves time when investigating incidents, and supports the increasing complexity of modern architectures.\nAPM    Q: What is the Sidecar Pattern? A: The sidecar pattern is a design pattern for having related services contected directly by infrastructure. Related services can be adding functionality or supporting the application logic they are connected to. It is used heavily as a method for deploying agents associated with the management plan along with the application service they support.\nIn Observability the sidecar services are the application logic, and the agent collecting data from that service. The setup requires two containers one with the application service, and one running the agent. The containers share a pod, and resources such as disk, network, and namespace. They are also deployed together and share the same lifecycle.\nObservability    ","categories":"","description":"","excerpt":"A collection of the common questions and their answers associated with …","ref":"/observability-workshop/v4.22/resources/faq/","tags":"","title":"Frequently Asked Questions"},{"body":"The goal is to walk through the basic steps to configure the following components of the Splunk Observability platform:\n Splunk Infrastructure Monitoring (IM) Splunk Application Performance Monitoring (APM)  Database Query Performance AlwaysOn Profiling   Splunk Real User Monitoring (RUM) Splunk LogsObserver (LO)  We will also show the steps about how to clone (download) a sample Java application (Spring PetClinic), as well as how to compile, package and run the application.\nOnce the application is up and running, we will instrument the application using Splunk OpenTelemetry Java Instrumentation libraries that will generate traces and metrics used by the Splunk APM product.\nAfter that, we will instrument the PetClinic’s end user interface (HTML pages rendered by the application) with the Splunk OpenTelemetry Javascript Libraries (RUM) that will generate RUM traces about all the individual clicks and page loads executed by the end user.\nLastly, we will configure the Spring PetClinic application to write application logs to the filesystem and also configure the Splunk OpenTelemetry Collector to read (tail) the logs and report to Splunk Observability Cloud.\n","categories":"","description":"","excerpt":"The goal is to walk through the basic steps to configure the following …","ref":"/observability-workshop/v4.22/pet-clinic/docs/intro/","tags":"","title":"Getting Started"},{"body":"Ubuntu Virtual Machine Multipass deploys and runs Ubuntu virtual machines easily on Mac and Windows.\nMake sure multipass is the CURRENT version:\nbrew upgrade multipass Workshop examples have been tested on this configuration:\nmultipass launch -n primary -d 16G -m 6G Make sure to always run sudo apt-get -y update before executing any step in the workshop.\nK8s Cluster Setup Hints K3s is a lightweight Kubernetes deployment from Rancher: https://k3s.io/\nTo K3s install on Linux:\ncurl -sfL https://get.k3s.io | sh - Make a .kube directory in your home directory and create the config file e.g.:\nmkdir /home/ubuntu/.kube \u0026\u0026 kubectl config view --raw \u003e /home/ubuntu/.kube/config Set the correct permissions and owndership on the newly created config file e.g.:\nchmod 400 /home/ubuntu/.kube/config chown -R ubuntu:ubuntu /home/ubuntu The stock configuration of K3s and this workshop’s K8s examples have been tested on the following configurations:\n K3s cluster (default settings) on Ubuntu VM:  Macbook Pro 32GB RAM, Windows 10 Laptop 12GB RAM: Multipass started with multipass launch -n primary -d 8G -m 4G AWS EC2    The Kubernetes lab has also been tested on:\n Azure: Kubernetes 1.17.9 Azure Kubernetes Service EKS / GKE: Not tested yet but 99.9% chance will have no issues Workshop will NOT work on: Macbook Pro Docker Desktop Kubernetes  To tear down a Multipass VM:\nmultipass delete primary --purge Using tmux tmux is recommended to split your terminal into several panes so that you can run an application in each pane without having to containerize applications- and you can keep a separate pane open for checking status of spans, the host, etc.\nImportant: each pane runs as its own bash shell so environment variables must be set in each pane. The workshop includes setup shell scripts to make it easy to do this.\nTo install tmux:\nsudo apt-get install tmux Tmux works by using ++ctrl+b++ as a command key followed by: \" make a new horizontal pane % make a new vertical pane Arrow keys: move between panes.\n","categories":"","description":"","excerpt":"Ubuntu Virtual Machine Multipass deploys and runs Ubuntu virtual …","ref":"/observability-workshop/v4.22/otelw/appendix/appendix/","tags":"","title":"Helpful Tips"},{"body":"Splunk Observability -\u003e Data Setup -\u003e Linux\nChoose the following:\n   Key Value     Access Token Select from list   Mode Agent   Log Collection No    Follow Data Setup Wizard for instructions on Linux installation:\nCheck status of collector:\nsudo systemctl status splunk-otel-collector Should output something like:\n● splunk-otel-collector.service - Splunk OpenTelemetry Collector Loaded: loaded (/lib/systemd/system/splunk-otel-collector.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/splunk-otel-collector.service.d └─service-owner.conf Active: active (running) since Sun 2021-10-31 13:07:27 UTC; 1min 11s ago Main PID: 37949 (otelcol) Tasks: 9 (limit: 19200) Memory: 100.2M CGroup: /system.slice/splunk-otel-collector.service └─37949 /usr/bin/otelcol Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.556Z info builder/receivers_builder.go:73 \u003e Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.556Z info builder/receivers_builder.go:68 \u003e Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.556Z info builder/receivers_builder.go:73 \u003e Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.556Z info healthcheck/handler.go:129 \u003e Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.556Z info service/telemetry.go:92 Se\u003e Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.557Z info service/telemetry.go:116 S\u003e Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.557Z info service/collector.go:230 S\u003e Oct 31 13:07:27 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:27.557Z info service/collector.go:132 E\u003e Oct 31 13:07:37 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:37.826Z info hostmetadata/metadata.go:75 \u003e Oct 31 13:07:37 ip-172-31-70-180 otelcol[37949]: 2021-10-31T13:07:37.826Z info hostmetadata/metadata.go:83 \u003e Your machine will be visible in Splunk Observability in Infrastructure either in the public cloud platform you are using or My Data Center if you are using Multipass or any other non public cloud machine.\n","categories":"","description":"","excerpt":"Splunk Observability -\u003e Data Setup -\u003e Linux\nChoose the following: …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_single_host/host/","tags":"","title":"Install Otel Collector On Host"},{"body":" Introduction to the Dashboards and charts Editing and creating charts Filtering and analytical functions Using formulas Saving charts in a dashboard Introduction to SignalFlow   1. Dashboards Dashboards are groupings of charts and visualizations of metrics. Well-designed dashboards can provide useful and actionable insight into your system at a glance. Dashboards can be complex or contain just a few charts that drill down only into the data you want to see.\nDuring this module we are going to create the following charts and dashboard and connect it to your Team page.\n 2. Your Teams' Page Click on the from the navbar. As you have already been assigned to a team, you will land on the team dashboard.\nWe use the Example Team as an example here. The one in your workshop will be different!\nThis page shows the total number of team members, how many active alerts for your team and all dashboards that are assigned to your team. Right now they are no dashboards assigned but as stated before, we will add the new dashboard that you will create to your Teams page later.\n 3. Sample Charts To continue, click on All Dashboards on the top right corner of the screen. This brings you to the view that shows all the available dashboards, including the pre-built ones.\nIf you are already receiving metrics from a Cloud API integration or another service through the Splunk Agent you will see relevant dashboards for these services.\n 4. Inspecting the Sample Data Among the dashboards you will see a Dashboard group called Sample Data. Expand the Sample Data dashboard group by clicking on it, and then click on the Sample Charts dashboard.\nIn the Sample Charts dashboard you can see a selection of charts that show a sample of the various styles, colors and formats you can apply to your charts in the dashboards.\nHave a look through all the dashboards in this dashboard group (PART 1, PART 2, PART 3 and INTRO TO SPLUNK OBSERVABILITY CLOUD)\n","categories":"","description":"","excerpt":" Introduction to the Dashboards and charts Editing and creating charts …","ref":"/observability-workshop/v4.22/imt/docs/dashboards/intro/","tags":"","title":"Working with Dashboards, Charts and Metrics"},{"body":"The goal of this workshop is to focus on the art of doing observability with Splunk Observability Cloud.\nQuestions to think about  We learn about sending metrics, traces and logs, but do we know why? We frequently start with logs. How can we do better? When something goes wrong, and we only have logs to rely on (to be alerted on and to get to the bottom of the problem), is there a better way?  To get the full experience it’s really important that you go through this workshop in order. While the systems will change (based on what we learn), the discussions are what’s important. Conclusions you make may even influence a future version of this workshop!\nThings to keep in mind  We are presenting this in the context of Splunk, and we will show you how to achieve that in Splunk, but as you think about the questions you can think about them in the context of capabilities, not products. The answers provided are just suggested answers. There is no single answer. One of your answers may be added to a future version of the workshop!  Draft Outline  Event 1: Starve Kubernetes of resources  Should be able to solve this issue in Kubernetes Navigator Resolve that change   Event 2: Cause disk space issue  Should be able to solve on an infrastructure dashboard Add a detector to find this proactively   Event 3: Starve Kafka of resources  Solve in logs? Discuss what we could do better Implement Kafka collection Show Kafka dashboard Create detector on Kafka   Event 4: Slow down a database query  Scenario is that a customer calls in to complain about slowness Discuss that we have nothing looking at the application Implement Java collection Show how the database analyzer can surface this issue   Event 5: Release a new version of “currency converter”  See slow downs in the map How would we know it was the new version? Introduce version to the instrumentation Introduce version event to dashboards How would we know it is specific to just one of the currencies? Introduce span tag of currency from the conversion    Application  5 microservices 1 or 2 languages OTS components (Kafka and DB)  Go to Initial State\n","categories":"","description":"","excerpt":"The goal of this workshop is to focus on the art of doing …","ref":"/observability-workshop/v4.22/realworld/docs/intro/","tags":"","title":"Introduction"},{"body":"CAVEAT: THIS LAB IS DESIGNED FOR THE UBUNTU SANDBOX CREATED AT THE START OF THE APM WORKSHOP AND IS TESTED IN THAT ENVIRONMENT ONLY THIS LAB IS A WORK IN PROCESS AND YOUR RESULTS MAY VARY\nThis exercise will install an Istio service mesh on a Kubernetes cluster that directs external requests to a Python Flask server. Both the service mesh and the Flask server will emit spans. The result will show tracing of the external request to the node and through the mesh to the Flask server.\n1: Install OpenTelemetry Collector If you have an existing collector running remove it.\nFollow Data Setup wizard but add:\n--set autodetect.istio=true` i.e.\nhelm install \\ --set splunkAccessToken='YOURTOKENHERE' \\ --set clusterName='YOURCLUSTERNAMEHERE' \\ --set splunkRealm='YOURREALMHERE' \\ --set autodetect.istio=true \\ --set provider=' ' \\ --set distro=' ' \\ --set otelCollector.enabled='false' \\ --generate-name \\ splunk-otel-collector-chart/splunk-otel-collector 2: Set Up Istio Download Istio:\ncd ~ curl -L https://istio.io/downloadIstio | sh - Follow instructions from the installer script that are now in your terminal to add Istio’s bin path to your env then:\nistioctl install 3: Deploy Istio configurations and example Flask microservice Enable automatic Istio proxy injection. More info here\nkubectl label namespace default istio-injection=enabled Change to the APM Workshop Istio directory:\ncd ~/otelworkshop/k8s/istio Install the Splunk tracing profile for Istio:\nistioctl install -f tracing.yaml Set and validate ingress ports for Nodeport example and configure ingress host for local k3s workshop example:\nsource setup-envs.sh You should see a result that looks like:\nTCP_INGRESS_PORT= INGRESS_PORT=30785 INGRESS_HOST=172.31.19.248 SECURE_INGRESS_PORT=32071 Deploy Flask service configured for Istio:\n!!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the flask-deployment-istio.yaml below and add your initials to the environment i.e. change all instances:\ndeployment.environment=apm-workshop\nto deployment.environment=sjl-apm-workshop\nkubectl apply -f flask-deployment-istio.yaml Single test Flask service:\nsource test-flask.sh\nResults should show a direct request to the Flask server:\nYou getted: b'' Request headers: Host: localhost:30001 User-Agent: curl/7.68.0 Accept: */* Server: 1 Single test Istio:\nsource test-istio.sh When hitting the service mesh from outside the cluster, you’ll receive the mesh diagnostic data plus the B3 Trace/Span ID:\nYou getted: b'' Request headers: Host: 172.31.19.248:31177 User-Agent: curl/7.68.0 Accept: */* Server: 1 X-Forwarded-For: 10.42.0.1 X-Forwarded-Proto: http X-Envoy-Internal: true X-Request-Id: 447af547-7b8f-96db-a0b5-08efce526a8d X-Envoy-Decorator-Operation: server-flask-otel-k8s.default.svc.cluster.local:5000/echo X-Envoy-Peer-Metadata: ChQKDkFQUF9DT05UQUlORVJTEgIaAAoaCgpDTFVTVEVSX0lEEgwaCkt... 3NnYXRld2F5 X-Envoy-Peer-Metadata-Id: router~10.42.0.11~istio-ingressgateway-7d97f78f5-dg5zc.istio-system~istio-system.svc.cluster.local X-Envoy-Attempt-Count: 1 X-B3-Traceid: 5035304e854aa834e990df295b1d98e9 X-B3-Spanid: e990df295b1d98e9 X-B3-Sampled: 1 To generate many requests so that the example appears in the APM service map, use the load generator:\nLoad gen Istio for two minutes:\nsource loadgen.sh You will see a service map with the Istio mesh and the Flask server:\nTraces will show the request hitting the service mesh and the mesh hitting the service itself:\nIstio2\nStop loadgen:\n++ctrl+c++\nCleanup:\nremove k8s examples:\nsource delete-all.sh Remove Istio:\nFrom the Istio bin directory:\nistioctl x uninstall --purge ","categories":"","description":"","excerpt":"CAVEAT: THIS LAB IS DESIGNED FOR THE UBUNTU SANDBOX CREATED AT THE …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_k8s/examples/istio/","tags":"","title":"Istio Setup"},{"body":"","categories":"","description":"**10 minutes**\n\nDeploy the instrumented Online Boutique microservice application into Kubernetes\n","excerpt":"**10 minutes**\n\nDeploy the instrumented Online Boutique microservice …","ref":"/observability-workshop/v4.22/apm/docs/online-boutique/","tags":"","title":"Online Boutique K8s Workshop"},{"body":"","categories":"","description":"**10 分**\n\n計装済みのOnline Boutiqueマイクロサービス・アプリケーションをKubernetesにデプロイします。\n","excerpt":"**10 分**\n\n計装済みのOnline Boutiqueマイクロサービス・アプリケーションをKubernetesにデプロイします。\n","ref":"/observability-workshop/v4.22/ja/apm/docs/online-boutique/","tags":"","title":"Online Boutique K8s Workshop"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/otelw/","tags":"","title":"OpenTelemetry"},{"body":" Splunk Helm chartを使用して、K3s に OpenTelemetry Collector をインストールします Kubernetes Navigatorでクラスタを探索します   1. Access Tokenの取得 Kubernetes が起動したら、Splunk の UI から Access Token1 を取得する必要があります。Access Token は、左下にある » を開き、 Settings → Access Tokens を選択すると表示されます。\n主催者が指示したワークショップトークン（例： O11y-Workshop-ACCESS 等）を開き、 Show Token をクリックしてトークンを公開します。 Copy    ボタンをクリックし、クリップボードにコピーしてください。 Default のトークンは使用しないでください。\n独自のトークを新たに作成しないようにしてください\nワークショップ環境の削除の際に、すこし手間が必要になってしまいます。 もし作成する場合は、スコープのチェックで INGEST のみが有効になっていることを確認してください!!   また、Splunk アカウントの Realm2 の名前を取得する必要があります。 サイドメニューの最上部の名前をクリックし、Account Settings を選択します。Realm はページの中央にある Organizations セクションにあります。 この例では「us0」となっています。\n2. Helmによるインストール 環境変数 ACCESS_TOKEN と REALM を作成して、進行中の Helm のインストールコマンドで使用します。例えば、Realm が us1 の場合は、export REALM=us1 と入力し、eu0 の場合は、export REALM=eu0 と入力します。\nExport Variables   export ACCESS_TOKEN=\u003creplace_with_O11y-Workshop-ACCESS_token\u003e export REALM=\u003creplace_with_splunk_realm\u003e  Splunk Helm チャートを使って OpenTelemetry Collector をインストールします。まず、Splunk Helm chart のリポジトリを Helm に追加してアップデートします。\nHelm Repo Add  Helm Repo Add Output   helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 \"splunk-otel-collector-chart\" has been added to your repositories Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"splunk-otel-collector-chart\" chart repository Update Complete. ⎈Happy Helming!⎈  以下のコマンドでOpenTelemetry Collector Helmチャートをインストールします。これは 変更しないでください。\nHelm Install  Helm Install Output   helm install splunk-otel-collector \\  --set=\"splunkObservability.realm=$REALM\" \\  --set=\"splunkObservability.accessToken=$ACCESS_TOKEN\" \\  --set=\"clusterName=$(hostname)-k3s-cluster\" \\  --set=\"splunkObservability.logsEnabled=true\" \\  --set=\"environment=$(hostname)-apm-env\" \\  splunk-otel-collector-chart/splunk-otel-collector \\  -f ~/workshop/k3s/otel-collector.yaml Using ACCESS_TOKEN={REDACTED} Using REALM=eu0 NAME: splunk-otel-collector LAST DEPLOYED: Fri May 7 11:19:01 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None  kubectl get pods を実行すると、約30秒程度待つと新しいポッドが稼働していることが報告され、デプロイメントの進捗を監視することができます。\n続行する前に、ステータスがRunningと報告されていることを確認してください。\nKubectl Get Pods  Kubectl Get Pods Output   kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-agent-2sk6k 0/1 Running 0 10s splunk-otel-collector-k8s-cluster-receiver-6956d4446f-gwnd7 0/1 Running 0 10s  OpenTelemetry Collector podのログを確認して、エラーがないことを確認します。出力は、以下の出力例にあるログに似ているはずです。\nログを確認するには、helm のインストールで設定したラベルを使用してください（終了するには ctrl+c を押します）。もしくは、インストールされている k9s ターミナル UI を使うとボーナスポイントがもらえます！\nKubectl Logs  Kubectl Logs Output   kubectl logs -l app=splunk-otel-collector -f --container otel-collector 2021-03-21T16:11:10.900Z INFO service/service.go:364 Starting receivers... 2021-03-21T16:11:10.900Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"prometheus\", \"component_name\": \"prometheus\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:70 Receiver is starting... {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/watcher.go:195 Configured Kubernetes MetadataExporter {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\", \"exporter_name\": \"signalfx\"} 2021-03-21T16:11:11.009Z INFO builder/receivers_builder.go:75 Receiver started. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.009Z INFO healthcheck/handler.go:128 Health Check state change {\"component_kind\": \"extension\", \"component_type\": \"health_check\", \"component_name\": \"health_check\", \"status\": \"ready\"} 2021-03-21T16:11:11.009Z INFO service/service.go:267 Everything is ready. Begin running and processing data. 2021-03-21T16:11:11.009Z INFO k8sclusterreceiver@v0.21.0/receiver.go:59 Starting shared informers and wait for initial cache sync. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"} 2021-03-21T16:11:11.281Z INFO k8sclusterreceiver@v0.21.0/receiver.go:75 Completed syncing shared informer caches. {\"component_kind\": \"receiver\", \"component_type\": \"k8s_cluster\", \"component_name\": \"k8s_cluster\"}  インストールに失敗した場合に削除する\nOpenTelemetry Collectorのインストールに失敗した場合は、次のようにしてインストールを削除することで、最初からやり直すことができます。 helm delete splunk-otel-collector    3. UI でメトリクスを確認する Splunk の UI で左下の » を開いて Infrastructure をクリックします。\nContainers の下にある Kubernetes をクリックして Kubernetes Navigator Cluster Map を開き、メトリクスが送信されていることを確認します。\nクラスタが検出され、レポートされていることを確認するには、自分のクラスタを探します（ワークショップでは、他の多くのクラスタが表示されます）。クラスタ名を見つけるには、以下のコマンドを実行し、出力をクリップボードにコピーしてください。\nEcho Cluster Name   echo $(hostname)-k3s-cluster  次に、UIで、Splunkロゴのすぐ下にある「Cluster: - 」メニューをクリックし、先程コピーしたクラスタ名を検索ボックスに貼り付け、チェックボックスをクリックしてクラスタを選択し、最後にメニューのその他の部分をクリックしてフィルタを適用します。\nノードの状態を確認するには、クラスターの淡いブルーの背景にカーソルを置き、左上に表示される青い虫眼鏡 をクリックしてください 。\nこれで、ノードレベルまでドリルダウンできます。 次に、サイドバーボタンをクリックしてサイドバーを開き、Metricsサイドバーを開きます。\nサイドのスライダーを使って、CPU、メモリ、ネットワーク、イベントなど、クラスタ/ノードに関連する様々なチャートを見ることができます。\n  Access Tokens (Org Tokensと呼ばれることもあります)は、長期間利用を前提とした組織レベルのトークンです。デフォルトでは、これらのトークンは 5 年間保存されます。そのため、長期間にわたってデータポイントを送信するエミッターに組み込んだり、Splunk API を呼び出す長期的なスクリプトに使用したりするのに適しています。 ↩︎\n Realm とは、Splunk内部の管理単位ので、その中で組織がホストされます。異なる Realm には異なる API エンドポイントがあります (たとえば、データを送信するためのエンドポイントは、us1 realm では ingest.us1.signalfx.com 、eu0 レルムでは ingest.eu0.signalfx.com となります)。このrealm名は、Splunk UI のプロファイルページに表示されます。エンドポイントを指定する際にレルム名を含めない場合、Splunk は us0 レルムを指していると解釈します。 ↩︎\n   ","categories":["IMT"],"description":"","excerpt":" Splunk Helm chartを使用して、K3s に OpenTelemetry Collector をインストー …","ref":"/observability-workshop/v4.22/ja/imt/docs/gdi/k3s/","tags":["k3s"],"title":"Kubernetes環境にOpenTelemetry Collectorをデプロイする"},{"body":"This Lab walks your through using the Chrome Selenium IDE extension to create a synthetic transaction against a Splunk demo instance and creating a Splunk Synthetic Monitoring Real Browser Check (RBC). In addition you also get to learn other Splunk Synthetic Monitoring checks like REST API checks and Uptime Checks.\n1. Prerequisites Ensure you can login with your username and password at https://monitoring.rigor.com and https://optimization.rigor.com. Also, make sure you are assigned to your own account for example: O11y Workshop.\nEdit your Splunk Synthetic Monitoring account personal information and adjust your timezone and email notifications. Splunk Synthetic Monitoring will default to start sending you notifications, you can turn them off at the monitor configuration level.\nAdd the Chrome Selenium IDE extension to your Chrome Browser. Once installed click on the extension and you will see the following screen:\n2. Using Selenium IDE You can now go ahead and record a web transaction using Selenium IDE to check on http://splunk.o11ystore.com.\nClick on Record a new test in a new project, name the project [YOUR_INITIALS] - O11y Store e.g. RWC - O11y Store.\n!!! question “What is Selenium IDE?” - Selenium IDE is an open source record and playback test automation for the web. - Selenium is a portable framework for testing web applications. - Selenium provides a playback tool for authoring functional tests without the need to learn a test scripting language (Selenium IDE). - It also provides a test domain-specific language (Selenese) to write tests in a number of popular programming languages, including C#, Groovy, Java, Perl, PHP, Python, Ruby and Scala. - The tests can then run against most modern web browsers. - Selenium runs on Windows, Linux, and macOS. - It is open-source software released under the Apache License 2.0.\nEnter http://splunk.o11ystore.com as your base URL.\nClick Start Recording   , a new window should open up with splunk.o11ystore.com. Click Vintage Camera Lens, click Add To Cart and then click Place Order.\nClose the window and then stop the recording by navigating back to Selenium IDE. Finally name the test: [YOUR_INITIALS] - Checkout Flow (Desktop) e.g. RWC - Checkout Flow (Desktop).\nYour Selenium IDE Project will look something like this:\nTest your recording by pressing on the play button, make sure your recording successfully completes the transaction:\nSave your Selenium IDE Project to your Downloads folder as Workshop.side\n3. Create Real Browser Check Login to Splunk Synthetic Monitoring using https://monitoring.rigor.com. Click on REAL BROWSER and click +New{: .label-button .sfx-ui-button-blue}.\nClick on “From File” and select your recording then click on Import\nSet the Frequency to 5 Minutes\nClick on Steps and make the following adjustments to your recording provide a friendly name to Steps 1 (Click Camera), 2 (Add to Cart) \u0026 3 (Place Order).\nNext, click + Add Step, with this new step we will add some validation to the monitor. This is to ensure the checkout completed successfully.\nEnter Confirm Order for the Name and change the Action to Wait for text present and finally enter Your order is complete! for the Value. You will now have a Start Url and 4 steps in your monitor configuration.\nTip\nAs you are creating the steps think about how to go about using the Business Transaction feature in Splunk Synthetic Monitoring which is very powerful. “Business Transactions are a combined group of contiguous steps in a Real Browser script that are to be measured as a whole. These transactions logically group similar parts of a flow together, so that users can view the performance of multiple steps and page(s) grouped under one Business Transaction.\"\n  Click on Advanced and make sure the Viewport Size is set to Default desktop: 1366 x 768\nClick on “Test” to test your monitor. Once the test has successfully completed make sure to click on “AFTER” in Step 4 to validate the monitor was able to get to the order complete screenshot.\nClick on Create{: .label-button .sfx-ui-button-blue} to save your Real Browser Monitor. After 5-10 minutes validate your monitor is working and producing successful checks e.g.\nTip\nYou can force to run your monitor now using Run Now\n  Change your view to Segment by location and observe the difference. You can turn off/on locations by clicking on them.\n!!! question “Question?” Which Location has the poorest Response Time?\nClick on one of the successful circles to drilldown into that Run:\nTake a moment to explore the metrics with the CONFIGURE METRICS/HIDE METRICS dropdown.\nClick Page 2 in the dropdown, and scroll down to view the Filmstrip and the Waterfall Chart.\nClick on Click Here to Analyze with Optimization which will prompt you to login to your Splunk Synthetic Monitoring Optimization Account. If you don’t have this option, navigate to this page.\nClick the “Best Practices Score” tab. Scroll down, and review all the findings\nSpend some time to review the findings. Click into any line item\n4. Create Mobile Check Copy the RBC you created above:\nRename it, for example: RWC - Checkout Flow (Tablet)\nUnder the Advanced tab, update the following three settings and create your new mobile RBC.\nTest \u0026 Validate the new monitor\nTip\nAs you are creating the steps try using the Business Transaction feature in Splunk Synthetic Monitoring. “Business Transactions are a combined group of contiguous steps in a Real Browser script that are to be measured as a whole. These transactions logically group similar parts of a flow together, so that users can view the performance of multiple steps and page(s) grouped under one Business Transaction.\"\n  5. Resources   Getting Started With Selenium IDE\n  Splunk Synthetic Monitoring Scripting Guide\n  How Can I Fix A Broken Script?\n  Introduction to the DOM (Document Object Model (DOM)\n  Selenium IDE\n  ","categories":"","description":"Scripting and configuring a Real Browswer Check\n","excerpt":"Scripting and configuring a Real Browswer Check\n","ref":"/observability-workshop/v4.22/synthetics/docs/real-browser-checks/","tags":"","title":"Real Browser Check"},{"body":"This Lab walks your through using the Chrome Selenium IDE extension to create a synthetic transaction against a Splunk demo instance and creating a Splunk Synthetic Monitoring Real Browser Check (RBC). In addition you also get to learn other Splunk Synthetic Monitoring checks like REST API checks and Uptime Checks.\n1. Prerequisites Ensure you can login with your username and password at https://monitoring.rigor.com and https://optimization.rigor.com . Also, make sure you are assigned to your own account for example: O11y Workshop.\nEdit your Splunk Synthetic Monitoring account personal information and adjust your timezone and email notifications. Splunk Synthetic Monitoring will default to start sending you notifications, you can turn them off at the monitor configuration level.\nAdd the Chrome Selenium IDE extension to your Chrome Browser. Once installed click on the extension and you will see the following screen:\n2. Using Selenium IDE You can now go ahead and record a web transaction using Selenium IDE to check on http://splunk.o11ystore.com .\nClick on Record a new test in a new project, name the project [YOUR_INITIALS] - O11y Store e.g. RWC - O11y Store.\n!!! question “What is Selenium IDE?” - Selenium IDE is an open source record and playback test automation for the web. - Selenium is a portable framework for testing web applications. - Selenium provides a playback tool for authoring functional tests without the need to learn a test scripting language (Selenium IDE). - It also provides a test domain-specific language (Selenese) to write tests in a number of popular programming languages, including C#, Groovy, Java, Perl, PHP, Python, Ruby and Scala. - The tests can then run against most modern web browsers. - Selenium runs on Windows, Linux, and macOS. - It is open-source software released under the Apache License 2.0.\nEnter http://splunk.o11ystore.com as your base URL.\nClick Start Recording{: .label-button .sfx-ui-button-grey}, a new window should open up with splunk.o11ystore.com . Click Vintage Camera Lens, click Add To Cart and then click Place Order.\nClose the window and then stop the recording by navigating back to Selenium IDE. Finally name the test: [YOUR_INITIALS] - Checkout Flow (Desktop) e.g. RWC - Checkout Flow (Desktop).\nYour Selenium IDE Project will look something like this:\nTest your recording by pressing on the play button, make sure your recording successfully completes the transaction:\nSave your Selenium IDE Project to your Downloads folder as Workshop.side\n3. Create Real Browser Check Login to Splunk Synthetic Monitoring using https://monitoring.rigor.com . Click on REAL BROWSER and click +New{: .label-button .sfx-ui-button-blue}.\nClick on “From File” and select your recording then click on Import\nSet the Frequency to 5 Minutes\nClick on Steps and make the following adjustments to your recording provide a friendly name to Steps 1 (Click Camera), 2 (Add to Cart) \u0026 3 (Place Order).\nNext, click + Add Step, with this new step we will add some validation to the monitor. This is to ensure the checkout completed successfully.\nEnter Confirm Order for the Name and change the Action to Wait for text present and finally enter Your order is complete! for the Value. You will now have a Start Url and 4 steps in your monitor configuration.\nTip\nAs you are creating the steps think about how to go about using the Business Transaction feature in Splunk Synthetic Monitoring which is very powerful. “Business Transactions are a combined group of contiguous steps in a Real Browser script that are to be measured as a whole. These transactions logically group similar parts of a flow together, so that users can view the performance of multiple steps and page(s) grouped under one Business Transaction.\"\n  Click on Advanced and make sure the Viewport Size is set to Default desktop: 1366 x 768\nClick on “Test” to test your monitor. Once the test has successfully completed make sure to click on “AFTER” in Step 4 to validate the monitor was able to get to the order complete screenshot.\nClick on Create{: .label-button .sfx-ui-button-blue} to save your Real Browser Monitor. After 5-10 minutes validate your monitor is working and producing successful checks e.g.\nTip\nYou can force to run your monitor now using Run Now\n  Change your view to Segment by location and observe the difference. You can turn off/on locations by clicking on them.\n!!! question “Question?” Which Location has the poorest Response Time?\nClick on one of the successful circles to drilldown into that Run:\nTake a moment to explore the metrics with the CONFIGURE METRICS/HIDE METRICS dropdown.\nClick Page 2 in the dropdown, and scroll down to view the Filmstrip and the Waterfall Chart.\nClick on Click Here to Analyze with Optimization which will prompt you to login to your Splunk Synthetic Monitoring Optimization Account. If you don’t have this option, navigate to this page .\nClick the “Best Practices Score” tab. Scroll down, and review all the findings\nSpend some time to review the findings. Click into any line item\n4. Create Mobile Check Copy the RBC you created above:\nRename it, for example: RWC - Checkout Flow (Tablet)\nUnder the Advanced tab, update the following three settings and create your new mobile RBC.\nTest \u0026 Validate the new monitor\nTip\nAs you are creating the steps try using the Business Transaction feature in Splunk Synthetic Monitoring. “Business Transactions are a combined group of contiguous steps in a Real Browser script that are to be measured as a whole. These transactions logically group similar parts of a flow together, so that users can view the performance of multiple steps and page(s) grouped under one Business Transaction.\"\n  5. Resources   Getting Started With Selenium IDE   Splunk Synthetic Monitoring Scripting Guide   How Can I Fix A Broken Script?   Introduction to the DOM (Document Object Model (DOM)\n  Selenium IDE   ","categories":"","description":"Scripting and configuring a Real Browswer Check\n","excerpt":"Scripting and configuring a Real Browswer Check\n","ref":"/observability-workshop/v4.22/ja/synthetics/docs/real-browser-checks/","tags":"","title":"Real Browser Check"},{"body":"","categories":"","description":"Environment Configuration and Hands-On Exercises\n","excerpt":"Environment Configuration and Hands-On Exercises\n","ref":"/observability-workshop/v4.22/imt/docs/","tags":"","title":"Setup and Exercises"},{"body":"During this technical Splunk Observability Cloud Infrastructure Monitoring and APM Workshop you will build out an environment based on a lightweight Kubernetes1 cluster.\nIn order to simplify the workshop modules, a pre-configured AWS/EC2 instance is provided.\nThe instance is pre-configured with all the software required to deploy the Splunk OpenTelemetery Connector2 in Kubernetes, deploy a NGINX3 ReplicaSet4 and finally deploy a microservices based application which has been instrumented using OpenTelemetry to send metrics, traces, spans and logs5.\nThe workshops also introduce you to dashboards, editing and creating charts, creating detectors to fire alerts, Monitoring as Code6 and the Service Bureau6\nBy the end of these technical workshops you will have a good understanding of some of the key features and capabilities of the Splunk Observability Cloud.\nHere are the instructions on how to access you pre-configured AWS/EC2 instance\n  Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. ↩︎\n The OpenTelemetry Collector offers a vendor-agnostic implementation on how to receive, process and export telemetry data. In addition, it removes the need to run, operate and maintain multiple agents/collectors in order to support open-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) sending to multiple open-source or commercial back-ends. ↩︎\n NGINX is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. ↩︎\n Kubernetes ReplicaSet ↩︎\n Jaeger, inspired by Dapper and OpenZipkin, is a distributed tracing system released as open source by Uber Technologies. It is used for monitoring and troubleshooting microservices-based distributed systems ↩︎\n Monitoring as Code and Service Bureau ↩︎\n   ","categories":"","description":"","excerpt":"During this technical Splunk Observability Cloud Infrastructure …","ref":"/observability-workshop/v4.22/imt/","tags":"","title":"Introduction"},{"body":"この テクニカル Splunk Observability Cloud ワークショップでは、 lightweight Kubernetes1 クラスタをベースにした環境を構築します。\nワークショップのモジュールを簡素化するために、あらかじめ設定されたAWS/EC2インスタンスが提供されます。\nこのインスタンスには、ワークショップに必要となるソフトウェアが予め設定されています。これに対してOpenTelemetery Collector2 を Kubernetes 上でデプロイし、 NGINX3 の ReplicaSet4 をデプロイし、最後に OpenTelemetry を使用して計装されたマイクロサービスベースのアプリケーションをデプロイして、メトリクス、トレース、スパン5を送信していきます。\nさらにこのワークショップでは、ダッシュボード、チャートの編集と作成、アラートを発するためのディテクターの作成、Monitoring as Code6 および Service Bureau6 についても紹介します。\nこのテクニカルワークショップを終える頃には、Splunk Observability Cloudの主要な機能や性能を十分に理解していることでしょう。\n事前に設定された AWS/EC2 インスタンス へのアクセス方法をご紹介します。\n  Kubernetes は、コンテナ化されたワークロードやサービスを管理するためのポータブルで拡張可能なオープンソースのプラットフォームで、宣言的な構成と自動化の両方を促進します。 ↩︎\n OpenTelemetry Collector は、遠隔測定データの受信、処理、およびエクスポートの方法について、ベンダーに依存しない実装を提供します。さらに、複数のオープンソースまたは商用バックエンドに送信するオープンソースの遠隔測定データ形式（Jaeger、Prometheusなど）をサポートするために、複数のエージェント/コレクターを実行、運用、保守する必要性を排除します。 ↩︎\n NGINX は、リバースプロキシ、ロードバランサー、メールプロキシ、HTTPキャッシュとしても使用できるWebサーバーです。 ↩︎\n Kubernetes ReplicaSet を使用しています。 ↩︎\n Jaeger は、Dapper や OpenZipkin にインスパイアされた、Uber Technologies がオープンソースとして公開している分散型トレースシステムです。マイクロサービスベースの分散システムの監視とトラブルシューティングに使用されています。 ↩︎\n Monitoring as Code and Service Bureau を使用しています。 ↩︎\n   ","categories":"","description":"","excerpt":"この テクニカル Splunk Observability Cloud ワークショップでは、 lightweight Kubernetes1 …","ref":"/observability-workshop/v4.22/ja/imt/","tags":"","title":"はじめに"},{"body":"Welcome to the bootcamp During this technical Splunk Observability Content Pack Workshop you will build send data from Splunk Observability Cloud 1 into an IT Service Intelligence instance 2.\nIn order to simplify the workshop modules, a pre-configured AWS/EC2 instance is provided for your AWS environnemnt\nBy the end of these technical workshops you will have a good understanding of some of the key features and capabilities of the Splunk Observability Cloud.\nHere(link) are the instructions on how to access you pre-configured AWS/EC2 instance\n  Splunk Observability Cloud provides full-fidelity monitoring and troubleshooting across infrastructure, applications, and user interfaces, in real-time and at any scale, to help you keep your services reliable, deliver great customer experiences and innovate faster ↩︎\n Splunk IT Service Intelligence (ITSI) is a monitoring and analytics solution powered by artificial intelligence for IT Operations (AIOps). It provides visibility into the health of critical IT and business services and their infrastructure. Use ITSI to solve a variety of IT challenges, including deriving service-level insights and analysis on events, metrics, and logs to find and fix the most important issues first. ↩︎\n   ","categories":"","description":"","excerpt":"Welcome to the bootcamp During this technical Splunk Observability …","ref":"/observability-workshop/v4.22/itsi/","tags":"","title":"Splunk Observability Content Pack Workshop"},{"body":" ダッシュボードとチャートの紹介 チャートの編集と作成 フィルタリングと分析関数 数式の使用 ダッシュボードでのチャートの保存 SignalFlowの紹介   1. ダッシュボード ダッシュボードとは、チャートをグループ化し、メトリクスを視覚化したものです。適切に設計されたダッシュボードは、システムに関する有益で実用的な洞察を一目で提供します。ダッシュボードは複雑なものもあれば、見たいデータだけを掘り下げたいくつかのチャートだけのものもあります。\nこのモジュールでは、次のようなチャートとダッシュボードを作成し、それをチームページに接続します。\n 2. あなたのチームのページ 左のナビゲーションから を開きます。あなたはすでにチームに割り当てられているので、チームダッシュボードが表示されます。\nここでは、チーム Observability を例に挙げています。実際のワークショップでは、別のチーム名かも知れません。\nこのページには、チームメンバーの総数、チームのアクティブなアラートの数、チームに割り当てられているすべてのダッシュボードが表示されます。現在、ダッシュボードは割り当てられていませんが、この後で、あなたが作成する新しいダッシュボードをチームページに追加していきます。\n 3. サンプルチャート 続けて、画面右上の All Dashboards をクリックします。事前に作成されたものも含め、利用可能なすべてのダッシュボードが表示されます。\nすでにSplunk Agentを介してCloud APIインテグレーションや他のサービスからメトリクスを受信している場合は、これらのサービスに関連するダッシュボードが表示されます。\n 4. サンプルデータの確認 ダッシュボードの中に、 Sample Data というダッシュボードグループがあります。Sample Data ダッシュボードグループをクリックして展開し、Sample Charts ダッシュボードをクリックします。\nSample Charts ダッシュボードでは、ダッシュボードでチャートに適用できる様々なスタイル、色、フォーマットのサンプルを示すチャートが表示されます。\nこのダッシュボードグループのすべてのダッシュボード（PART 1、PART 2、PART 3、INTRO TO SPLUNK OBSERVABILITY CLOUD）に目を通してみてください。\n","categories":"","description":"","excerpt":" ダッシュボードとチャートの紹介 チャートの編集と作成 フィルタリングと分析関数 数式の使用 ダッシュボードでのチャー …","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/intro/","tags":"","title":"ダッシュボード、チャート、メトリクスを使う"},{"body":"","categories":"","description":"ここでは、環境設定とハンズオン演習を行います\n","excerpt":"ここでは、環境設定とハンズオン演習を行います\n","ref":"/observability-workshop/v4.22/ja/imt/docs/","tags":"","title":"セットアップと演習"},{"body":".NET CORE example uses the .NET http client to get non-responding URL so makes valid traces with 403 status code\nContainerized with these instructions from Microsoft\n.NET Core 5\ncd ~/otelworkshop/k8s/dotnet Deploy:\nsource deploy-client.sh Delete deployment:\nsource delete-all.sh .NET Core 2.1 .NET Core 2.1 located here\n","categories":"","description":"","excerpt":".NET CORE example uses the .NET http client to get non-responding URL …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_k8s/examples/dotnet/","tags":"","title":".Net Setup"},{"body":" How to retrieve the IP address of the AWS/EC2 instance assigned to you. Connect to your instance using SSH, Putty1 or your web browser. Verify your connection to your AWS/EC2 cloud instance.   1. AWS/EC2 IP Address In preparation for the workshop, Splunk has prepared an Ubuntu Linux instance in AWS/EC2.\nTo get access to the instance that you will be using in the workshop please visit the URL to access the Google Sheet provided by the workshop leader.\nSearch for your AWS/EC2 instance by looking for your first and last name, as provided during registration for this workshop.\nFind your allocated IP address, SSH command (for Mac OS, Linux and the latest Windows versions) and password to enable you to connect to your workshop instance.\nIt also has the Browser Access URL that you can use in case you cannot connect via ssh or putty - see EC2 access via Web browser\nImportant\nPlease use SSH or Putty to gain access to your EC2 instance if possible and make a note of the IP address as you will need this during the workshop.   2. SSH (Mac OS/Linux) Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device.\nTo use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in Step #1).\nWhen prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes.\nEnter the password provided in the Google Sheet from Step #1.\nUpon successful login you will be presented with the Splunk logo and the Linux prompt.\nAt this point you are ready to continue and start the workshop\n 3. Putty (Windows) If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find here.\nImportant\nIf you cannot install Putty, please go to Web Browser (All).   Open Putty and enter the in Host Name (or IP address) field the IP address provided in the Google Sheet.\nYou can optionally save your settings by providing a name and pressing Save.\nTo then login to your instance click on the Open button as shown above.\nIf this is the first time connecting to your AWS/EC2 workshop instance, you will be presented with a security dialog, please click Yes.\nOnce connected, login in as ubuntu and the password is the one provided in the Google Sheet.\nOnce you are connected successfully you should see a screen similar to the one below:\nAt this point you are ready to continue and start the workshop\n 4. Web Browser (All) If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser.\nNote\nThis assumes that access to port 6501 is not restricted by your company’s firewall.   Open your web browser and type http://x.x.x.x:6501 (where X.X.X.X is the IP address from the Google Sheet).\nOnce connected, login in as ubuntu and the password is the one provided in the Google Sheet.\nOnce you are connected successfully you should see a screen similar to the one below:\nUnlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions.\nWhen the workshop ask you to copy instructions into your terminal, please do the following:\nCopy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below:\nThis will open a dialog box asking for the text to be pasted into the web terminal:\nPaste the text in the text box as show, then press OK to complete the copy and paste process.\n Note\nUnlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal.\nSimply click the Connect button and you will be reconnected and will be able to continue.\n  At this point you are ready to continue and start the workshop.\n 5. Multipass (All) If you are unable to access AWS, but you want to install software locally, follow the instructions for using Multipass.\n  Download Putty ↩︎\n   ","categories":"","description":"**5 minutes**\n","excerpt":"**5 minutes**\n","ref":"/observability-workshop/v4.22/imt/docs/initial-setup/","tags":"","title":"How to connect to your workshop environment"},{"body":"The API Check provides a flexible way to check the functionality and performance of API endpoints. The shift toward API-first development has magnified the necessity to monitor the back-end services that provide your core front-end functionality. Whether you’re interested in testing the multi-step API interactions or you want to gain visibility into the performance of your endpoints, the API Check can help you accomplish your goals.\n1. Create a Global Variable View the global variable that we’ll use to perform our API check. Click on Global Variables under Admin Tools. View the global variable that we’ll use to make the spotify API transaction\n2. Create an API Check Create a new API Check and name it \u003cyour initials\u003e followed by Splunk REST API Check for example: AP - Spotify API\nTake a second to explore the notification tab after you’ve named your check\nAdd the following API Check Steps:\nAvailable Variables to choose from:\nRequest Step\n A Request Step makes an HTTP request to some endpoint and collects data from that interaction. Unlike other check types, API Checks do not require an initial URL to start the check. All HTTP requests are configured within Request Steps.  Extract Step\n  An Extract Step extracts data out of JSON, XML, or HTML formatted data.\n  To extract data out of JSON, supply three things:\n  The source containing the JSON,\n  The JSONPath expression to extract out the data, and\n  The name of the custom variable that you want to save to.\n  The source can be any JSON, but most likely will come from the response body. The source could also come from a response header or can be a custom value. The source must be well-formed JSON.\n  Save Step\n  A Save Step stores some data to be reused later in the check. To save data, supply the source and the name of the custom variable to save to. The source can be selected from the presets, including response headers, or by providing a custom value.\n  Some additional use cases are appending bits of information to easily reuse in other steps and saving the results from one request to be reused after another request is made.\n  It is important to remember that request variables are only available after a request is made. If you try to save a value from a request but haven’t made a request yet, then an empty string will be saved.\n  Assert Step\n An Assert Step makes an assertion on two values. To make an assertion, supply two parameters along with the comparison that you would like to perform between the two.  Comparisons\n  We currently support 3 types of comparisons: string, numeric, and regular expression.\n  For string and numeric comparisons, values are coerced to the comparison type before the comparison is made.\n  For a regular expression comparison, the first parameter is a string and the second parameter is a regular expression.\n  Tag your API Check with Splunk and API and SAVE it\n3. Test your REST API Check Press got back into the edit configuration and press ‘test’ at the bottom of the page to ensure there are no errors\nSlide the window up to view details about the successful run\nNow, let’s add some more functionality to the monitor. Slide the detailed window back down and add steps 5-8\nBONUS: use step 6 to assert that the following response came back in a timely manner (1000 ms)\nOnce the steps are added, test \u0026 save the monitor.\n4. Resources   How to Create an API Check\n  API Check Overview\n  How Do I Use Business Transactions?\n  ","categories":"","description":"Scripting and configuring an API Check\n","excerpt":"Scripting and configuring an API Check\n","ref":"/observability-workshop/v4.22/synthetics/docs/api-checks/","tags":"","title":"API Checks"},{"body":"API Checkは、APIエンドポイントの機能およびパフォーマンスをチェックする柔軟な方法を提供します。APIファーストの開発へのシフトにより、フロントエンドのコア機能を提供するバックエンドサービスを監視する必要性が高まっています。複数ステップのAPIインタラクションのテストに興味がある場合でも、エンドポイントのパフォーマンスを可視化したい場合でも、API Checkは目標の達成に役立ちます。\nグローバル変数の作成 API Checkを行うために使用するグローバル変数を表示します。 Admin Tools の下にある Global Variables をクリックします。 spotifyのAPIトランザクションを行うために使用するグローバル変数を確認してください。\nAPI Check の作成 新しい API Check を作成し、\u003cあなたのイニシャル\u003e の後に Splunk REST API Check をつけた名前にします （例: AP - Spotify API）\nチェックに名前を付けたら、notificationタブを開いて、どのような設定があるか眺めてみましょう。\n次に、以下のAPI Check Stepsを追加します。\n変数はこちらから選ぶことができます:\nRequest Step \n リクエストステップは、あるエンドポイントにHTTPリクエストを行い、そのレスポンスからデータを取得します。他のチェックタイプとは異なり、APIチェックでは、チェックを開始するための初期URLは必要ありません。すべてのHTTPリクエストは、リクエストステップ内で設定されます。  Extract Step \n  Extractステップでは、JSON、XML、HTML形式のデータからデータを抽出します。\n  JSONからデータを抽出するには、次の3つを用意します:\n  JSONを含むソース\n  データを抽出するためのJSONPath式\n  保存先のカスタム変数名\n  ソースはどのようなJSONでもかまいませんが、たいていはレスポンスのBodyから取得するでしょう。レスポンスヘッダから取得することもできますし、また、カスタムの値も可能です。ソースは、整形されたJSONでなければなりません。\n  Save Step \n  Saveステップでは、チェックの後で再利用するためのデータを保存します。データを保存するには、ソースと保存先のカスタム変数名を指定します。ソースは、応答ヘッダを含むプリセットから選択するか、カスタム値を指定します。\n  その他の使用例としては、他のステップで簡単に再利用できるように情報を追加したり、リクエストの結果を保存して別のリクエストで再利用できるようにするなどがあります。\n  リクエスト変数は、リクエストが作成された後にのみ使用可能であることを覚えておくことが重要です。もし、リクエストから値を保存しようとしても、まだリクエストを行っていない場合は、空の文字列が保存されます。\n  Assert Step \n Assertステップは、2つの値に対してアサーションを行います。アサーションを行うには、2つのパラメータと、その2つの比較方法を指定します。  Comparisons \n  現在、string（文字列）、 numeric（数値）、＊＊regular expression（正規表現）** の3種類の比較をサポートしています。\n  string と numeric では、値が比較タイプに強制されます。\n  reqular expression での比較の場合、最初のパラメータは文字列で、2番目のパラメータは正規表現になります。\n  API Check に Splunk と API のタグを付けて SAVE します。\nREST API Checkのテスト edit configuration に戻り、ページの下にある ‘test’ を押して、エラーがないことを確認します。\nウィンドウを上にスライドさせると、正常に実行された場合の詳細が表示されます\nさて、モニターにもう少し機能を追加してみましょう。詳細ウィンドウを下にスライドさせ、手順5～8を追加します。\nBONUS：ステップ6を使用して、以下のレスポンスがタイムリーに戻ってきたことをアサートします（1000 ms)\nステップを追加したら、モニターをテストして保存します。\nリソース   How to Create an API Check   API Check Overview   How Do I Use Business Transactions?   ","categories":"","description":"Scripting and configuring an API Check\n","excerpt":"Scripting and configuring an API Check\n","ref":"/observability-workshop/v4.22/ja/synthetics/docs/api-checks/","tags":"","title":"API Checks"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_k8s/","tags":"","title":"APM for Kubernetes"},{"body":" 各自に割り当てられたAWS/EC2インスタンスのIPアドレスを確認します SSH、Putty1、またはWebブラウザを使ってインスタンスに接続します クラウド上にある AWS/EC2 インスタンスへの接続を確認します   1. AWS/EC2 の IP アドレス ワークショップの準備として、Splunk は AWS/EC2 に Ubuntu Linux インスタンスを用意しています。\nワークショップで使用するインスタンスにアクセスするには、ワークショップのリーダーが提供する Google Sheets のURLにアクセスしてください。\nAWS/EC2 インスタンスの検索には、本ワークショップの登録時にご記入いただいたお名前（姓名）を入力してください。\nワークショップのインスタンスに接続するためのIPアドレス、SSHコマンド（Mac OS、Linux、最新のWindowsバージョン用）、パスワードが表示されています。\nまた、ssh や putty で接続できない場合に使用するブラウザアクセスのURLも記載されています。「ブラウザ経由でEC2に接続する 」を参照してください。\nImportant\n可能であれば、SSH または Putty を使用してEC2インスタンスにアクセスしてください。 ワークショップで必要になるので、IPアドレスをメモしておいてください。   2. SSH (Mac OS/Linux) Mac や Linux の端末から SSH を使ってワークショップに接続することができます。\nSSH を使用するには、お使いのシステムでターミナルを開き、ssh ubuntu@x.x.x.xと入力してください（x.x.x.xをステップ1で見つけたIPアドレスに置き換えてください）。\nAre you sure you want to continue connecting (yes/no/[fingerprint])? というプロンプトが表示されたら yes と入力してください。\nステップ1の Google Sheets に記載されているパスワードを入力してください。\nログインに成功すると、Splunk のロゴと Linux のプロンプトが表示されます。\nこれで ワークショップを開始する に進む準備が整いました。\n 3. Putty (Windows) ssh がプリインストールされていない場合や、Windows システムを使用している場合、putty がおすすめです。Putty は こちら からダウンロードできます。\nImportant\nPutty がインストールできない場合は、ブラウザ経由でEC2に接続する で進めてください。\n  Putty を開き、Host Name (or IP address) の欄に、Google Sheets に記載されているIPアドレスを入力してください。\n名前を入力して Save を押すと、設定を保存することができます。\nインスタンスにログインするには、Open ボタンをクリックします。\n初めて AWS/EC2 ワークショップインスタンスに接続する場合は、セキュリティダイアログが表示されますので、Yes をクリックしてください。\n接続されたら、ubuntu としてログインし、パスワードは Google Sheets に記載されているものを使用します。\n接続に成功すると、以下のような画面が表示されます。\nこれで ワークショップを開始する 準備が整いました。\n 4. ブラウザ経由でEC2に接続する SSH（ポート22） の使用が禁止されている場合や、Putty がインストールできない場合は、Webブラウザを使用してワークショップのインスタンスに接続することができます。\nNote\nここでは、6501番ポートへのアクセスが、ご利用のネットワークのファイアウォールによって制限されていないことを前提としています。   Webブラウザを開き、http:/x.x.x.x:6501 （X.X.X.Xは Google Sheetsに記載されたIPアドレス）と入力します。\n接続されたら、ubuntu としてログインし、パスワードは Google Sheets に記載されているものを使用します。\n接続に成功すると、以下のような画面が表示されます。\n通常のSSHを使用しているときとは異なり、ブラウザセッションを使用しているときは、コピー＆ペースト を使うための手順が必要です。これは、クロスブラウザの制限によるものです。\nワークショップで指示をターミナルにコピーするように言われたら、以下のようにしてください。\n通常通り指示をコピーし、ウェブターミナルにペーストする準備ができたら、以下のように Paste from browser を選択します。\nすると、ウェブターミナルに貼り付けるテキストを入力するダイアログボックスが表示されます。\n表示されているテキストボックスにテキストを貼り付け、OK を押すと、コピー＆ペーストができます。\nNote\n通常のSSH接続とは異なり、Webブラウザには60秒のタイムアウトがあり、接続が解除されると、Webターミナルの中央に Connect ボタンが表示されます。\nこの Connect ボタンをクリックするだけで、再接続され、次の操作が可能になります。\n  これで ワークショップを開始する 準備が整いました。\n 5. Multipass (全員) AWSへはアクセスできないが、ローカルにソフトウェアをインストールできる場合は、「Multipassを使用する 」の手順に従ってください。\n  Putty のダウンロード  ↩︎\n   ","categories":"","description":"**5 分**\n","excerpt":"**5 分**\n","ref":"/observability-workshop/v4.22/ja/imt/docs/initial-setup/","tags":"","title":"ワークショップ環境へのアクセス"},{"body":"Configure the Infrastructure Add-on and the Observability Content Pack Now that we have access to our instances, which bear the pre-installed Infrastructure Monitoring Add-On and the Observability Content Pack, we need to configure those two by follwoing the steps below.\nConfiguration of the Infrastructure Monitoring Add-on After you accessed your instance, navigate to the Splunk Infrastructure Monitoring Add-On listed on the left under Apps. We want to set up an account, and we can do so by navigating to the Configuration Tab and clicking on the ‘Connect an Account'-Button.\nOnce you clicked the ‘Connect an Account’-Button, a dialogue appears, prompting you for the user credentials of your Observability Cloud account. These are the Access Token and the Realm, with which the Add-On can access the Oberservability Cloud. In the next steps, we are going to locate our Realm inside our individual Observability Cloud account and create a new Access Token.\nLocate your Realm: Log in to your Splunk Observability account. In the menu on the left on the bottom click on the little gear icon (Settings). On the very top of this menu, you should see your username right next to a profile picture. Click on it. You are now in the Account Settings, where you can find the Realm (see screenshot below).\nCopy and paste the Realm into the input field of of the dialogue in the IM Add-On.\nLocate your Access Token: Being still in your Account Settings, click on Generate User API Access Token to generate an access token, and subsequently on Show User API Access Token to show the associated string. Copy and paste that string into the input field of of the dialogue in the IM Add-On.\nOnce the Realm and Access Token have been inserted into the input dialogue, make sure to verify whether or not a connection to the Observability Cloud could be established by clicking on the Check Connection-button. If so, click submit. You can enable data collection for the account by selecting the Data Collection toggle.\nFor additional information on this topic, see Configure the Splunk Infrastructure Monitoring Add-on.\n*Watch this video introducing the content pack concepts.\n Configure the Content Pack for Observability As soon as we have successfully configured the Infrastructure Monitoring Add-On, we will continue by installing and configuring the Content Pack for Observability. The first step to accomplish that is to select the IT Service Intelligence app. Inside the app, click on the Configuration tab and select Data Integrations from the dopdown menu.\nOn the next screen, select Add content packs and choose Splunk Observability Cloud.\nUpon clicking on the Splunk Observability Cloud-tile, you are presented with an overview of what is included in the Content Pack. Review it, and finally click on\nNext, you are presented with a settings menu to configure the content pack. The following is important: Please disable the Import as enable-option, leave the Enter a prefix input field blank, and disable the Backfill service KPIs option.\nFinally, click on the [Install selected] button.\nThe Splunk Observability Cloud tile on the Data Integrations page should now have a little green checkmark on the upper right corner. This means that we are all set. Perfect!\nFor additional information on this topic, see Install the Content Pack for Splunk Observability Cloud.\n","categories":"","description":"","excerpt":"Configure the Infrastructure Add-on and the Observability Content Pack …","ref":"/observability-workshop/v4.22/itsi/docs/configuration/","tags":"","title":"Configuration"},{"body":" Create a Detector from one of your charts Setting Alert conditions Running a pre-flight check Working with muting rules   1. Introduction Splunk Observability Cloud uses detectors, events, alerts, and notifications to keep you informed when certain criteria are met. For example, you might want a message sent to a Slack channel or to an email address for the Ops team when CPU Utilization has reached 95%, or when the number of concurrent users is approaching a limit that might require you to spin up an additional AWS instance.\nThese conditions are expressed as one or more rules that trigger an alert when the conditions in the rules are met. Individual rules in a detector are labeled according to criticality: Info, Warning, Minor, Major, and Critical.\n2. Creating a Detector In Dashboards click on your Custom Dashboard Group (that you created in the previous module) and then click on the dashboard name.\nWe are now going to create a new detector from a chart on this dashboard. Click on the bell icon on the Latency vs Load chart, and then click New Detector From Chart.\nIn the text field next to Detector Name, ADD YOUR INITIALS before the proposed detector name.\nNaming the detector\nIt’s important that you add your initials in front of the proposed detector name.\nIt should be something like this: XYZ’s Latency Chart Detector.   Click on Create Alert Rule   \nIn the Detector window, inside Alert signal, the Signal we will alert on is marked with a (blue) bell in the Alert on column. The bell indicates which Signal is being used to generate the alert.\nClick on Proceed to Alert Condition   \n 3. Setting Alert condition In Alert condition, click on Static Threshold and then on Proceed to Alert Settings   \nIn Alert Settings, enter the value 290 in the Threshold field. In the same window change Time on top right to past day (-1d).\n 4. Alert pre-flight check A pre-flight check will take place after 5 seconds. See the Estimated alert count. Based on the current alert settings, the amount of alerts we would have received in 1 day would have been 3.\nAbout pre-flight checks\nOnce you set an alert condition, the UI estimates how many alerts you might get based on the current settings, and in the timeframe set on the upper right corner - in this case, the past day.\nImmediately, the platform will start analyzing the signals with the current settings, and perform something we call a Pre-flight Check. This enables you to test the alert conditions using the historical data in the platform, to ensure the settings are logical and will not inadvertently generate an alert storm, removing the guess work from configuring alerts in a simple but very powerful way, only available using the Splunk Observability Cloud.\nTo read more about detector previewing, please visit this link Preview detector alerts.   Click on Proceed to Alert Message   \n 5. Alert message In Alert message, under Severity choose Major.\nClick on Proceed to Alert Recipients   \nClick on Add Recipient and then on your email address displayed as the first option.\nNotification Services\nThat’s the same as entering that email address OR you can enter another email address by clicking on E-mail….\nThis is just one example of the many Notification Services the suite has available. You can check this out by going to the Integrations tab of the top menu, and see Notification Services.\n   6. Alert Activation Click on Proceed to Alert Activation   \nIn Activate… click on Activate Alert Rule   \nIf you want to get alerts quicker you edit the rule and lower the value from 290 to say 280.\nIf you change the Time to -1h you can see how many alerts you might get with the threshold you have chosen based on the metrics from the last 1 hour.\nClick on the in the navbar and then click on Detectors. You can optionally filter for your initials.\nYou will see you detector listed here. If you don’t then please refresh your browser.\nCongratulations! You have created your first detector and activated it!\n","categories":"","description":"","excerpt":" Create a Detector from one of your charts Setting Alert conditions …","ref":"/observability-workshop/v4.22/imt/docs/detectors/creating/","tags":"","title":"Working with Detectors - Lab Summary"},{"body":" Deploy a NGINX ReplicaSet into your K3s cluster and confirm the discovery of your NGINX deployment. Run a load test to create metrics and confirm them streaming into Splunk Observability Cloud!   1. Start your NGINX Verify the number of pods running in the Splunk UI by selecting the WORKLOADS tab. This should give you an overview of the workloads on your cluster.\nNote the single agent container running per node among the default Kubernetes pods. This single container will monitor all the pods and services being deployed on this node!\nNow switch back to the default cluster node view by selecting the MAP tab and select your cluster again.\nIn your AWS/EC2 or Multipass shell session change into the nginx directory:\nChange Directory   cd ~/workshop/k3s/nginx   2. Create NGINX deployment Create the NGINX configmap1 using the nginx.conf file:\nKubectl Configmap Create  Kubectl Create Configmap Output   kubectl create configmap nginxconfig --from-file=nginx.conf configmap/nginxconfig created  Then create the deployment:\nKubectl Create Deployment  Kubectl Create Deployment Output   kubectl create -f nginx-deployment.yaml deployment.apps/nginx created service/nginx created  Next we will deploy Locust2 which is an open source tool used for creating a load test against NGINX:\nKubectl Create Deployment  Kubectl Create Deployment Output   kubectl create -f locust-deployment.yaml deployment.apps/nginx-loadgenerator created service/nginx-loadgenerator created  Validate the deployment has been successful and that the Locust and NGINX pods are running.\nIf you have the Splunk UI open you should see new Pods being started and containers being deployed.\nIt should only take around 20 seconds for the pods to transition into a Running state. In the Splunk UI you will have a cluster that looks like below:\nIf you select the WORKLOADS tab again you will now see that there is a new ReplicaSet and a deployment added for NGINX:\n Let’s validate this in your shell as well:\nKubectl Get Pods  Kubectl Get Pods Output   kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-77784c659c-ttmpk 1/1 Running 0 9m19s splunk-otel-collector-agent-249rd 1/1 Running 0 9m19s svclb-nginx-vtnzg 1/1 Running 0 5m57s nginx-7b95fb6b6b-7sb9x 1/1 Running 0 5m57s nginx-7b95fb6b6b-lnzsq 1/1 Running 0 5m57s nginx-7b95fb6b6b-hlx27 1/1 Running 0 5m57s nginx-7b95fb6b6b-zwns9 1/1 Running 0 5m57s svclb-nginx-loadgenerator-nscx4 1/1 Running 0 2m20s nginx-loadgenerator-755c8f7ff6-x957q 1/1 Running 0 2m20s   3. Run Locust load test Locust, an open source load generator, is available on port 8080 of the EC2 instance’s IP address. Open a new tab in your web browser and go to http://{==EC2-IP==}:8080/, you will then be able to see the Locust running.\nSet the Spawn rate to be 2 and click Start Swarming.\nThis will start a gentle continuous load on the application.\nAs you can see from the above screenshot, most of the calls will report a fail, this is expected, as we have not yet deployed the application behind it, however NGINX is reporting on your attempts and you should be able to see those metrics.\nValidate you are seeing those metrics in the UI by selecting Dashboards → Built-in Dashboard Groups → NGINX → NGINX Servers. Using the Overrides filter on k8s.cluster.name:, find the name of your cluster as returned by echo $(hostname)-k3s-cluster in the terminal.\n  A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable. ↩︎\n What is Locust? ↩︎\n   ","categories":["IMT"],"description":"","excerpt":" Deploy a NGINX ReplicaSet into your K3s cluster and confirm the …","ref":"/observability-workshop/v4.22/imt/docs/gdi/nginx/","tags":["NGNIX"],"title":"Deploying NGINX in K3s"},{"body":" Deploy the Online Boutique application into Kubernetes (K3s) Verify the application is running Generate some artificial traffic using Locust See APM metrics in the UI   1. Deploy Online Boutique To deploy the Online Boutique application into K3s apply the deployment:\nDeploy Online Boutique  Deployment Output   cd ~/workshop kubectl apply -f apm/microservices-demo/k8s/deployment.yaml deployment.apps/checkoutservice created service/checkoutservice created deployment.apps/redis-cart created service/redis-cart created deployment.apps/productcatalogservice created service/productcatalogservice created deployment.apps/loadgenerator created service/loadgenerator created deployment.apps/frontend created service/frontend created service/frontend-external created deployment.apps/paymentservice created service/paymentservice created deployment.apps/emailservice created service/emailservice created deployment.apps/adservice created service/adservice created deployment.apps/cartservice created service/cartservice created deployment.apps/recommendationservice created service/recommendationservice created deployment.apps/shippingservice created service/shippingservice created deployment.apps/currencyservice created service/currencyservice created  To ensure the Online Boutique application is running:\nGet Pods  Get Pods Output   kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-56585564cc-xclzj 1/1 Running 0 84s splunk-otel-collector-agent-hkshj 1/1 Running 0 84s svclb-frontend-external-c74n6 1/1 Running 0 53s currencyservice-747b74467f-xxrl9 1/1 Running 0 52s redis-cart-74594bd569-2jb6c 1/1 Running 0 54s adservice-6fb948b8c6-2xlrc 0/1 Running 0 53s recommendationservice-b5df8776c-sbt4h 1/1 Running 0 53s shippingservice-6d6f7b8d87-5lg9g 1/1 Running 0 53s svclb-loadgenerator-jxwct 1/1 Running 0 53s emailservice-9dd74d87c-wjdqr 1/1 Running 0 53s checkoutservice-8bcd56b46-bfj7d 1/1 Running 0 54s productcatalogservice-796cdcc5f5-vhspz 1/1 Running 0 53s paymentservice-6c875bf647-dklzb 1/1 Running 0 53s frontend-b8f747b87-4tkxn 1/1 Running 0 53s cartservice-59d5979db7-bqf64 1/1 Running 1 53s loadgenerator-57c8b84966-7nr4f 1/1 Running 3 53s  Info\nUsually it should only take around 1min 30secs for the pods to transition into a Running state.    3. Validate in the UI In the Splunk UI click on Infrastructure this will bring you to the Infrastructure Overview dashboard, then click on Kubernetes.\nUse the Cluster dropdown so select your cluster, you should see the new pods started and containers deployed.\nWhen you click on your cluster in the Splunk UI you should have a view that looks like below:\nIf you select the WORKLOADS tab again you should now see that there are a number of Deployments and ReplicaSets:\n 4. View Online Boutique The Online Boutique is viewable on port 81 of the EC2 instance’s IP address. The IP address is the one you used to SSH into the instance at the beginning of the workshop.\nOpen your web browser and go to http://{==EC2-IP==}:81/ where you will then be able to see the Online Boutique running.\n","categories":"","description":"","excerpt":" Deploy the Online Boutique application into Kubernetes (K3s) Verify …","ref":"/observability-workshop/v4.22/apm/docs/online-boutique/deploy/","tags":"","title":"Deploying the Online Boutique in K3s"},{"body":"In this workshop we will be role playing as an SRE team responsible for an application consisting of a handful of microservices.\nWe have started with our application with no observability capability.\nOur developers have added some log messages into the application as a way to get some visibility into the application. We have also deployed the Open Telemetry Collector, which is a vendor-agnostic collector that can receive, process, and send telemetry data. We have done minimal configuration on it, so it includes sending out-of-the-box metrics (CPU, memory, disk, etc.) and has also been configured to pass application logs as well.\nHere are a few views we get from this implementation:\n(INSERT: Picture of infrastructure IM)\n(INSERT: Picture of Log Observer Logs)\nGo to Event 1\n","categories":"","description":"","excerpt":"In this workshop we will be role playing as an SRE team responsible …","ref":"/observability-workshop/v4.22/realworld/docs/initial_state/","tags":"","title":"Initial State"},{"body":"Install The Open Telemetry Collector The OpenTelemetry Collector is the core component of instrumenting infrastructure and applications. Its role is to collect and send:\n Infrastructure metrics (disk, cpu, memory, etc) Application Performance Monitoring (APM) traces Host and application logs  Splunk Observability Cloud offers wizards to walk you through the setup of the Collector on both your infrastrucutre and applications. To get to the wizard, click in the top left corner icon (the hamburger menu), then click on Data Setup\nYou’ll be taken to a short wizard where you will select some options. The default settings should work, no need to make changes. The wizard will output a few commands that need to be executed in the shell.\nHowever, if you have already completed the Splunk IMT workshop you can take advantage of the existing environment variables. Othwewise, create the ACCESS_TOKEN and REALM environment variables to use in the proceeding OpenTelemetry Collector install command. For instance, if your realm is us1, you would type export REALM=us1 and for eu0 type export REALM=eu0.\nExport Variables   export ACCESS_TOKEN=\u003creplace_with_O11y-Workshop-ACCESS_token\u003e export REALM=\u003creplace_with_splunk_realm\u003e  Delete any existing OpenTelemtry Collectors\nIf you have completed the Splunk IMT workshop, please ensure you have deleted the collector running in Kubernetes before continuing. This can be done by running the following command: helm delete splunk-otel-collector.   We can then go ahead and install the Collector:\ncurl -sSL https://dl.signalfx.com/splunk-otel-collector.sh \u003e /tmp/splunk-otel-collector.sh \u0026\u0026 \\ sudo sh /tmp/splunk-otel-collector.sh --realm $REALM -- $ACCESS_TOKEN --mode agent This command will download and setup the OpenTelemetry Collector. Once the install is completed, you can navigate to the Infrastructure page to see the data from your host, Infrastructure → My Data Center → Hosts.\nClick Add Filter    select host.name and type or select the hostname of your virtual machine. Once you see data flowing for your host, we are then ready to get started with the APM component.\n","categories":"","description":"","excerpt":"Install The Open Telemetry Collector The OpenTelemetry Collector is …","ref":"/observability-workshop/v4.22/pet-clinic/docs/imt/","tags":"","title":"Install the OpenTelemetry Collector"},{"body":"The Splunk OpenTelemetry Workshop will teach you how to level up your Observability practice by using the OpenTelemetry Collector and APM Instrumentation to emit industry standard telemetry from your infrastructure and applications.\nSource repo is here: https://github.com/signalfx/otelworkshop\nRequirements Audience  Intermediate and advanced developers, devops, and SREs who have already set up their Splunk Observability Cloud account and have tried out integrations and dashboards Skill level should include setting up and troubleshooting Linux and Kubernetes environments as well as deploying applications written in current versions of Java, Python, Node. This workshop is designed to run as a single user, or be run in a small group (upwards of 6) along with a leader guiding the group. If running as a group, a pre-workshop prep call is necessary to ensure that all group members can spin up and/or access an Ubuntu Linux lab environment. Details are below.  Prerequisites  Completion of Splunk Observability Workshop which trains on using metrics/APM and charts/dashboards/alerts or equivalent devops/SRE skills Splunk Observability Cloud Account Ability to use a multi-terminal IDE i.e. Microsoft Visual Studio Code or equivalent Ability to spin up a VM or access a host with a Debian Linux environment with the following specs: Debian (i.e. Ubuntu) Linux environment with minimum 12G RAM and 20G disk w/ lightweight Kubernetes (Rancher k3s) installed OR your own k8s cluster. The Prep section has some tools to help build a local or AWS environment.  Document Conventions Variables from your Splunk Observability account are displayed like this: YOURVARIABLEHERE. I.e. to change your REALM to us1 change api.YOURREALMHERE.signalfx.com to api.us1.signalfx.com\n K8s = Kubernetes K3s = a lightweight Kubernetes from Rancher signalfx = Splunk Observability domain name/endpoint/technology name otel = OpenTelemetry  Workshop Agenda  (Optional) Build a local Lab Environment Ubuntu Sandbox on Mac or Windows OpenTelemetry Collector and APM Labs  Linux Host  Set up OpenTelemetry Collector Agent on a Linux Host OpenTelemetry APM Instrumentation on Java, Python, and Node apps   Kubernetes (k8s) Click to start at k8s labs  Set up OpenTelemetry Collector Agent on a k8s cluster OpenTelemetry APM Instrumentation on k8s on Java, Python k8s pods Manual APM Instrumentation for Java JVM Metrics Span processing with redaction example APM for Istio service mesh OpenTelemetry Collector configuration / troubleshooting Prometheus scraping and custom metrics Collectd: receive metrics from any platform Troubleshooting the Collector   Option: Docker workshop w/ Otel Collector and APM Examples    Disclaimers  This is not product documentation: Click for Official documentation Breaking changes to OpenTelemetry and Splunk services may occur- please submit issues on the GitHub repo if any are encountered These examples are not commercial products and are for experimentation and educational purposes only  ","categories":"","description":"","excerpt":"The Splunk OpenTelemetry Workshop will teach you how to level up your …","ref":"/observability-workshop/v4.22/otelw/introduction/","tags":"","title":"Introduction"},{"body":" Online BoutiqueアプリケーションをKubernetes(K3s)にデプロイします アプリケーションが動作していることを確認します Locustを使って人工的なトラフィックを生成します UI で APM のメトリクスを見ましょう   1. Online Boutiqueをデプロイする Online BoutiqueアプリケーションをK3sにデプロイするには、以下のデプロイメントを適用します。\nDeploy Online Boutique  Deployment Output   cd ~/workshop kubectl apply -f apm/microservices-demo/k8s/deployment.yaml  deployment.apps/checkoutservice created service/checkoutservice created deployment.apps/redis-cart created service/redis-cart created deployment.apps/productcatalogservice created service/productcatalogservice created deployment.apps/loadgenerator created service/loadgenerator created deployment.apps/frontend created service/frontend created service/frontend-external created deployment.apps/paymentservice created service/paymentservice created deployment.apps/emailservice created service/emailservice created deployment.apps/adservice created service/adservice created deployment.apps/cartservice created service/cartservice created deployment.apps/recommendationservice created service/recommendationservice created deployment.apps/shippingservice created service/shippingservice created deployment.apps/currencyservice created service/currencyservice created   Online Boutique アプリケーションが起動していることを確認するには:\nGet Pods  Get Pods Output   kubectl get pods  NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-56585564cc-xclzj 1/1 Running 0 84s splunk-otel-collector-agent-hkshj 1/1 Running 0 84s svclb-frontend-external-c74n6 1/1 Running 0 53s currencyservice-747b74467f-xxrl9 1/1 Running 0 52s redis-cart-74594bd569-2jb6c 1/1 Running 0 54s adservice-6fb948b8c6-2xlrc 0/1 Running 0 53s recommendationservice-b5df8776c-sbt4h 1/1 Running 0 53s shippingservice-6d6f7b8d87-5lg9g 1/1 Running 0 53s svclb-loadgenerator-jxwct 1/1 Running 0 53s emailservice-9dd74d87c-wjdqr 1/1 Running 0 53s checkoutservice-8bcd56b46-bfj7d 1/1 Running 0 54s productcatalogservice-796cdcc5f5-vhspz 1/1 Running 0 53s paymentservice-6c875bf647-dklzb 1/1 Running 0 53s frontend-b8f747b87-4tkxn 1/1 Running 0 53s cartservice-59d5979db7-bqf64 1/1 Running 1 53s loadgenerator-57c8b84966-7nr4f 1/1 Running 3 53s   Info\n通常、ポッドがRunning状態に移行するのに1分30秒程度かかります。    3. UIで検証する 左上のハンバーガーメニューから、Infrastructure → Kubernetes をクリックします。\nClusterのドロップダウンを使用してクラスタを選択すると、新しいポッドが開始され、コンテナがデプロイされていることが確認できます。\nSplunk UI で Cluster をクリックすると、次のような画面が表示されているはずです。\nもう一度 WORKLOADS タブを選択すると、いくつかのデプロイメントとレプリカセットがあることがわかるはずです。\n 4. Online Boutique を閲覧する Online Boutique は、EC2インスタンスのIPアドレスの81番ポートで閲覧できます。このIPアドレスは、ワークショップの冒頭でインスタンスにSSH接続したときに使用したものと同じIPアドレスです。\nウェブブラウザを開き、 http://{==EC2-IP==}:81/ にアクセスすると、Online Boutique が起動しているのが確認できます。\n","categories":"","description":"","excerpt":" Online BoutiqueアプリケーションをKubernetes(K3s)にデプロイします アプリケーションが動作していることを確認し …","ref":"/observability-workshop/v4.22/ja/apm/docs/online-boutique/deploy/","tags":"","title":"K3s環境にOnline Boutiqueをデプロイする"},{"body":" NGINX ReplicaSet を K3s クラスタにデプロイし、NGINX デプロイメントのディスカバリーを確認します。 負荷テストを実行してメトリクスを作成し、Splunk Observability Cloudにストリーミングすることを確認します！   1. NGINX の起動 Splunk UI で WORKLOADS タブを選択して、実行中の Pod の数を確認します。これにより、クラスタ上のワークロードの概要がわかるはずです。\nデフォルトの Kubernetes Pod のうち、ノードごとに実行されている単一のエージェントコンテナに注目してください。この1つのコンテナが、このノードにデプロイされているすべての Pod とサービスを監視します！\n次に、MAP タブを選択してデフォルトのクラスタノードビューに戻し、再度クラスタを選択します。\nMultipass または AWS/EC2 のシェルセッションで、nginx ディレクトリに移動します。\nChange Directory   cd ~/workshop/k3s/nginx   2. NGINXのデプロイメント作成 NGINX の configmap1 を nginx.conf ファイルを使って作成します。\nKubectl Configmap Create  Kubectl Create Configmap Output   kubectl create configmap nginxconfig --from-file=nginx.conf configmap/nginxconfig created  続いて、デプロイメントを作成します。\nKubectl Create Deployment  Kubectl Create Deployment Output   kubectl create -f nginx-deployment.yaml deployment.apps/nginx created service/nginx created  次に、NGINXに対する負荷テストを作成するため、 Locust2 をデプロイします。\nKubectl Create Deployment  Kubectl Create Deployment Output   kubectl create -f locust-deployment.yaml deployment.apps/nginx-loadgenerator created service/nginx-loadgenerator created  デプロイメントが成功し、Locust と NGINX Pod が動作していることを確認しましょう。\nSplunk UI を開いていれば、新しい Pod が起動し、コンテナがデプロイされているのがわかるはずです。\nPod が実行状態に移行するまでには 20 秒程度しかかかりません。Splunk UIでは、以下のようなクラスタが表示されます。\nもう一度 WORKLOADS タブを選択すると、新しい ReplicaSet と NGINX 用のデプロイメントが追加されていることがわかります。\n これをシェルでも検証してみましょう。\nKubectl Get Pods  Kubectl Get Pods Output   kubectl get pods NAME READY STATUS RESTARTS AGE splunk-otel-collector-k8s-cluster-receiver-77784c659c-ttmpk 1/1 Running 0 9m19s splunk-otel-collector-agent-249rd 1/1 Running 0 9m19s svclb-nginx-vtnzg 1/1 Running 0 5m57s nginx-7b95fb6b6b-7sb9x 1/1 Running 0 5m57s nginx-7b95fb6b6b-lnzsq 1/1 Running 0 5m57s nginx-7b95fb6b6b-hlx27 1/1 Running 0 5m57s nginx-7b95fb6b6b-zwns9 1/1 Running 0 5m57s svclb-nginx-loadgenerator-nscx4 1/1 Running 0 2m20s nginx-loadgenerator-755c8f7ff6-x957q 1/1 Running 0 2m20s   3. Locust の負荷テストの実行 Locust はオープンソースの負荷テストツールで、EC2 インスタンスの IP アドレスの8080番ポートで Locust が利用できるようになりました。Webブラウザで新しいタブを開き、http://{==EC2-IP==}:8080/にアクセスすると、Locust が動作しているのが確認できます。\nSpawn rate を 2 に設定し、Start Swarming をクリックします。\nこれにより、アプリケーションに緩やかな連続した負荷がかかるようになります。\nサイドメニューから Dashboards → NGINX → NGINX Servers を選択して、UIにメトリクスが表示されていることを確認します。さらに Overrides フィルターを適用して、 k8s.cluster.name: に、ターミナルの　echo $(hostname)-k3s-cluster で返されるクラスタの名前を見つけます。\n  ConfigMap とは、キーと値のペアで非機密データを保存するために使用される API オブジェクトです。Pod は、環境変数、コマンドライン引数、またはボリューム内の構成ファイルとして ConfigMap を利用することができます。ConfigMap を使用すると、環境固有の構成をコンテナイメージから切り離すことができるため、アプリケーションの移植が容易になります。 ↩︎\n Locust とは？ . ↩︎\n   ","categories":["IMT"],"description":"","excerpt":" NGINX ReplicaSet を K3s クラスタにデプロイし、NGINX デプロイメントのディスカバリーを確認します。 負荷テストを …","ref":"/observability-workshop/v4.22/ja/imt/docs/gdi/nginx/","tags":["NGNIX"],"title":"K3s に NGINX をデプロイする"},{"body":"Each step should be performed in a separate terminal window.\nMake sure your Ubuntu environment was prepared properly as described in the Preparation section.\nConfigure Environment Variables for Otel and Run Python Flask Server Open the first terminal window in your Linux instance and set up environment and run Python Flask server using auto-instrumentation:\n!!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the run-server.sh script below and add your initials to the environment i.e. change:\nexport OTEL_RESOURCE_ATTRIBUTES=deployment.environment=apm-workshop\nto export OTEL_RESOURCE_ATTRIBUTES=deployment.environment=sjl-apm-workshop\ncd ~/otelworkshop/host/python source run-server.sh You will see the server startup text when this is run.\nRun Python Client Application Open a new terminal window in your Linux instance and run the Python client to sent POST requests to the Flask server:\nRun the client Python app via the splunk-py-trace command to send requests to the Flask server:\n!!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the run-client.sh script below and add your initials to the environment i.e. change:\nexport OTEL_RESOURCE_ATTRIBUTES=deployment.environment=apm-workshop\nto export OTEL_RESOURCE_ATTRIBUTES=deployment.environment=sjl-apm-workshop\ncd ~/otelworkshop/host/python source run-client.sh The python-requests.py client will make calls to the flask server with a random short sleep time.\nYou can stop the requests with ++ctrl+c++\nValidate span are being sent Open a new terminal window in your Linux instance to check OpenTelemetry Collector Statistics to see that spans are being sent.\nlynx localhost:55679/debug/tracez will show the metrics and spans being gathered and sent by the Collector.\nLynx is a text browser that was installed during with the setup-tools. Enabling a web browser to access your environment will allow for a full web GUI.\nAPM Dashboard Traces / services will now be viewable in the APM dashboard. A new service takes about 90 seconds to register for the first time, and then all data will be available in real time.\nThe Environment pulldown will let you see the APM map associated with your individual environment that you set with your initials if this was done earlier.\nAdditionally span IDs will print in the terminal where flask-server.py is running. You can use ++ctrl+c++ to stop the requests and server any time.\nThe Python server application will be called: py-otel-flask-server and the client will be called py-otel-client.\nNavigate to Splunk Overvability -\u003e APM\nService map of this python demo\nClick on one of the peaks in the grey graph within “Services By Latency (P90)” on the right hand side and then click the trace to see spans. Also try out Tag Spotlight to see how application operations are broken down in a granular way. You can also try the Tags menu on top to search for a single trace or group of traces by key:value.\nTo learn more about traces and spans see the Splunk APM documentation\nWhere is the OpenTelemetry Instrumentation? The run-server.sh and run-client.sh scripts set up the environment variables for OpenTelemetry and invoke the Python auto instrumentation:\nspluk-py-trace is the auto instrumenting function that runs Python3 with the instrumentation that automatically emits spans from the Python app. No code changes are necessary. Splunk Observability Cloud has a Data Setup Wizard to guide through instrumentation setup.\nOpenTelemetry repo for Python is here.\n!!! important Leave the Flask server running you’ll need need this process for the next client examples in the workshop.\n","categories":"","description":"","excerpt":"Each step should be performed in a separate terminal window.\nMake sure …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_single_host/python/","tags":"","title":"Python- Deploy HTTP Server and Client"},{"body":"Splunk APM is a NoSample™ Full-fidelity application performance monitoring and troubleshooting solution for cloud-native, microservices-based applications.\nBy collecting all traces, instead of a sampled subset, no anomaly goes undetected. Whether a user experiences an error or longer-than-usual latency, you’ll be able to know and act on it within seconds. Furthermore, not all bad behavior results in errors — as your developers create new applications they need to know whether their canary releases provide the expected results. Only by collecting all trace data will you ensure that your cloud-native applications behave the way they are supposed to.\nInfrastructure and application performance are interdependent. To see the full picture, Splunk APM provides seamless correlation between cloud infrastructure and the microservices running on top of it. If your application acts out because of memory leakage, a noisy neighbor container or any other infrastructure-related issue, Splunk will let you know. To complete the picture, in-context access to Splunk logs and events enable deeper troubleshooting and root-cause analysis.\n","categories":"","description":"","excerpt":"Splunk APM is a NoSample™ Full-fidelity application performance …","ref":"/observability-workshop/v4.22/apm/","tags":"","title":"Introduction"},{"body":"Splunk APM は、クラウドネイティブなマイクロサービスベースのアプリケーション向けの NoSample™ で Full-fidelity なアプリケーションパフォーマンスモニタリングおよびトラブルシューティングソリューションです。\nサンプリングされた部分的な情報ではなく、すべてのトレースを収集することで、異常が検出されないことはありません。ユーザーがエラーを経験しても、通常より長いレイテンシーを経験しても、数秒以内にそれを知り、対処することができます。ときに、悪い動作がエラーとして扱われないこともあります。開発者が新しいアプリケーションを作成する際には、そのカナリアリリースが期待通りの結果をもたらすかどうかを知る必要があります。すべてのトレースデータを収集して、初めて、クラウドネイティブアプリケーションが想定通り動作していることを確信できるようになります。\nインフラとアプリケーションのパフォーマンスは相互に依存しています。全体像を把握するために、Splunk APM はクラウドのインフラとその上で動作するマイクロサービスをシームレスに相関付けます。メモリリーク、ノイズの多い隣のコンテナ、その他のインフラ関連の問題が原因でアプリケーションが動作した場合、Splunk がすぐに知らせてくれます。さらに、Splunk のログやイベントにインコンテキストでアクセスすることで、より詳細なトラブルシューティングや根本原因の分析が可能になります。\n","categories":"","description":"","excerpt":"Splunk APM は、クラウドネイティブなマイクロサービスベースのアプリケーション向けの NoSample™ …","ref":"/observability-workshop/v4.22/ja/apm/","tags":"","title":"はじめに"},{"body":" APM の概要 - RED メトリクス サービスマップを利用する タグスポットライトの紹介 トレースの例 Infrastructure Monitoringとのリンク  1. トレースとスパンについて トレースは、同じトレースIDを共有するスパンの集合体であり、アプリケーションとその構成サービスが処理する固有のトランザクションを表します。\n各スパンには、そのスパンでキャプチャされた操作を表す名前と、その操作がどのサービス内で行われたかを表すサービス名があります。\nさらにスパンは、その親として別のスパンを参照することができ、そのトランザクションを処理するために実行されたトレースでキャプチャされた処理の関係を定義します。\n各スパンには、キャプチャされたメソッド、オペレーション、コードブロックに関する以下のような多くの情報が含まれています。例えば:\n 処理名 処理の開始時間（マイクロ秒単位の精度） 処理の実行時間（マイクロ秒単位の精度） 処理が行われたサービスの論理名 処理が行われたサービスインスタンスのIPアドレス  2. サービスマップ サービスマップの paymentservice をクリックし、paymentservice の下にある内訳のドロップダウンフィルタから version を選択します。これにより、カスタムスパンタグの version でサービスマップがフィルタリングされます。\nこれで、サービスマップが以下のスクリーンショットのように更新され、paymentservice の異なるバージョンが表示されていることがわかります。\n3. タグスポットライト 画面の右側にある Tag Spotlight をスクロールダウンし、ドロップダウンから Top Across All Indexed Tags を選択します。選択したら、下のスクリーンショットにあるように矢印をクリックします。\nタグスポットライトのページが表示されます。このページでは、アプリケーションの上位のタグと、それに対応するエラー率や秒間リクエスト数を確認できます。\nversion スパンタグでは、バージョン 350.10 のエラー率が100%であることがわかります。また、tenant.level スパンタグでは、3つのテナント（Gold、Silver、Bronze）すべてにエラーがあることがわかります。\nタグスポットライトのページはインタラクティブに、目的のタグをクリックするだけでフィルタとしてタグを追加することができます。tenant.level の下の gold をクリックして、フィルターとして追加します。これを行うと、ページには tenant.level が gold のデータのみが表示されます。\nタグスポットライトは、データを分析して傾向を見極めるのに非常に便利です。Gold Tenantでは、リクエストの総数のうち55件がエラーであることがわかります。\nこれをバージョンタグと関連付けると、バージョン 350.10 が55件、バージョン 350.9 が17件のリクエストに対応していることがわかります。つまり、バージョン 350.10 を経由したリクエストは、すべてエラー状態になったということになります。\npaymentservice のバージョン 350.10 からのすべてのリクエストがエラーになるというこの理論をさらに検証するために、タグセレクタを使用して、フィルタを別のテナントに変更することができます。フィルターを gold テナントから silver テナントに変更します。\nここで、silver テナントのエラーのあるリクエスト数を見て、バージョン番号と相関させることで、同様の分析を行うことができます。silver テナントのエラー数は、バージョン 350.10 のリクエスト数と一致していることに注目してください。\nタグスポットライトでは、秒間リクエスト数やエラー率だけでなく、サービスごとのレイテンシーも見ることができます。これを行うには、レイテンシーボタンを選択し、silver テナントタグを削除することで、すべての paymentservice のレイテンシーを確認することができます。\n右端の Clear All の下にある X ボタンを押して、サービスマップに戻りましょう。\n4. サンプルトレース 「Service Requests \u0026 Errors」グラフのピンク色の線上をクリックします。選択すると、サンプルトレースのリストが表示されます。サンプルトレースの1つをクリックしてください\nスパンとともに、選択したトレースの全体が表示されます。エラーが発生したスパンは、その横に赤い！マークが表示されます。\nこれらをクリックすると、そのスパンが展開され、関連するメタデータやエラーの詳細が表示されます。このエラーが401エラーによるものであることがわかります。また、「テナント」や「バージョン」などの有用な情報も表示されています。\n","categories":"","description":"**15 分**\n\nSplunk APMの使い方概要\n","excerpt":"**15 分**\n\nSplunk APMの使い方概要\n","ref":"/observability-workshop/v4.22/ja/apm/docs/using-splunk-apm/","tags":"","title":"Splunk APMを利用する"},{"body":" APM Overview - RED metrics Using the Service Map Introduction to Tag Spotlight Example Traces Contextual Links to Infra  1. Traces and Spans explained A trace is a collection of spans that share the same trace ID, representing a unique transaction handled by your application and its constituent services.\nEach span has a name, representing the operation captured by this span, and a service name, representing within which service the operation took place.\nAdditionally, spans may reference another span as their parent, defining the relationships between the operations captured in the trace that were performed to process that transaction.\nEach span contains a lot of information about the method, operation, or block of code that it captures, including:\n the operation name the start time of the operation with microsecond precision how long the operation took to execute, also with microsecond precision the logical name of the service on which the operation took place the IP address of the service instance on which the operation took place  2. Service Map Click on paymentservice in the service map and select version from the breakdown drop down filter underneath paymentservice. This will filter our service map by the custom span tag version.\nYou will now see the service map has been updated like the below screenshot to show the different versions of the paymentservice.\n3. Tag Spotlight On the right hand side of the screen scroll down on Tag Spotlight, ensure Top Across All Indexed Tags is selected in the dropdown click the full screen button as indicated in the screenshot below.\nThe Tag Spotlight Page will be displayed. From this page you can view the top tags in your application and their corresponding error rates and request rates.\nNote that for the version span tag it appears that version 350.10 has a 100% error rate and for our tenant.level span tag it shows that all three tenants (Gold, Silver \u0026 Bronze) have errors present.\nThe Tag Spotlight page is interactive and allows you to add a tag as a filter by simply clicking on your desired tag. Click on gold under tenant.level to add it as a filter. Once this is done the page will now only display data with gold as it’s tenant.level.\nTag Spotlight is very useful for analysing your data and spotting trends. We can see that for the Gold Tenant that out of the total number of requests, 55 of them are in error (this number will vary in your workshop).\nIf we correlate this to the version tag, we can see that version 350.10 served 55 requests and version 350.9 served 17 requests. This means that all of the requests that went through version 350.10 ended up in an error state.\nIn order to test this theory further that all of the requests from paymentservice version 350.10 result in an error, we can change our filter to another tenant by using the tag selector. Change your filter from gold tenant to silver tenant.\nNow we can perform a similar analysis by looking at the number of requests in error for the silver tenant and correlating that with the version number. Note the amount of errors for the silver tenant match the amount of requests for version 350.10.\nTag Spotlight not only allows you to look at request and error rates but also at the latency per service. In order to do this just select the latency button and remove your Silver Tenant Tag so that you can see the latency for all of the Payment Service.\nGo back to your service map by pressing the X button on the far right underneath Clear All.\nClick anywhere on the pink line in the ‘Services by Error Rate’ graph in the top right hand corner. Once selected you should see a list of example traces. Click on one of the example traces with an Initiating Operation of frontend: POST /cart/checkout.\n4. Example Trace You should now see the entire trace along with the spans for the example trace that was selected. Spans which have errors are indicated by a red exclamation mark beside it. If you have a number such as x6 in a grey box, click it to expand the compacted paymentservice spans.\nNow click one of the paymentservice spans with the red exclamation mark to expand it and see the associated metadata and some error details. Note that we are able to see that this error is caused by a 401 error and other useful information such as ‘tenant’ and ‘version’ is also displayed.\nSo we now know that the error is caused by an Invalid Request but we don’t know what exact request. At the bottom of the page you should see a contextual link to Logs, clink on this link to view the logs associated with this span.\nYou should now be looking at a Log Observer dashboard simialar to the image below.\nWe can use the filter to display only the error logs. Click on ERROR in the top right hand corner, then Add to filter\nYou should now have a shorter list of log entries which have a severity of ERROR\nSelect any of the entries to view the details. We can now see how the error was caused by the use of an Invalid API Token that our developers have accidentally pushed to production!\nCongratulations, you have now completed this APM Workshop.\n","categories":"","description":"**15 minutes**\n\nAn overview of how to use Splunk APM\n","excerpt":"**15 minutes**\n\nAn overview of how to use Splunk APM\n","ref":"/observability-workshop/v4.22/apm/docs/using-splunk-apm/","tags":"","title":"Using Splunk APM"},{"body":" チャートからディテクターを作成する アラート条件を設定する プレフライトチェックを実行する ミューティングルールを設定する   1. はじめに Splunk Observability Cloud では、ディテクター（検出器）、イベント、アラート、通知を使用して、特定の条件が満たされたときに情報を提供することができます。たとえば、CPU使用率が95%に達したときや、同時ユーザー数が制限値に近づいてAWSインスタンスを追加で立ち上げなければならない可能性があるときに、Slack チャンネルや Ops チームのメールアドレスにメッセージを送信したいと考えるでしょう。\nこれらの条件は1つまたは複数のルールとして表現され、ルール内の条件が満たされたときにアラートが発生します。ディテクターに含まれる個々のルールは、重要度に応じてラベル付けされています。Info、Warning、Minor、Major、Criticalとなっています。\n2. ディテクターの作成 Dashboards で、前のモジュールで作成した カスタムダッシュボードグループ をクリックし、ダッシュボードの名前をクリックします。\nこのチャートから、新しいディテクターを作成していきます。チャートが表示されたら、チャート上のベルのアイコンをクリックし、 New Detector From Chart をクリックします。\nDetector Name の横にあるテキストフィールドで、提案されたディテクター名の最初に、あなたののイニシャル を追加してください。\nディテクターの名前を決める\n提案されたディテクター名の前に自分のイニシャルを追加することをお忘れなく。\n次のような名前にしてください: XYZ’s Latency Chart Detector    Create Alert Rule    をクリックします。\nDetector ウィンドウの Alert signal の中で、アラートするシグナルは Alert on 欄に青のベルが表示されています。このベルは、どのシグナルがアラートの生成に使用されているかを示しています。\n Proceed to Alert Condition    をクリックします。\n 3. アラート条件の設定 Alert condition で、Static Threshold をクリックし、 Proceed to Alert Settings    をクリックしてください。\nAlert Settings で、 Threshold フィールドに値 290 を入力します。同じウィンドウで、右上の Time を過去1日（-1d）に変更します。\n 4. プリフライトチェックの警告 5秒後にプリフライトチェックが行われます。Estimated alert count に、アラート回数の目安が表示されます。現在のアラート設定では、1日に受信するアラート量は3となります。\nプリフライトチェックについて\nアラート条件を設定すると、UIは現在の設定に基づいて、右上に設定された時間枠（ここでは過去1日）の中で、どのくらいのアラートが発生するかを予測します。\nすぐに、プラットフォームは現在の設定でシグナルの分析を開始し、「プリフライトチェック」と呼ばれる作業を行います。これにより、プラットフォーム内の過去のデータを使用してアラート条件をテストし、設定が妥当であり、誤って大量のアラートを発生させないようにすることができます。Splunk Observability Cloud を使用してのみ利用できるシンプルかつ非常に強力な方法で、アラートの設定から推測作業を取り除くことができます。\nディテクターのプレビューについての詳細は、こちらのリンクをご覧ください。 Preview detector alerts    Proceed to Alert Message    をクリックし、次に進みます。\n 5. アラートメッセージ Alert message の Severity で Major を選択します。\n Proceed to Alert Recipients    をクリックします。\nAdd Recipient（受信者の追加）をクリックし、最初の選択肢として表示されているメールアドレスをクリックします。\n通知サービス\nこれは、そのメールアドレスを入力したときと同じです。または、E-mail… をクリックして別のメールアドレスを入力することもできます。\nこれは、予め用意されている多くの Notification Services の一例です。全てを確認するには、トップメニューの Integrations タブに移動し、Notification Services を参照してください。\n   6. アラートの有効化  Proceed to Alert Activation    をクリックします。\nActivivate… で Activate Alert Rule    をクリックします。\nアラートをより早く取得したい場合は、Alert Settings をクリックして、値を 290 から **280**に下げてみてください。\nTime を -1h に変更すると、過去1時間のメトリクスに基づいて、選択した閾値でどれだけのアラートを取得できるかを確認できます。\nナビバーにある ボタンをクリックして、その後 Detectors をクリックすると、ディテクターの一覧が表示されます。あなたのイニシャルでフィルタして、作成したディテクターを確認しましょう。表示されない場合は、ブラウザをリロードしてみてください。\nおめでとうございます。最初のディテクターが作成され、有効化されました。\n","categories":"","description":"","excerpt":" チャートからディテクターを作成する アラート条件を設定する プレフライトチェックを実行する ミューティングルールを設定する   1. はじ …","ref":"/observability-workshop/v4.22/ja/imt/docs/detectors/creating/","tags":"","title":"ディテクターを利用する"},{"body":".NET CORE example uses the .NET http client to get non-responding URL so makes valid traces with 403 status code\nContainerized with these instructions from Microsoft\nFor .NET Core 2.1\ncd ~/otelworkshop/k8s/dotnet21 Deploy:\nsource deploy-client.sh Delete deployment:\nsource delete-all.sh ","categories":"","description":"","excerpt":".NET CORE example uses the .NET http client to get non-responding URL …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_k8s/examples/dotnet21/","tags":"","title":".Net Core 2.1"},{"body":"Spring PetClinic Application Prerequisites\nSplunk run workshop where instance is provided OR Self led workshop on own system / multipass instance For own system you will need the following on your machine:\n Java installed Port 8080 open inbound/outbound    First thing we need to setup APM is… well, an application. For this exercise, we will use the Spring Pet Clinic application. This is a very popular sample java application built with Spring framework (Springboot).\nNext we will clone the PetClinic repository, then we will compile, build, package and test the application.\ngit clone https://github.com/spring-projects/spring-petclinic Change into the spring-petclinic directory:\ncd spring-petclinic Start a MySQL database for Pet Clinic to use:\ndocker run -d -e MYSQL_USER=petclinic -e MYSQL_PASSWORD=petclinic -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=petclinic -p 3306:3306 mysql:5.7.8 Next, run the maven command to compile/build/package Pet Clinic:\n./mvnw package -Dmaven.test.skip=true  Information\nThis will take a few minutes the first time you run, maven will download a lot of dependencies before it actually compiles the app. Future executions will be a lot shorter.   Once the compilation is complete, you can run the application with the following command:\njava -jar target/spring-petclinic-*.jar --spring.profiles.active=mysql You can validate if the application is running by visiting http://\u003cVM_IP_ADDRESS\u003e:8080.\nOnce your validation is complete you need to stop the application by pressing Ctrl-c so that we can auto-instrument the application with Splunk OpenTelemetry.\nAuto-Instrument with Splunk OpenTelemetry Now that the application is running, it is time to setup the APM instrumentation. The Splunk APM product uses OpenTelemetry libraries to instrument the applications https://github.com/signalfx/splunk-otel-java. The Otel-Java library will instrument code to generate metrics and spans/traces that are reported to the OpenTelemetry Collector we installed previously.\nLet’s continue the process by visiting the Splunk Observability Cloud UI again Hamburguer Menu → Data Setup then Monitor applications → Java (Provide Traces) → Add Intergration.\nThe APM Instrumentation Wizard will show a few options for you to select, things like service name, environment, etc. In this scenario we are using:\n Service Name: \u003chostname\u003e-petclinic-service Endpoint: http://localhost:4317 Environment: \u003chostname\u003e-petclinic-env AlwaysOn Profiling: Yes Collect Metrics: Yes Kubernetes: No Legacy Agent: No  At the end of the wizard, you’ll be given a set of commands to run (similar to the Splunk IM instructions) make sure you are in the spring-petclinic directory!\nDownload the latest version of the Splunk Open Telemetry Java Instrumentation library:\ncurl -L https://github.com/signalfx/splunk-otel-java/releases/latest/download/splunk-otel-javaagent-all.jar -o splunk-otel-javaagent.jar Set the following environment variables:\nexport OTEL_SERVICE_NAME=$(hostname)-petclinic-service export OTEL_RESOURCE_ATTRIBUTES=deployment.environment=$(hostname)-petclinic-env,version=0.314 export OTEL_EXPORTER_OTLP_ENDPOINT='http://localhost:4317' Lastly, we will run our application adding the -javaagent tag in front of the command\njava -javaagent:./splunk-otel-javaagent.jar \\ -Dsplunk.profiler.enabled=true \\ -Dsplunk.metrics.enabled=true \\ -jar target/spring-petclinic-*-SNAPSHOT.jar \\ --spring.profiles.active=mysql Let’s go visit our application again to generate some traffic http://\u003cVM_IP_ADDRESS\u003e:8080. Click around, generate errors, add visits, etc. Then you can visit the Splunk APM UI and examine the application components, traces, etc. Hamburguer Menu → APM → Explore.\n","categories":"","description":"","excerpt":"Spring PetClinic Application Prerequisites\nSplunk run workshop where …","ref":"/observability-workshop/v4.22/pet-clinic/docs/apm/","tags":"","title":"Auto-instrument Java app"},{"body":" Learn how to configure Muting Rules Learn how to resume notifications   1. Configuring Muting Rules There will be times when you might want to mute certain notifications. For example, if you want to schedule downtime for maintenance on a server or set of servers, or if you are testing new code or settings etc. For that you can use muting rules in Splunk Observability Cloud. Let’s create one!\nClick on the from the navbar and then click Detectors to see your list of active detectors. You can filter for yours if you want.\nIf you created a detector in Creating a Detector you can click on the three dots ... on the far right for that detector; if not, do that for another detector.\nFrom the drop-down click on Create Muting Rule…\nIn the Muting Rule window check Mute Indefinitely and enter a reason.\nImportant\nThis will mute the notifications permanently until you come back here and un-check this box or resume notifications for this detector.   Click Next and in the new modal window confirm the muting rule setup.\nClick on Mute Indefinitely    to confirm.\nYou won’t be receiving any email notifications from your detector until you resume notifications again. Let’s now see how to do that!\n 2. Resuming notifications To Resume notifications, click on Muting Rules, you will see the name of the detector you muted notifications for under Detector heading.\nClick on the thee dots ... on the far right, and click on Resume Notifications.\nClick on Resume    to confirm and resume notifications for this detector.\nCongratulations! You have now resumed your alert notifications!\n","categories":"","description":"","excerpt":" Learn how to configure Muting Rules Learn how to resume notifications …","ref":"/observability-workshop/v4.22/imt/docs/detectors/muting/","tags":"","title":"Working with Muting Rules - Lab Summary"},{"body":"In this section, we will create a custom Service in Splunk ITSI based on observations in the Splunk Observability Cloud Platform.\nAccess the Splunk Observability Cloud Platform  Visit the Splunk Observability Cloud Platform and log in with the credentials that have been provided to you. (In case of trouble, please don’t hesitate to reach out to us!) If everything went well, you can see the Home screen which looks like this:  On the menu on the left (you might have to expand it by clicking on the double arrow on the bottom left), choose Dashboards, locate the pre-build AWS EBS Dashboard, and select EBS Volumes.  Open the dashboard with the title Total Ops/Reporting Interval by locating the relating tile, clicking on the three little dots on the upper right of the tile, and then Open. Click on View SignalFlow. You should see the following:\nA = data('VolumeReadOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='A') B = data('VolumeWriteOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='B') Let’s change the SignalFlow to create our query in Splunk Enterprise :  data('VolumeReadOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='A'); data('VolumeWriteOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='B') Now go to Splunk Enterprise and open the Search \u0026 Reporting default app. Based on the SignalFlow statement, we can craft the following SPL Search Term:\n| sim flow query=\"data('VolumeReadOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='A'); data('VolumeWriteOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='B')\" Execute this search.\n Caution\nInternal note: If I run the below search, my line chart is empty. Is this supposed to be like this?   If you would like to see the results in a visualisation, you can append a timechart command to the search term with a “|”. The complete search term would then look something like this:\n| sim flow query=\"data('VolumeReadOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='A'); data('VolumeWriteOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').scale(60).sum().publish(label='B')\" | timechart max(VolumeReadOps) max(VolumeWriteOps) Let’s create our EBS service In order to create our own custom create our EBS service, let’s navigate to the Splunk IT Service Intelligence Add-On. Click on Configuration and select Service from the dropdown menu. Next, click on the green Create Service button and select the Create Service option from the dropdown menu.\nIn the following input form, provide a meaningful title (we choose EBS Volumes on the screenshot below), tick the Manually add service content radio button, and click on the Create button.\nIn the following view, select the KPIs tab, click the New dropdown menu, and select Generic KPI.\nWhat follows now is a Step-by-Step walkthrough. Please click the Next button which brings you to the Step 2 of 7: Source screen. Paste the command we just created in the textbox with the lable Search.\n| sim flow query=\"data('VolumeReadOps', filter=filter('namespace', 'AWS/EBS') and filter('stat', 'sum'), rollup='rate', extrapolation='zero').publish()\" | rename _value as VolumeReadOps In the field with the Threshold label, enter “VolumeReadOps”.\nNavigate to Step 7 of 7 by clicking on the Next buttons. You can leave everything between the first and seventh step as default. On the Step 7 of 7 dialog, manually add a threshold by clicking + Add Threshhold. (if nothing is happening on the Disk it might show close to 0 as a number)\nClick on the Finish button, and subsequently, when the dialog is closed, on the Save button on the lower right.\nLet’s attach our standalone to the AWS service. To do that, click on the Configurations dropdown menu and select the Services option. Locate the AWS service in the list of available services and open it by clicking on it. Click on the Service Dependencies tab, and click on the Add dependencies button.\nLocate our custom EBS Volume service by using the filter, select it, and select the ServiceHealthScore KPI.\nClick on the Done button. Verify that the service got added to the list of available serives and click the Save button on the lower right. In the pop-up dialogue, click Save and Enable.\ngo to Service open AWS serv go to Service Analyzer -\u003e Default Analyzer review what you built\nFinally, view the result of your work by clicking on the Service Analyzer tab and choose the Default Analyzer option. In the Service Analyzer view, click the Tree view button. You should see the Tree view with our newly created service:\n","categories":"","description":"","excerpt":"In this section, we will create a custom Service in Splunk ITSI based …","ref":"/observability-workshop/v4.22/itsi/docs/custom-service/","tags":"","title":"Custom Service"},{"body":"Create aws ecs create-cluster --cluster-name YOURCLUSTERNAMEHERE\naws ecs register-task-definition --cli-input-json file://YOURTASKDEFINITIONHERE.json\naws ecs create-service --cluster test-cluster --service-name signalfx-demo --task-definition signalfx-demo:1 \\\n--desired-count 1 --launch-type \"FARGATE\" \\\n--network-configuration \"awsvpcConfiguration={subnets=[subnet-YOURSUBNETIDHERE],securityGroups=[sg-YOURSECURITYGROUPIDHERE],assignPublicIp=ENABLED}\"\nMonitor aws ecs list-task-definitions\naws ecs list-clusters\naws ecs list-services --cluster YOURCLUSTERNAMEHERE\naws ecs describe-services --cluster YOURCLUSTERNAMEHERE --services YOURSERVICENAMEHERE\nCleanup aws ecs deregister-task-definition --task-definition FAMILYNAMEHERE:VERSIONHERE\naws ecs delete-service --cluster YOURCLUSTERNAMEHERE --service YOURSERVICENAMEHERE --force\naws ecs delete-cluster --cluster YOURCLUSTERNAMEHERE\necs-cli down --cluster YOURCLUSTERNAMEHERE --region YOURREGIONHERE\n","categories":"","description":"","excerpt":"Create aws ecs create-cluster --cluster-name YOURCLUSTERNAMEHERE\naws …","ref":"/observability-workshop/v4.22/otelw/appendix/ecs-cli-commands/","tags":"","title":"ECS-CLI Commands"},{"body":"1. Editing a chart Select the SAMPLE CHARTS dashboard and then click on the three dots ... on the Latency histogram chart, then on Open (or you can click on the name of the chart which here is Latency histogram).\nYou will see the plot options, current plot and signal (metric) for the Latency histogram chart in the chart editor UI.\nIn the Plot Editor tab under Signal you see the metric demo.trans.latency we are currently plotting.\nYou will see a number of Line plots. The number 18 ts indicates that we are plotting 18 metric time series in the chart.\nClick on the different chart type icons to explore each of the visualizations. Notice their name while you swipe over them. For example, click on the Heat Map icon:\nSee how the chart changes to a heat map.\nNote\nYou can use different charts to visualize your metrics - you choose which chart type fits best for the visualization you want to have.\nFor more info on the different chart types see Choosing a chart type.   Click on the Line chart type and you will see the line plot.\n2. Changing the time window You can also increase the time window of the chart by changing the time to Past 15 minutes by selecting from the Time dropdown.\n3. Viewing the Data Table Click on the Data Table tab.\nYou now see 18 rows, each representing a metric time series with a number of columns. These columns represent the dimensions of the metric. The dimensions for demo.trans.latency are:\n demo_datacenter demo_customer demo_host  In the demo_datacenter column you see that there are two data centers, Paris and Tokyo, for which we are getting metrics.\nIf you move your cursor over the lines in the chart horizontally you will see the data table update accordingly. If you click on one of the lines in the chart you will see a pinned value appear in the data table.\n Now click on Plot editor again to close the Data Table and let’s save this chart into a dashboard for later use!\n","categories":"","description":"","excerpt":"1. Editing a chart Select the SAMPLE CHARTS dashboard and then click …","ref":"/observability-workshop/v4.22/imt/docs/dashboards/editing/","tags":"","title":"Editing charts"},{"body":"1. Generate traffic The Online Boutique deployment contains a container running Locust that we can use to generate load traffic against the website to generate metrics, traces and spans.\nLocust is available on port 82 of the EC2 instance’s IP address. Open a new tab in your web browser and go to http://{==EC2-IP==}:82/, you will then be able to see the Locust running.\nSet the Spawn rate to be 2 and click Start Swarming, this will start a gentle continous load on the application.\n Now go to Dashboards → All Dashboards → APM Services → Service.\nFor this we need to know the name of your application environment. In this workshop all the environments use: {==hostname==}-apm-env.\nTo find the hostname, on the AWS/EC2 instance run the following command:\nEcho Hostname  Output Example   echo $(hostname)-apm-env  bdzx-apm-env   Select your environment you found in the previous step then select the frontend service and set time to Past 15 minutes.\nWith this automatically generated dashboard you can keep an eye out for the health of your service(s) using RED (Rate, Error \u0026 Duration) metrics. It provides various performance related charts as well as correlated information on the underlying host and Kubernetes pods (if applicable).\nTake some time to explore the various charts in this dashboard\n 2. Verify Splunk APM metrics In the left hand menu card click on APM this will bring you to the APM Overview dashboard:\nSelect the Explore on the right hand side and select your environment you found before and set the time to 15 minutes. This will show you the automatically generated Dependency/Service Map for the Online Boutique application.\nIt should look similar to the screenshot below:\nThe legend at the bottom of the page explains the different visualizations in the Dependency/Service Map.\n Service requests, error rate and root error rate. Request rate, latency and error rate  Also in this view you can see the overall Error and Latency rates over time charts.\n3. OpenTelemetry Dashboard Once the Open Telemetery Collector is deployed the platform will automatically provide a built in dashboard display OpenTelemetry Collector metrics.\nFrom the top left hamburger menu, select Dashboards → OpenTelemetry Collector, scroll all the way to the bottom of the page and validate metrics and spans are being sent:\n4. OpenTelemetry zpages To debug the traces being sent you can use the zpages extension. zpages are part of the OpenTelemetry collector and provide live data for troubleshooting and statistics. They are available on port 55679 of the EC2 instance’s IP address. Open a new tab in your web browser and enter in http://{==EC2-IP==}:55679/debug/tracez, you will then be able to see the zpages output.\nAlternatively, from your shell prompt you can run a text based browser:\nLynx Command   lynx http://localhost:55679/debug/tracez   ","categories":"","description":"","excerpt":"1. Generate traffic The Online Boutique deployment contains a …","ref":"/observability-workshop/v4.22/apm/docs/online-boutique/locust/","tags":"","title":"Generate traffic using Locust"},{"body":"","categories":"","description":"**15 minutes**\n","excerpt":"**15 minutes**\n","ref":"/observability-workshop/v4.22/imt/docs/gdi/","tags":"","title":"Get Data In"},{"body":"Make sure that you still have the Python Flask server from the Python Lab running. If you accidentally shut it down follow steps from the Python lab to restart the Python Flask server.\nStart in the Java example directory Open a new terminal window\ncd ~/otelworkshop/host/java Download Otel Java Instrumentation Download Splunk OpenTelemetry Java Auto-instrumentation to /opt:\nsource install-java-otel.sh Run the Java HTTP requests client Run the Java client example which uses OKHTTP requests to the Python Flask Server:\n!!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the run-client.sh script below and add your initials to the environment i.e. change:\nexport OTEL_RESOURCE_ATTRIBUTES=deployment.environment=apm-workshop\nto export OTEL_RESOURCE_ATTRIBUTES=deployment.environment=sjl-apm-workshop\nsource run-client.sh You will see requests printed to the terminal.\nAPM Dashboard Traces/services will now be viewable in the APM dashboard. A new service takes about 90 seconds to register for the first time, the Python and n all data will be available in real time.\nAdditionally the requests made by Java will print in the terminal where flask-server.py is running. You can use ++ctrl+c++ to stop the requests and server any time.\nYou should now see a new Java requests service alongside the Python one.\nWhere is the OpenTelemetry Instrumentation? In the run-client.sh script the java command:\njava \\ -Dexec.executable=\"java\" \\ -Dotel.resource.attributes=service.name=java-otel-client,deployment.environment=apm-workshop \\ -javaagent:/opt/splunk-otel-javaagent.jar \\ -jar ./target/java-app-1.0-SNAPSHOT.jar The splunk-otel-javaagent.jar file is the automatic OpenTelemetry instrumentation that will emit spans from the app. No code changes are necessary! The otel. resources set up the service name Aand environment. Config details can be found here\nSplunk’s OpenTelmetry autoinstrumentation for Java is here\n","categories":"","description":"","excerpt":"Make sure that you still have the Python Flask server from the Python …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_single_host/java/","tags":"","title":"Java- Deploy HTTP Client"},{"body":"1. トラフィックを発生させる Online Boutique のデプロイメントには、Locust が動作するコンテナが含まれており、これを使用してウェブサイトに対する負荷トラフィックを生成し、メトリクス、トレース、スパンを生成することができます。\nLocust は、EC2インスタンスのIPアドレスの82番ポートで利用できます。ウェブブラウザで新しいタブを開き、 http://{==EC2-IP==}:82/ にアクセスすると、Locust が動作しているのが確認できます。\nSpawn rate を 2 に設定し、Start Swarming をクリックすると、アプリケーションに緩やかな負荷がかかり続けます。\n それでは、Dashboards → APM Services → Service を開きましょう。\nこのためには、アプリケーションの Environment 名を知る必要があります。このワークショップでは、{==hostname==}-apm-env のような Environment 名で定義されています。\nホスト名を調べるには、AWS/EC2インスタンス上で以下のコマンドを実行します:\nEcho Hostname  Output Example   echo $(hostname)-apm-env  bdzx-apm-env   前のステップで見つけた Environment を選択し、「frontend」サービスを選択し、時間を「Past 15 minutes」に設定します。\nこの自動生成されたダッシュボードでは、RED (Rate, Error \u0026 Duration) メトリクスを使用して、サービスの状態を監視することができます。このダッシュボードでは、パフォーマンスに関連したさまざまなチャートのほか、基盤となるホストやKubernetesポッド（該当する場合）の相関情報も提供されます。\nダッシュボードの様々なチャートを見てみましょう。\n 2. Splunk APM のメトリクスを確認する 左上のハンバーガーメニューから「APM」をクリックすると、APM Overview ダッシュボードが表示されます。\n右側の Explore を選択し、先ほど見つけた Environment を選択し、時間を15分に設定します。これにより、自動的に生成されたOnline BoutiqueアプリケーションのDependency/Service Mapが表示されます。\n以下のスクリーンショットのように表示されます:\nページの下部にある凡例では、依存関係/サービスマップでの表記について説明しています。\n{: : .shadow .zoom}\n サービスリクエスト、エラーレート、ルートエラーレート。 リクエストレート、レイテンシー、エラーレート  また、このビューでは、全体的なエラー率とレイテンシー率のタイムチャートを見ることができます。\n3. OpenTelemetry ダッシュボード Open Telemetery Collector がデプロイされると、プラットフォームは自動的に OpenTelemetry Collector のメトリクスを表示するダッシュボードを作成します。\n左上のハンバーガーメニューから、 Dashboards → OpenTelemetry Collector を選択し、メトリクスとスパンが送信されていることを確認しましょう。\n4. OpenTelemetry zpages 送信されたトレースをデバッグするには、zpages 拡張機能を使用できます。zpages は OpenTelemetry Collector の一種で、トラブルシューティングや統計用のライブデータを提供します。これらは、EC2インスタンスのIPアドレスのポート 55679 で利用できます。Webブラウザで新しいタブを開き、 http://{==EC2-IP==}:55679/debug/tracez と入力すると、zpages の出力を見ることができます。\nまた、シェルプロンプトから、テキストベースのブラウザを実行することもできます。\nLynx Command   lynx http://localhost:55679/debug/tracez   ","categories":"","description":"","excerpt":"1. トラフィックを発生させる Online Boutique のデプロイメントには、Locust が動作するコンテナが含まれており、これを …","ref":"/observability-workshop/v4.22/ja/apm/docs/online-boutique/locust/","tags":"","title":"Locustでトラフィックを発生させる"},{"body":"Log in to your Splunk Observability account to identify token/realm For individuals and groups- allow 30-45 minutes of prep time to identify account credentials and prepare a lab environment. When running as a group we recommend doing a separate prep meeting before running the workshop together.\nCheck your Splunk Observability Account (your welcome email has this link) and identify your TOKEN and REALM - these are available in the profile menu in your Splunk Observability account. Note that the realm component i.e. us1 may be different for your account based on how you signed up.\nHow to find realm:\nSplunk Observability Menu -\u003e Your Name -\u003e Account Settings\nHow to find token:\nCreate Lab Environment Splunk Observability is for server environments. This workshop uses Ubuntu Linux as the lab server environment. You can use any Ubuntu platform - bare metal, VM, or cloud VM.\nRecommended Environment For optimal learning we recommend that you use a fresh cloud VM running Ubuntu with minimum 12GB RAM and 20GB disk space.\nSplunk provdes AWS EC2 setup/bootstrap scripts via Terraform. Also, cloud-init YAML files are available for Multipass.\nIf you chose your own Ubuntu machine, you can set it up with the Workshop software with this command:\nbash \u003c(curl -s https://raw.githubusercontent.com/signalfx/otelworkshop/master/setup-tools/ubuntu.sh) How To Build An Ubuntu VM on your Windows or Mac PC If you cannot procure a cloud VM, you can create an Ubuntu Linux environment on a Mac or PC and install the necessary software components\nMac OS Install Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Make sure Homebrew is fully upgraded:\nbrew upgrade Results should be at least 1.5:\nbrew --version We will use Multipass as a hypervisor for Mac:\nbrew cask install multipass If needed, further instructions are here. Do one final brew upgrade before spinning up VM:\nbrew upgrade Windows Follow Multipass Windows installation instructions\nLaunch Multipass Ubuntu VM Create your VM called “primary”:\nmultipass launch -n primary -d 20G -m 12G -c4 This will download Ubuntu and may take a few minutes the first time.\nBasic multipass commands:\n Shell into VM: multipass shell primary Exit VM: exit  To manage multipass VM:\n multipass stop primary stops the VM multipass delete primary deletes the VM from the hypervisor multipass purge purges created images but leaves the ubuntu template intace  Install OTel Workshop A bootstrap script will install everything needed and clone this repo.\nThis will take up to 10 minutes to execute- leave it running until complete.\nmultipass shell primary Once in your Multipass Ubuntu VM:\nbash \u003c(curl -s https://raw.githubusercontent.com/signalfx/otelworkshop/master/setup-tools/ubuntu.sh) Key OTel APM concepts Moving parts that make APM happen in OpenTelemetry:\n Application Spans: OpenTelemetry instrumentation causes spans to be emitted by your applications OpenTelmetry auto-instrumentation (no code changes) for most languages is availabile but you can use any framework/library that emits spans in formats accepted by the Otel Collector i.e zipkin, OpenTracing, or OpenTelemetry. The spans are received by the OpenTelemetry Collector which both doubles as an infrastructure metrics collection agent and a telemetry processor. The Collector then forwards all telemetry (metrics/traces/logs) to Splunk Observability Cloud. Instructructure metrics: Infrastructure metrics are collected by your OpenTelemetry Collector which is observing the application’s host or container cluster. The infrastructure agent is lightweight, open source, real-time, and designed for microservices, containers, and cloud as well as on premise servers or cloud virtual machines. Application spans will be sent to the OpenTelemetry Collector running on a host or k8s pod to correlate APM with host/cluster metrics. The Collector then relays the spans to Splunk Observability Cloud APM where they will be assembled into traces. The APM spans flow in real time and there is no sampling. Pre-made default Service Dashboards with application metrics for each app will appear once spans are received by Splunk APM. The APM view has directed troubleshooting. Environment variables control the setup of APM. These names vary based on instrumentation but they always include:  Endpoint: destination to send spans Service name: the name of the application as you want it to appear in a service map Environment: a value for segmenting betwen dev/prod etc. Can be set with instrumentation and not necessarily as part of an ENV variable.    ","categories":"","description":"","excerpt":"Log in to your Splunk Observability account to identify token/realm …","ref":"/observability-workshop/v4.22/otelw/setup/","tags":"","title":"Preparation"},{"body":"Splunk Synthetic Monitoring offers the most comprehensive and in-depth capabilities for uptime and web performance optimization as part of the only complete observability suite, Splunk Observability Cloud.\nEasily set up monitoring for APIs, service endpoints and end-user-experience. With Splunk Synthetic monitoring, go beyond basic uptime and performance monitoring and focus on proactively finding and fixing issues, optimizing web performance, and ensuring customers get the best user experience.\nWith Splunk Synthetic Monitoring you can:\n Detect and resolve issues fast across critical user flows, business transactions and API endpoints Prevent web performance issues from affecting customers with an intelligence web optimization engine And improve performance of all page resources and third-party dependencies  ","categories":"","description":"","excerpt":"Splunk Synthetic Monitoring offers the most comprehensive and in-depth …","ref":"/observability-workshop/v4.22/synthetics/","tags":"","title":"Introduction"},{"body":"Splunk Synthetic Monitoring offers the most comprehensive and in-depth capabilities for uptime and web performance optimization as part of the only complete observability suite, Splunk Observability Cloud.\nEasily set up monitoring for APIs, service endpoints and end-user-experience. With Splunk Synthetic monitoring, go beyond basic uptime and performance monitoring and focus on proactively finding and fixing issues, optimizing web performance, and ensuring customers get the best user experience.\nWith Splunk Synthetic Monitoring you can:\n Detect and resolve issues fast across critical user flows, business transactions and API endpoints Prevent web performance issues from affecting customers with an intelligence web optimization engine And improve performance of all page resources and third-party dependencies  ","categories":"","description":"","excerpt":"Splunk Synthetic Monitoring offers the most comprehensive and in-depth …","ref":"/observability-workshop/v4.22/ja/synthetics/","tags":"","title":"Introduction"},{"body":" Check the original HEAD section of your Online-boutique webpage (or use the examples here) in your browser Find the Web address of your workshop hosts Online Boutique Compare the changes made to the hosts Online-Boutique and compare with the base one.   1. Review the original code of your NON RUM Online-Boutique If you have access to an EC2 instance and have previously installed the Online Boutique as part of the APM session, you can view it on port 81 of the EC2 instance’s IP address.\nIf you have not got access to an EC2 instance with the Online Boutique installed then your workshop instructor will provide you with the Online Boutique URL that does not have RUM installed so that you can complete the next steps.\nOpen your web browser and go to the Online Boutique. (The one you previously used, or the one provided by the Workshop instructor). You will see the Non RUM Online Boutique running.\nFollow the instructions for your preferred browser below:\n1.1 Chrome, FireFox \u0026 Microsoft Edge Users - Check the Web page source In Chrome \u0026 Firefox or Microsoft Edge you can right click on the Online-Boutique site, you will have an option to “View Page Source”\nSelecting it will show you the HTML page source code in a separate Tab.\nIf successful you can skip to 2 - Review the unchanged HEAD section.\n1.2 Safari Users - Check the Web page source For Safari users, you may have to enable the extra menu in Safari by selecting ‘Preferences’ under Safari in the OS X menu bar.\nThen in the dialog that pops up, under the ‘Advanced’ pane select the checkbox that says ‘Show Develop menu in menu bar. ‘ and close the Dialog box.\nYou can now right click on the Online-Boutique and you now will have an option ‘Show Page Source’.\nIf you select that option on the Online-Boutique you will see the HTML source code as shown below:\nIf successful you can skip to 2 - Review the unchanged HEAD section.\n1.3 Internet Explorer Users - Check the Web page source For Internet Explorer 11 Users, you may have trouble with this exercise as it will require a specific version of the Splunk Open Telemetry Javascript for Web/RUM.\nHowever you will be able to see the changes required by right clicking on the Online-Boutique site, you see an option to “View Source”\nIf you select that option on the Online-Boutique you will see the HTML source code as shown below:\n2 - Review the unchanged HEAD section The changes for RUM will be placed in the HEAD section of your Web page, Below are the original lines as you should have it in your local Base version.\nThere is no reference to the Splunk or Open Telemetry Beacon (The function that is used to send RUM Metrics and Traces )\n3. Find the web (URL) of the RUM enabled Online Boutique The Online Boutique we are going to use for RUM is viewable on port 81 of the RUM Enabled instance’s IP address and the url will be provided to you by the workshop instructor at this point.\nWe are all connecting to the extra RUM Enabled Online Boutique provided by the workshops instructor for this RUM session. Open a new web browser and go to http://{==RUM-HOST-EC2-IP==}:81/ where you will then be able to see the RUM enabled Online Boutique running. Again, view the source of the HTML Page as described in the previous section:\n4. Review the Changes made to enable RUM in the HEAD section of the RUM enabled Online-Boutique The changes needed for RUM are placed in the HEAD section of the hosts Web page, Below is the hosts updated HEAD section with the changes required to enable RUM:\nThe first three lines (marked in red) have been added to the HEAD section of the host Web page to enable RUM Tracing, the last three (marked in blue) are optional and used to enable Custom RUM events.\n\u003cscript src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" type=\"text/javascript\"\u003e\u003c/script\u003e \u003cscript\u003ewindow.SplunkRum \u0026\u0026 window.SplunkRum.init({beaconUrl: \"https://rum-ingest.eu0.signalfx.com/v1/rum\", rumAuth: \"1wCqZVUWIP5XSdNjPoQRFg\", app: \"ksnq-rum-app\", environment: \"ksnq-rum-env\"});\u003c/script\u003e \u003cscript\u003e const Provider = SplunkRum.provider; var tracer=Provider.getTracer('appModuleLoader'); \u003c/script\u003e  The first part is to indicate where to download the Splunk Open Telemetry Javascript file from: https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js (This can also be loaded locally if so required) The second line defines the location where to send the traces to in the beacon url: {beaconUrl: \"https://rum-ingest.eu0.signalfx.com/v1/rum\" It also adds an Access Token to : rumAuth: \"1wCqZVUWIP5XSdNjPoQRFg\" (this of course is an example, you can create multiple RUM Access Tokens for all your applications) And it is used to add identification tags like the application Name and environment to the RUM trace for use in the SPLUNK RUM UI: app: \"ksnq-rum-app\", environment: \"ksnq-rum-env\"}  Info\nIn this example the app name is ksnq-rum-app, this will be different in the Workshop. Check with your host what the app name and environment to use in the RUM session will be and make a note of it!   The above two lines are all that is required to enable RUM on your website!\nThe (blue) optional section that uses var tracer=Provider.getTracer('appModuleLoader'); will add a custom event for every page change allow you to better track your website conversions and usage.\n","categories":"","description":"","excerpt":" Check the original HEAD section of your Online-boutique webpage (or …","ref":"/observability-workshop/v4.22/rum/docs/setup/","tags":"","title":"Example of RUM enablement in your Website"},{"body":"1. チャートの編集 Sample Data ダッシュボードにある Latency histogram チャートの3点 ... をクリックして、Open をクリックします（または、チャートの名前をクリックしてください、ここでは Latency histogram です）。\nチャートエディターのUIには、Latency histogram チャートのプロットオプション、カレントプロット、シグナル（メトリック）が表示されます。\nPlot Editor タブの Signal には、現在プロットしている demo.trans.latency というメトリックが表示されます。\nいくつかの Line プロットが表示されます。18 ts という数字は、18個の時系列メトリックをチャートにプロットしていることを示しています。\n異なるチャートタイプのアイコンをクリックして、それぞれの表示を確認してください。スワイプしながらその名前を確認してください。例えば、ヒートマップのアイコンをクリックします。\nチャートがヒートマップに変わります。\nNote\n様々なチャートを使用してメトリクスを視覚化することができます。自分が望む視覚化に最も適したチャートタイプを選択してください。\n各チャートタイプの詳細については、Choosing a chart type を参照してください。\n  チャートタイプの Line をクリックすると、線グラフが表示されます。\n2. タイムウィンドウの変更 また、Time ドロップダウンから Past 15 minutes に変更することで、チャートの時間枠を変更することができます。\n3. データテーブルの表示 Data Table タブをクリックします。\n18行が表示され、それぞれがいくつかの列を持つ時系列メトリックを表しています。これらの列は、メトリックのディメンションを表しています。demo.trans.latency のディメンジョンは次のとおりです。\n demo_datacenter demo_customer demo_host  demo_datacenter 列では、メトリクスを取得している2つのデータセンター、Paris と Tokyo があることがわかります。\nグラフの線上にカーソルを横に移動させると、それに応じてデータテーブルが更新されるのがわかります。チャートのラインの1つをクリックすると、データテーブルに固定された値が表示されます。\n ここでもう一度 Plot editor をクリックしてデータテーブルを閉じ、このチャートをダッシュボードに保存して、後で使用しましょう。\n","categories":"","description":"","excerpt":"1. チャートの編集 Sample Data ダッシュボードにある Latency histogram チャートの3点 ... をクリックし …","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/editing/","tags":"","title":"チャートを編集する"},{"body":"","categories":"","description":"**15 分**\n","excerpt":"**15 分**\n","ref":"/observability-workshop/v4.22/ja/imt/docs/gdi/","tags":"","title":"データを取り込む"},{"body":" ミューティングルールを設定する 通知を再開する   1. ミューティングルールの設定 特定の通知をミュートする必要がある場合があります。例えば、サーバーやサーバー群のメンテナンスのためにダウンタイムを設定したい場合や、新しいコードや設定をテストしている場合などがあります。このような場合には、Splunk Observability Cloud でミューティングルールを使用できます。それでは作成してみましょう。\nナビバーにある ボタンをクリックし、Detectors を選択します。現在設定されているディテクターの一覧が表示されます。フィルタを使ってディテクターを探すこともできます。\nCreating a Detector でディテクターを作成した場合は、右端の3つの点 ... をクリックすると、そのディテクターが表示されます。\nドロップダウンから Create Muting Rule… をクリックします。\nMuting Rule ウィンドウで、 Mute Indefinitely をチェックし、理由を入力します。\nImportant\nこの操作をすると、ここに戻ってきてこのボックスのチェックを外すか、このディテクターの通知を再開するまで、通知が永久的にミュートされます。   Next をクリックして、新しいモーダルウィンドウでミュートルールの設定を確認します。\n Mute Indefinitely    をクリックして、設定を確定させます。\nこれで、通知を再開するまで、ディテクターからのEメール通知は受け取ることがなくなりました。では、再開する方法を見てみましょう。\n 2. 通知を再開する Muting Rules をクリックして、Detector の見出しの下に、通知をミュートしたディテクターの名前が表示されます。\n右端にあるドット ... を開いて、Resume Notifications をクリックします。\n Resume    をクリックして、このディテクターの通知を確認し、再開します。\nおめでとうございます！ これでアラート通知が再開されました。\n","categories":"","description":"","excerpt":" ミューティングルールを設定する 通知を再開する   1. ミューティングルールの設定 特定の通知をミュートする必要がある場合があります。例 …","ref":"/observability-workshop/v4.22/ja/imt/docs/detectors/muting/","tags":"","title":"ミューティングルールを利用する"},{"body":"Splunk APM Entity type The Splunk APM Entity Type comes with the Splunk observability content pack. In this section we will enable the APM entity and add a vital metric and a Navigation Suggestion to the APM Entity Type.\nGet to know Entity types Splunk IT Service Intelligence (ITSI) visualizes entity data using entity types, analysis data filters, and navigations. ITSI has default configurations for supported integrations. Analysis data filters and navigations are components of entity types. You can create custom entity types, analysis data filters, and navigations.\n*Watch this video introducing the custom entity type concept.\n Enable Modular Input for APM error rate and APM thruput Enable the Splunk APM Services Enable APM Service (4 service to enable)\n Application Duration Application Error Rate Application Performance Monitoring Application Rate (Throughput)  Enable Cloud Entity Search for APM Go to Settings -\u003e Searches, Reports, and Alerts\nSelect App Splunk Observability Cloud | Owner All\nFind the line ITSI Import Objects - Splunk-APM Application Entity Search -\u003e (Actions) Edit -\u003e Enable\nNOTE those searches are called Cloud Entity Searches\nOpen ITSI -\u003e Infrastructure Overview\nVerify that you have your entities are showing up Note: there isn’t any out of the box Key vital metrics so the visualisation will look like this\nTask 4.5: Add a dashboard Navigation  Configuration -\u003e Entity management -\u003e Entity Types Find SplunkAPM -\u003e Edit Open Navigations type Navigation Name: Traces View URL : https://app.${sf_realm}.signalfx.com/#/apm/traces Save navigation!! Save Entity type  In Service Analyzer open a Splunk APM entity and test your new navigation suggestion\nAdd Key Vital metrics for Splunk APM Configuration -\u003e Entity management -\u003e Entity Types Find SplunkAPM -\u003e Edit Open Vital Metrics Enter a name Add a metric Enter the search below and click run search\n| mstats avg(*) span=5m WHERE \"index\"=\"sim_metrics\" AND sf_streamLabel=\"thruput_avg_rate\" GROUPBY sf_service sf_environment | rename avg(service.request.count) as \"val\" Entity matching field sf_service\n(note: verify that you are matching entities 10 entities matched in last hour)\nUnit of Display Percent (%)\nChoose a Key Metric Select Application Rate Thruput\nSave Application Rate\nSave Entity Type\nYour UI should look like this should look like this:\n","categories":"","description":"","excerpt":"Splunk APM Entity type The Splunk APM Entity Type comes with the …","ref":"/observability-workshop/v4.22/itsi/docs/entity-types/","tags":"","title":"APM Entity types"},{"body":"","categories":"","description":"**20 minutes**\n","excerpt":"**20 minutes**\n","ref":"/observability-workshop/v4.22/imt/docs/dashboards/","tags":"","title":"Working with Dashboards"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/otelw/labs/","tags":"","title":"Labs"},{"body":"Make sure that you still have the Python Flask server from the Python Lab running. If you accidentally shut it down follow steps from the Python lab to restart the Python Flask server.\nStart in the Node example directory Open a new terminal window\ncd ~/otelworkshop/host/node Configure Node.js environment During npm init below you can use all defaults when prompted:\nnpm init \u0026\u0026 \\ npm install @splunk/otel --save \u0026\u0026 \\ npm install @opentelemetry/instrumentation-http --save Run .js HTTP.get requests client Set up environment and run the node app with HTTP.get requests !!! important If you are doing this workshop as part of a group, before the next step, add your initials do the APM environment: edit the run-client.sh script below and add your initials to the environment i.e. change:\nexport OTEL_RESOURCE_ATTRIBUTES=deployment.environment=apm-workshop\nto export OTEL_RESOURCE_ATTRIBUTES=deployment.environment=sjl-apm-workshop\nsource run-client.sh You will see requests printed to the window\nAPM Dashboard Traces / services will now be viewable in the APM dashboard. A new service takes about 90 seconds to register for the firs time, and then all data will be available in real time.\nAdditionally span IDs will print in the terminal where flask-server.py is running. You can use ++ctrl+c++ to stop the requests and server any time. You should now see a Node requests service alongside the Python and Java ones.\nWhere is the OpenTelemetry Instrumentation? You can see in the run-client.sh how the environment has been set up for OpenTelemtry and where the autoinstrumentation takes place as the node app runs.\nSplunk’s Otel autoinstrumentation for node.js is here\n","categories":"","description":"","excerpt":"Make sure that you still have the Python Flask server from the Python …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_single_host/node/","tags":"","title":"Node.js- Deploy HTTP Client"},{"body":"Splunk Real User Monitoring For the Real User Monitoring (RUM) instrumentation, we will add the Open Telemetry Javascript https://github.com/signalfx/splunk-otel-js-web snippet in the pages, we will use the wizard again Data Setup → RUM Instrumentation → Monitor user experience → Browser Instrumentation → Add Integration.\nThen you’ll need to select the workshop RUM token and define the application and environment names. The wizard will then show a snipped of HTML code that needs to be place at the top at the pages in the \u003chead\u003e section. In this example we are using:\n Application Name: \u003chostname\u003e-petclinic-service Environment: \u003chostname\u003e-petclinic-env  Copy the following code snippet and update the value for app and environment accordingly:\n\u003cscript src=\"https://cdn.signalfx.com/o11y-gdi-rum/latest/splunk-otel-web.js\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e \u003cscript\u003e SplunkRum.init({ beaconUrl: \"https://rum-ingest.\u003cREALM\u003e.signalfx.com/v1/rum\", rumAuth: \"\u003cACCESS_TOKEN\u003e\", app: \"\u003chostname\u003e-petclinic-service\", environment: \"\u003chostname\u003e-petclinic-env\" }); \u003c/script\u003e The Spring PetClinic application uses a single HTML page as the “layout” page, that is reused across all pages of the application. This is the perfect location to insert the Splunk RUM Instrumentation Library as it will be loaded in all pages automatically.\nLet’s then edit the layout page:\nvi src/main/resources/templates/fragments/layout.html and let’s insert the snipped we generated above in the \u003chead\u003e section of the page. Now we need to rebuild the application and run it again:\nRebuild PetClinic run the maven command to compile/build/package PetClinic:\n./mvnw package -Dmaven.test.skip=true java -javaagent:./splunk-otel-javaagent.jar \\ -Dsplunk.profiler.enabled=true \\ -Dsplunk.metrics.enabled=true \\ -jar target/spring-petclinic-*-SNAPSHOT.jar \\ --spring.profiles.active=mysql Then let’s visit the application again to generate more traffic http://\u003cVM_IP_ADDRESS\u003e:8080, now we should see RUM traces being reported\nLet’s visit RUM and see some of the traces and metrics Hamburger Menu → RUM and you will see some of the Spring PetClinic URLs showing up in the UI.\n","categories":"","description":"","excerpt":"Splunk Real User Monitoring For the Real User Monitoring (RUM) …","ref":"/observability-workshop/v4.22/pet-clinic/docs/rum/","tags":"","title":"Real User Monitoring"},{"body":"Splunk RUM is the industry’s only end to end, full fidelity Real User Monitoring solution. It is built to optimize performance and aid in faster troubleshooting, giving you full visibility into end-user experiences.\nSplunk RUM allows you to identify performance problems in your web applications that impact the customer experience. We support benchmarking and measuring page performance with core web vitals. This includes but not limited to: W3C timings, the ability to identify long running tasks, along with everything that can impact your page load.\nWith Splunk’s end to end monitoring capabilities you are able to view the latency between all of the services that make up your application, from the service itself through to infrastructure metrics such as database calls and everything in between.\nOur full fidelity end to end monitoring solution captures 100% of your span data. We do not sample, we are framework agnostic and Open Telemetry standardized.\nMore often than not we find that the frontend and backend application’s performance are interdependent. Fully understanding and being able to visualize the link between your backend services and your user experience is increasingly important. To see the full picture, Splunk RUM provides seamless correlation between our front end and back end microservices. If your users are experiencing less than optimal conditions on your web based application due to an issue related to your microservice or infrastructure, Splunk will be able to detect this issue and alert you.\nTo complete the picture and offer full visibility, Splunk is also able to show in-context logs and events to enable deeper troubleshooting and root-cause analysis.\n","categories":"","description":"","excerpt":"Splunk RUM is the industry’s only end to end, full fidelity Real User …","ref":"/observability-workshop/v4.22/rum/","tags":"","title":"Splunk RUM, an introduction"},{"body":"","categories":"","description":"**20 分**\n","excerpt":"**20 分**\n","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/","tags":"","title":"ダッシュボードを利用する"},{"body":"Checkout the milestone for this task. See the introduction for a brief howto.\nShell Command   git reset --hard \u0026\u0026 git clean -fdx \u0026\u0026 git checkout 01service  Let’s get python sorted first. On a provided AWS instance, python3 is already available.\nIf you are on a Mac:\nShell Command   brew install python@3  On another system, install a recent version of python (i.e. 3.x) with your package manager.\nNavigate to o11y-bootcamp/bootcamp/service/src and run the provided python service:\nShell Command: python3  Example Output python3   python3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt python3 app.py * Serving Flask app 'app' (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on all addresses. WARNING: This is a development server. Do not use it in a production deployment. * Running on http://10.42.1.202:5000/ (Press CTRL+C to quit)  Then test the service in a separate shell in the ~/o11y-bootcamp/bootcamp/service/src directory with:\nShell Command: curl  Example Output: curl   curl -X POST http://127.0.0.1:5000/wordcount -F text=@hamlet.txt [[\"in\", 436], [\"hamlet\", 484], [\"my\", 514], [\"a\", 546], [\"i\", 546], [\"you\", 550], [\"of\", 671], [\"to\", 763], [\"and\", 969], [\"the\", 1143]]%  The bootcamp contains other text files at ~/nlp/resources/corpora. To use a random example:\nShell Command   SAMPLE=$(find ~/nlp/resources/corpora/gutenberg -name '*.txt' | shuf -n1) curl -X POST http://127.0.0.1:5000/wordcount -F text=@$SAMPLE  To generate load:\nShell Command   FILES=$(find ~/nlp/resources/corpora/gutenberg -name '*.txt') while true; do SAMPLE=$(shuf -n1 \u003c\u003c\u003c \"$FILES\") curl -X POST http://127.0.0.1:5000/wordcount -F text=@${SAMPLE} sleep 1 done  ","categories":"","description":"","excerpt":"Checkout the milestone for this task. See the introduction for a brief …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/monolith/","tags":"","title":"Create a Monolith Service"},{"body":"","categories":"","description":"**10 minutes**\n","excerpt":"**10 minutes**\n","ref":"/observability-workshop/v4.22/imt/docs/detectors/","tags":"","title":"Working with Detectors"},{"body":"Splunk Log Observer For the Splunk Log Observer component, we will configure the Spring PetClinic application to write logs to a file in the filesystem and configure the Splunk OpenTelemetry Collect to read (tail) that log file and report the information to the Splunk Observability Platform.\nFluentD Configuration We need to configure the Splunk OpenTelemetry Collector to tail the Spring Pet Clinic log file and report the data to the Splunk Observability Cloud endpoint.\nThe Splunk OpenTelemetry Collector uses FluentD to consume/report logs and to configure the proper setting to report Spring PetClinic logs, we just need to add a FluentD configuration file in the default directory (/etc/otel/collector/fluentd/conf.d/).\nHere’s the sample FluentD configuration file (petclinic.conf, reading the file /tmp/spring-petclinic.log)\n\u003csource\u003e @type tail @label @SPLUNK tag petclinic.app path /tmp/spring-petclinic.log pos_file /tmp/spring-petclinic.pos_file read_from_head false \u003cparse\u003e @type none \u003c/parse\u003e \u003c/source\u003e So we need to create the file\nsudo vi /etc/otel/collector/fluentd/conf.d/petclinic.conf We also need to change permission and ownership of the petclinic.conf file\nsudo chown td-agent:td-agent /etc/otel/collector/fluentd/conf.d/petclinic.conf sudo chmod 755 /etc/otel/collector/fluentd/conf.d/petclinic.conf And paste the contents from the snippet above. Once the file is created, we need to restart the FluentD process\nsudo systemctl restart td-agent PetClinic Logback Settings The Spring PetClinic application can be configure to use a number of different java logging libraries. In this scenario, we are using logback. Here’s a sample logback configuration file:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE xml\u003e \u003cconfiguration scan=\"true\" scanPeriod=\"30 seconds\"\u003e \u003ccontextListener class=\"ch.qos.logback.classic.jul.LevelChangePropagator\"\u003e \u003cresetJUL\u003etrue\u003c/resetJUL\u003e \u003c/contextListener\u003e \u003clogger name=\"org.springframework.samples.petclinic\" level=\"debug\"/\u003e \u003cappender name=\"file\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"\u003e \u003cfile\u003e/tmp/spring-petclinic.log\u003c/file\u003e \u003crollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"\u003e \u003cfileNamePattern\u003espringLogFile.%d{yyyy-MM-dd}.log\u003c/fileNamePattern\u003e \u003cmaxHistory\u003e5\u003c/maxHistory\u003e \u003ctotalSizeCap\u003e1GB\u003c/totalSizeCap\u003e \u003c/rollingPolicy\u003e \u003cencoder\u003e \u003cpattern\u003e %d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %msg trace_id=%X{trace_id} span_id=%X{span_id} trace_flags=%X{trace_flags} service.name=%property{otel.resource.service.name}, deployment.environment=%property{otel.resource.deployment.environment} %n \u003c/pattern\u003e \u003c/encoder\u003e \u003c/appender\u003e \u003croot level=\"debug\"\u003e \u003cappender-ref ref=\"file\" /\u003e \u003c/root\u003e \u003c/configuration\u003e We just need to create a file named logback.xml in the configuration folder.\nvi src/main/resources/logback.xml and paste the XML content from the snippet above. After that, we need to rebuild the application and run it again:\n./mvnw package -Dmaven.test.skip=true java -javaagent:./splunk-otel-javaagent.jar \\ -Dsplunk.profiler.enabled=true \\ -Dsplunk.metrics.enabled=true \\ -jar target/spring-petclinic-*-SNAPSHOT.jar \\ --spring.profiles.active=mysql Then let’s visit the application again to generate more traffic, now we should see log messages being reported http://\u003cVM_IP_ADDRESS\u003e:8080 (feel free to navigate and click around).\nThen visit: Hamburger Menu \u003e Log Observer\nAnd you can add a filter to select only log messages from your host and the Spring PetClinic Application:\n Add Filter → Fields → host.name → \u003cyour host name\u003e Add Filter → Fields → service.name → \u003cyour application name\u003e  Summary This the end of the exercise and we have certainly covered a lot of ground. At this point you should have metrics, traces, logs, database query performance and code profiling being reported into Splunk Observability Cloud. Congratulations!\n","categories":"","description":"","excerpt":"Splunk Log Observer For the Splunk Log Observer component, we will …","ref":"/observability-workshop/v4.22/pet-clinic/docs/logobserver/","tags":"","title":"Log Observer"},{"body":"The development team has broken up the monolithic service into microservices baesd on the docker-compose setup. Switch to the provided milestone 10microservices with the instructions from “Getting Started”.\nTest the service with:\nShell Command   curl -X POST http://127.0.0.1:8000/api -F text=@hamlet.txt  Add auto-instrumentation to the public_api microservice using the Splunk distribution of OpenTelemetry Python. Review the documentation and the getting Started steps and apply it to Dockerfile.\nTake into account the trace exporter settings and add the required environment variables to the .env file for docker-compose. Use the configuration to send traces directly to Splunk Observability Cloud.\nThe milestone for this task is 10microservices-autoi. It has auto-instrumentation applied for all microservices.\n","categories":"","description":"","excerpt":"The development team has broken up the monolithic service into …","ref":"/observability-workshop/v4.22/bootcamp/docs/apm/autoi/","tags":"","title":"Microservices Auto-instrumentation"},{"body":"We are going to work in the directory bootcamp/service/src. Your first task: Write a python app to count words in a text file.\nNo, wait - we’ve already done that for you.\nThis section will introduce the format for this workshop.\n  First, we will introduce a challenge or task for you to complete, e.g. “Task 1: Service”.\n  There will be concepts and references for you to review.\n  We will timebox self-paced content during a live workshop.\n  We provide copies of solutions for the tasks in the workshop. They are called milestones and live in their own directory at ~/milestones.\n  If you did not complete a specific task, you can use these milestones to proceed to the next task or review the solution. Some tasks also instruct you to use a specific milestone as a basis. Let’s look at how this works, and remember: milestones are just directories on the file system and you can copy over content into your working directory as needed.\nGetting started The task is to write a python app to count words in a text file. Here is how to get to the milestone that completes this step:\nShell Command   git checkout 01service  This will put you on the first milestone.\nIn case you have already worked on a milestone, you might see an error like:\nExample Output   error: Your local changes to the following files would be overwritten by checkout: app.py Please commit your changes or stash them before you switch branches. Aborting  This is because your work conflicts with changes on the milestone. You have the following options:\n  If you have worked on a task and want to progress to the next one and DROP all your changes: Shell Command: Git Reset   git reset --hard \u0026\u0026 git clean -fdx \u0026\u0026 git checkout service  You will have to re-apply any local changes like settings tokens or names.\n  To preserve your work but move it out of the way, you can use\nShell Command: Git Stash   git stash \u0026\u0026 git checkout service  To restore your work, switch to the previous milestone (main in this case) and retrieve the stashed changes:\n Shell Command: Git Checkout   git checkout main \u0026\u0026 git stash pop  Sometimes you run into conflicting changes with this approach. We recommend you use the first option in this case.\n  During development changes are recorded by adding and commiting to the repository. This is not necessary for this workshop.\n  Use the first option and proceed.\nTo compare two milestones, use\nShell Command: Git Checkout   git diff main..01service  To compare what you have with a milestone, , e.g. the milestone service use\nShell Command: Git Checkout  Example Output (excerpt)   git diff ..01service ... diff --git a/bootcamp/service/src/app.py b/bootcamp/service/src/app.py index 9bcae83..b7fc141 100644 --- a/bootcamp/service/src/app.py +++ b/bootcamp/service/src/app.py @@ -1,10 +1,12 @@ +import json import re -from unicodedata import category +from flask import Flask, request, Response ...  Future Tasks TODO YOUR Idea here? Let us know!\nTODO metrics method being traced - how to disable?\nfrom opentelemetry.context import attach, detach, set_value token = attach(set_value(\"suppress_instrumentation\", True)) TODO autodetect metrics with k8s labels: prometheus.io/scrape: true - run prometheus on separate port 9090.\nTODO tracing examples\n","categories":"","description":"","excerpt":"We are going to work in the directory bootcamp/service/src. Your first …","ref":"/observability-workshop/v4.22/bootcamp/","tags":"","title":"Welcome to the Observability Bootcamp"},{"body":"Goals  Understand components involved for GDI, e.g.:  OpenTelemetry Collector Prometheus exporters   Understand how Developers, DevOps and SREs work with infrastructure and microservices Ability to ingest data from common stacks:  Virtual Machines Containers and Container Orchestration (e.g. Docker, Docker Compose) Container Orchestration Platforms (e.g. Kubernetes)   Understand troubleshooting 101 for components (commands)  We are going to work in the directory o11y-bootcamp/bootcamp/service/src. Your first task: Write a python app to count words in a text file.\nNo, wait - we’ve already done that for you.\n","categories":"","description":"","excerpt":"Goals  Understand components involved for GDI, e.g.:  OpenTelemetry …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/","tags":"","title":"Lab: OpenTelemetry \u0026 Get Data In (GDI)"},{"body":" Find the Web address of your workshop hosts Online Boutique Generate traffic by shopping for bargains on your workshop hosted Online Boutique web shop.   1. URL of RUM enabled Online Boutique As discussed in the previous section we are going to use the Online Boutique running on the separate RUM host.\nWe are all connected to the Online Boutique running on the specific RUM host for this session. This will create more traffic from multiple locations, making the data more realistic.\nYou should have received the correct URL from your workshop host at this point. Open a new web browser and go to http://{==RUM-HOST-EC2-IP==}:81/ where you will then be able to see the RUM enabled Online Boutique running.\n2. Generate traffic The goal of this exercise is for you to browse the RUM enabled Online Boutique and buy different products and different quantities. For extra credit, you may even use the url from different browsers or from your smartphone.\nThis will create multiple sessions to investigate. Take your time to examine and buy the various products and put them in your cart:\nDoesn’t that HOME BARISTA KIT look tempting?… Your time to start shopping now!\n","categories":"","description":"","excerpt":" Find the Web address of your workshop hosts Online Boutique Generate …","ref":"/observability-workshop/v4.22/rum/docs/showcase/","tags":"","title":"Showcase of RUM with the Online Boutique"},{"body":"1. Saving a chart To start saving your chart, lets give it a name and description. Click the name of the chart Copy of Latency Histogram and rename it to “Active Latency”.\nTo change the description click on Spread of latency values across time. and change this to Overview of latency values in real-time.\nClick the Save As    button. Make sure your chart has a name, it will use the name Active Latency the you defined in the previous step, but you can edit it here if needed.\nPress the Ok    button to continue.\n2. Creating a dashboard In the Choose dashboard dialog, we need to create a new dashboard, click on the New Dashboard    button.\nYou will now see the New Dashboard Dialog. In here you can give you dashboard a name and description, and set Read and Write Permissions.\nPlease use your own name in the following format to give your dashboard a name e.g. YOUR_NAME-Dashboard.\nPlease replace YOUR_NAME with your own name, change the dashboard permissions to Restricted Read and Write access, and verify your user can read/write.\nYou should see you own login information displayed, meaning you are now the only one who can edit this dashboard. Of course you have the option to add other users or teams from the drop box below that may edit your dashboard and charts, but for now make sure you change it back to Everyone can Read or Write to remove any restrictions and press the Save    Button to continue.\nYour new dashboard is now available and selected so you can save your chart in your new dashboard.\nMake sure you have your dashboard selected and press the Ok    button.\nYou will now be taken to your dashboard like below. You can see at the top left that your YOUR_NAME-DASHBOARD is part of a Dashboard Group YOUR_NAME-Dashboard. You can add other dashboards to this dashboard group.\n 3. Add to Team page It is common practice to link dashboards that are relevant to a Team to a teams page. So let’s add your dashboard to the team page for easy access later. Use the from the navbar again.\nThis will bring you to your teams dashboard, We use the team Example Team as an example here, the workshop one will be different.\nPress the +    Add Dashboard Group button to add you dashboard to the team page.\nThis will bring you to the Select a dashboard group to link to this team dialog. Type your name (that you used above) in the search box to find your Dashboard. Select it so its highlighted and click the Ok button to add your dashboard.\nYour dashboard group will appear as part of the team page. Pleasu note during the course of the workshop many more will appear here.\n Now click on the link for your Dashboard to add more charts!\n","categories":"","description":"","excerpt":"1. Saving a chart To start saving your chart, lets give it a name and …","ref":"/observability-workshop/v4.22/imt/docs/dashboards/savingcharts/","tags":"","title":"Saving charts"},{"body":"1. チャートの保存 チャートの保存するために、名前と説明をつけましょう。チャートの名前 Copy of Latency Histogram をクリックして、名前を “現在のレイテンシー” に変更します。\n説明を変更するには、 Spread of latency values across time. をクリックし、 リアルタイムでのレイテンシー値 に変更します。\n Save As    ボタンをクリックします。チャートに名前が付いていることを確認します。前のステップで定義した 現在のレイテンシー という名前が使用されますが、必要に応じてここで編集することができます。\n Ok    ボタンを押して続行します。\n2. ダッシュボードの作成 Choose dashboard ダイアログでは、新しいダッシュボードを作成する必要があります。 New Dashboard    ボタンをクリックしてください。\nこれで、New Dashboard ダイアログが表示されます。ここでは、ダッシュボードの名前と説明を付け、Write Permissions で書き込み権限を設定します。\nダッシュボードの名前には、自分のお名前を使って YOUR_NAME-Dashboard の形式で設定してください。\nYOUR_NAME を自分の名前に置き換えてから、Anyone in this organization can edit のチェックボックスのチェックを外し、編集権限を設定してください。\nここには、自分のログイン情報が表示されます。つまり、このダッシュボードを編集できるのは自分だけということになります。もちろん、ダッシュボードやチャートを編集できる他のユーザーやチームを下のドロップボックスから追加することもできますが、ここでは、Anyone in this organization can edit ボックスを re-tick して制限を解除し、 Save    ボタンを押して続行してください。\n新しいダッシュボードが利用可能になり、選択されましたので、チャートを新しいダッシュボードに保存することができます。\nダッシュボードが選択されていることを確認して、Ok{: .label-button .sfx-ui-button-blue} ボタンを押します。\nすると、下図のようにダッシュボードが表示されます。左上に、YOUR_NAME-DASHBOARD がダッシュボードグループ YOUR_NAME-Dashboard の一部であることがわかります。このダッシュボードグループに他のダッシュボードを追加することができます。\n 3. チームページへの追加 チームに関連するダッシュボードは、チームページにリンクさせるのが一般的です。そこで、後で簡単にアクセスできるように、ダッシュボードをチームページに追加してみましょう。左上のハンバーガーメニューを使い、サイドメニューから Dashboards を選択します。\nここでは、チーム Observability を例にしていますが、ワークショップのものは異なります。\n +    を押し、 Add Dashboard Group ボタンを押して、チームページにダッシュボードを追加します。\nすると、 Select a dashboard group to link to this team ダイアログが表示されます。 検索ボックスにご自身のお名前（上記で使用したお名前）を入力して、ダッシュボードを探します。ダッシュボードがハイライトされるように選択し、Ok ボタンをクリックしてダッシュボードを追加します。\nダッシュボードグループがチームページの一部として表示されます。ワークショップを進めていくと、さらに多くのダッシュボードがここに表示されていくはずです。\n 次のモジュールでは、ダッシュボードのリンクをクリックして、チャートをさらに追加していきます！\n","categories":"","description":"","excerpt":"1. チャートの保存 チャートの保存するために、名前と説明をつけましょう。チャートの名前 Copy of Latency Histogram …","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/savingcharts/","tags":"","title":"チャートを保存する"},{"body":"","categories":"","description":"**10 分**\n","excerpt":"**10 分**\n","ref":"/observability-workshop/v4.22/ja/imt/docs/detectors/","tags":"","title":"ディテクターを利用する"},{"body":"Goals  Understand GDI path for APM for common tech stacks (Docker, K8s). Be able to instrument an app from scratch (traces, custom metadata). Understand how distributed tracing works across tech stacks (header propagation, …)  ","categories":"","description":"","excerpt":"Goals  Understand GDI path for APM for common tech stacks (Docker, …","ref":"/observability-workshop/v4.22/bootcamp/docs/apm/","tags":"","title":"Lab: Application Performance Monitoring"},{"body":"","categories":"","description":"**10 minutes**\n","excerpt":"**10 minutes**\n","ref":"/observability-workshop/v4.22/imt/docs/monitoring-as-code/","tags":"","title":"Monitoring as Code"},{"body":"","categories":"","description":"**10 分**\n","excerpt":"**10 分**\n","ref":"/observability-workshop/v4.22/ja/imt/docs/monitoring-as-code/","tags":"","title":"Monitoring as Code"},{"body":" Use Terraform1 to manage Observability Cloud Dashboards and Detectors Initialize the Terraform Splunk Provider2. Run Terraform to create detectors and dashboards from code using the Splunk Terraform Provider. See how Terraform can also delete detectors and dashboards.   1. Initial setup Monitoring as code adopts the same approach as infrastructure as code. You can manage monitoring the same way you do applications, servers, or other infrastructure components.\nYou can monitoring as code to build out your visualisations, what to monitor, and when to alert, among other things. This means your monitoring setup, processes, and rules can be versioned, shared, and reused.\nFull documentation for the Splunk Terraform Provider is available here. Remaining in your AWS/EC2 instance, change into the signalfx-jumpstart directory\nChange directory   cd ~/signalfx-jumpstart  The environment variables needed should already be set from Installation using Helm. If not, create the following environment variables to use in the Terraform steps below\nEnvironment Variables   export ACCESS_TOKEN=\u003creplace_with_O11y-Workshop-ACCESS_token\u003e export REALM=\u003creplace_with_splunk_realm\u003e  Initialize Terraform and upgrade to the latest version of the Splunk Terraform Provider\nNote: Upgrading the SignalFx Terraform Provider\nYou will need to run the command below each time a new version of the Splunk Terraform Provider is released. You can track the releases on GitHub.   Initialise Terraform  Initialise Output   terraform init -upgrade Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kafka in modules/kafka - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Finding latest version of splunk-terraform/signalfx... - Installing splunk-terraform/signalfx v6.7.10... - Installed splunk-terraform/signalfx v6.7.10 (signed by a HashiCorp partner, key ID 8B5755E223754FC9) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.  2. Create execution plan The terraform plan command creates an execution plan. By default, creating a plan consists of:\n Reading the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date. Comparing the current configuration to the prior state and noting any differences. Proposing a set of change actions that should, if applied, make the remote objects match the configuration.  The plan command alone will not actually carry out the proposed changes, and so you can use this command to check whether the proposed changes match what you expected before you apply the changes\nExecution Plan  Execution Plan Output   terraform plan -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Plan: 92 to add, 0 to change, 0 to destroy.  If the plan executes successfully, we can go ahead and apply:\n 3. Apply execution plan The terraform apply command executes the actions proposed in the Terraform plan above.\nThe most straightforward way to use terraform apply is to run it without any arguments at all, in which case it will automatically create a new execution plan (as if you had run terraform plan) and then prompt you to provide the Access Token, Realm (the prefix defaults to Splunk) and approve the plan, before taking the indicated actions.\nDue to this being a workshop it is required that the prefix is to be unique so you need to run the terraform apply below.\nApply Plan  Apply Plan Output   terraform apply -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Apply complete! Resources: 92 added, 0 changed, 0 destroyed.  Once the apply has completed, validate that the detectors were created, under the Alerts \u0026 Detectors and click on the Detectors tab. They will be prefixed by the hostname of your instance. To check the prefix value run:\nEcho Hostname   echo $(hostname)  You will see a list of the new detectors and you can search for the prefix that was output from above.\n3. Destroy all your hard work The terraform destroy command is a convenient way to destroy all remote objects managed by your Terraform configuration.\nWhile you will typically not want to destroy long-lived objects in a production environment, Terraform is sometimes used to manage ephemeral infrastructure for development purposes, in which case you can use terraform destroy to conveniently clean up all of those temporary objects once you are finished with your work.\nNow go and destroy all the Detectors and Dashboards that were previously applied!\nDestroy  Destroy Output   terraform destroy -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" Destroy complete! Resources: 92 destroyed.  Validate all the detectors have been removed by navigating to Alerts → Detectors\n  Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions.\nConfiguration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied.\nThe infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc. ↩︎\n A provider is responsible for understanding API interactions and exposing resources. Providers generally are an IaaS (e.g. Alibaba Cloud, AWS, GCP, Microsoft Azure, OpenStack), PaaS (e.g. Heroku), or SaaS services (e.g. Splunk, Terraform Cloud, DNSimple, Cloudflare). ↩︎\n   ","categories":"","description":"","excerpt":" Use Terraform1 to manage Observability Cloud Dashboards and Detectors …","ref":"/observability-workshop/v4.22/imt/docs/monitoring-as-code/terraform/","tags":"","title":"Monitoring as Code"},{"body":" Terraform1 を使用して Observability Cloud のダッシュボードとディテクターを管理します。 Terraform のSplunk Provider2 を初期化します Terraformを実行して、Splunk Terraform Provider を使用してコードからディテクターとダッシュボードを作成します。 Terraformでディテクターやダッシュボードを削除する方法を確認します。   1. 初期設定 Monitoring as Codeは、infrastructure as Codeと同じアプローチを採用しています。アプリケーション、サーバー、その他のインフラコンポーネントと同じようにモニタリングを管理できます。\nMonitoring as Codeでは、可視化したり、何を監視するか定義したり、いつアラートを出すかなどを管理します。つまり、監視設定、プロセス、ルールをバージョン管理、共有、再利用することができます。\nSplunk Terraform Providerの完全なドキュメントはこちら にあります。\nAWS/EC2 インスタンスにログインして、signalfx-jumpstart ディレクトリに移動します\nChange directory   cd ~/signalfx-jumpstart  必要な環境変数は、Helmによるインストール ですでに設定されているはずです。そうでない場合は、以下の Terraform のステップで使用するために、以下の環境変数を作成してください。\nEnvironment Variables   export ACCESS_TOKEN=\u003creplace_with_O11y-Workshop-ACCESS_token\u003e export REALM=\u003creplace_with_splunk_realm\u003e  Terraform を初期化し、Splunk Terraform Provider を最新版にアップグレードします。\nNote: SignalFx Terraform Provider のアップグレード\nSplunk Terraform Provider の新バージョンがリリースされるたびに、以下のコマンドを実行する必要があります。リリース情報は GitHub で確認できます。\n  Initialise Terraform  Initialise Output   terraform init -upgrade Upgrading modules... - aws in modules/aws - azure in modules/azure - docker in modules/docker - gcp in modules/gcp - host in modules/host - kafka in modules/kafka - kubernetes in modules/kubernetes - parent_child_dashboard in modules/dashboards/parent - pivotal in modules/pivotal - usage_dashboard in modules/dashboards/usage Initializing the backend... Initializing provider plugins... - Finding latest version of splunk-terraform/signalfx... - Installing splunk-terraform/signalfx v6.7.10... - Installed splunk-terraform/signalfx v6.7.10 (signed by a HashiCorp partner, key ID 8B5755E223754FC9) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.  2. プランの作成 terraform plan コマンドで、実行計画を作成します。デフォルトでは、プランの作成は以下のように構成されています。\n 既に存在するリモートオブジェクトの現在の状態を読み込み、Terraform の状態が最新であることを確認します 現在の設定を以前の状態と比較し、相違点を抽出します 適用された場合にリモートオブジェクトと設定とを一致させるための、一連の変更アクションを提案します  plan コマンドだけでは、提案された変更を実際に実行はされなません。変更を適用する前に、以下のコマンドを実行して、提案された変更が期待したものと一致するかどうかを確認しましょう。\nExecution Plan  Execution Plan Output   terraform plan -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Plan: 92 to add, 0 to change, 0 to destroy.  プランが正常に実行されれば、そのまま apply することができます。\n 3. プランの適用 terraform apply コマンドは、上記の Terraform プランで提案されたアクションを実行します。\nこの場合、新しい実行プランが自動的に作成され（terraform planを実行したときと同様です）、指示されたアクションを実行する前にAccess Token、Realm（プレフィックスのデフォルトはSplunk）の提供とプランの承認を求められます。\nこのワークショップでは、プレフィックスがユニークである必要があります。以下の terraform apply を実行してください。\nApply Plan  Apply Plan Output   terraform apply -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" -var=\"sfx_prefix=[$(hostname)]\" Apply complete! Resources: 92 added, 0 changed, 0 destroyed.  適用が完了したら、 Alerts → Detectors でディテクターが作成されたことを確認してください。ディテクターのプレフィックスには、インスタンスのホスト名が入ります。プレフィックスの値を確認するには以下を実行してください。\nEcho Hostname   echo $(hostname)  新しいディテクターのリストが表示され、上から出力されたプレフィックスを検索することができます。\n3. 苦労して作ったもの全てを壊す terraform destroy コマンドは、Terraform の設定で管理されているすべてのリモートオブジェクトを破壊する便利な方法です。\n通常、本番環境では長期保存可能なオブジェクトを破棄することはありませんが、Terraformでは開発目的で一時的なインフラを管理するために使用されることがあり、その場合は作業が終わった後に terraform destroy を実行して、一時的なオブジェクトをすべてクリーンアップすることができます。\nそれでは、ここまでで適用したダッシュボードとディテクターを全て破壊しましょう！\nDestroy  Destroy Output   terraform destroy -var=\"access_token=$ACCESS_TOKEN\" -var=\"realm=$REALM\" Destroy complete! Resources: 92 destroyed.  Alerts → Detectors に移動して、すべてのディテクターが削除されたことを確認してください。\n  Terraform は、インフラを安全かつ効率的に構築、変更、バージョン管理するためのツールです。Terraform は、既存の一般的なサービスプロバイダーだけでなく、様々なカスタムソリューションも管理できます。\nTerraform の設定ファイルには、単一のアプリケーションまたはデータセンター全体を実行するために必要なコンポーネントをに記述します。Terraform はそれを受けて、望ましい状態に到達するために何をするかを記述した実行プランを生成し、記述されたインフラを構築するために実行します。設定が変更されても、Terraform は何が変更されたかを判断し、差分の実行プランを作成して適用することができます。\nTerraform が管理できるインフラには、計算機インスタンス、ストレージ、ネットワークなどのローレベルのコンポーネントや、DNSエントリ、SaaS機能などのハイレベルのコンポーネントが含まれます。 ↩︎\n プロバイダーは、APIのインタラクションを理解し、リソースを公開する責務があります。一般的にプロバイダーは、IaaS（Alibaba Cloud、AWS、GCP、Microsoft Azure、OpenStackなど）、PaaS（Herokuなど）、またはSaaSサービス（Splunk、Terraform Cloud、DNSimple、Cloudflareなど）があります。 ↩︎\n   ","categories":"","description":"","excerpt":" Terraform1 を使用して Observability Cloud のダッシュボードとディテクターを管理します。 Terraform …","ref":"/observability-workshop/v4.22/ja/imt/docs/monitoring-as-code/terraform/","tags":"","title":"Monitoring as Code"},{"body":"Below are helpful resources about Splunk and the DevOps use case. Topics covered are Observability Cloud, Splunk On-Call, OpenTelemetry, Observability and Incident Response.\nDocumentation   IMT    Splunk Infrastructure Monitoring  APM    Splunk Application Performance Monitoring (APM)  LO    Splunk Log Observer  RUM    Splunk Real User Monitoring (RUM)  Synthetics    Splunk Synthetics Monitoring  API    Splunk Observability Cloud API Docs  Blog Posts   APM    How Splunk Does Site Reliability Engineering (SRE)  APM    Application Performance Redefined: Meet the New Splunk’s Microservices APM  Splunk    Splunk is Lambda Ready: Announcing a New Partnership with AWS  APM    Supporting APM for .NET Applications  OpenTelemetry    OpenTelemetry Consolidates Data for Observability  Observability    Using Observability as a Proxy for User Happiness  Observability    Observability is key to the future of software (and your DevOps career)  OpenTelemetry    Why to Use OpenTelemetry Processors to Change Collected Backend Data  APM    What Is Distributed Tracing and Why You Need It  APM    Understanding Cardinality in a Monitoring System and Why It’s Important  Observability    How to Monitor Your AWS Workloads  Observability    How to Maximize the Performance of Your Kubernetes Deployment  Webinars \u0026 Podcasts   Splunk    .conf Online  OpenTelemetry    OpenTelemetry Agent and Collector: Telemetry Built-in Into All Software  APM    Future of Microservices and APM  DevOps    Dissecting DevOps PlayList  Observability    A murder mystery: who killed our user experience?  ","categories":"","description":"","excerpt":"Below are helpful resources about Splunk and the DevOps use case. …","ref":"/observability-workshop/v4.22/resources/","tags":"","title":"Additional Splunk for DevOps Resources"},{"body":"","categories":"","description":"**10 minutes**\n","excerpt":"**10 minutes**\n","ref":"/observability-workshop/v4.22/imt/docs/servicebureau/","tags":"","title":"Service Bureau"},{"body":"","categories":"","description":"**10 分**\n","excerpt":"**10 分**\n","ref":"/observability-workshop/v4.22/ja/imt/docs/servicebureau/","tags":"","title":"管理機能"},{"body":"We need visibility into performance - let us add metrics with Prometheus.\nInstall the Python Prometheus client as a dependency:\nShell Command   echo \"prometheus-client\" \u003e\u003e requirements.txt python3 -m venv .venv source .venv/bin/activate .venv/bin/pip install -r requirements.txt  Import the modules by editing app.py. These imports go towards the top of the file:\nimport prometheus_client from prometheus_client.exposition import CONTENT_TYPE_LATEST from prometheus_client import Counter Define a metrics endpoint before @app.route('/wordcount', methods=['POST']):\n@app.route('/metrics') def metrics(): return Response(prometheus_client.generate_latest(), mimetype=CONTENT_TYPE_LATEST) And use this python snippet after app = Flask(__name__) to define a new counter metric:\nc_recv = Counter('characters_recv', 'Number of characters received') Increase the counter metric after data = request.files['text'].read().decode('utf-8'):\nc_recv.inc(len(data)) Test that the application exposes metrics by hitting the endpoint while the app is running:\nShell Command   curl http://127.0.0.1:5000/metrics  The milestone for this task is 02service-metrics.\n","categories":"","description":"","excerpt":"We need visibility into performance - let us add metrics with …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/prometheus/","tags":"","title":"Add Prometheus metrics"},{"body":" See RUM Metrics and Session information in the RUM UI See correlated APM traces in the RUM \u0026 APM UI   1. RUM Overview Pages Visit and login into your Splunk IMT/APM/RUM Website. From the top left hamburger menu select RUM from the side menu. This will bring you to the RUM user interface.\n2. RUM Browser Overview 2.1. Header The RUM UI consists of 6 major sections. The first is the selection header, where you can set/filter a number of options:\n A drop down for the time window you’re reviewing (You are looking at the past hour in this case) A drop down to select the Comparison window (You are comparing current performance on a rolling window - in this case compared to 1 hour ago) A drop down with the available Environments to view: (Choose the one provided by the workshop host or All like in the example) A drop down list with the Various Web apps (You can use the one provided by the workshop host or use All) Optionally a drop down to select Browser or Mobile metrics (Might not be available in your workshop)  2.2. Overview Pane The next section is the overview pane: This pane will give you a quick indication to the pages with highest increase in load times (75 percentile or higher)\nThe Highest P75 Page Load Times window will show you in a quick view if the load time of your top pages has increased or has an error.\nIn the example here you can see that the first page has an error due to the red square, and you can see that the load time has drastically increased by more than 8 seconds.\nYou also see an overview of the number of Front end Error and Backend Errors per minute.\nThe last two panes show you the Top Page Views and the Top Providers.\n2.3. Custom Event Pane The Custom Event View is the location where you will find the metrics for any event you may have added yourself to the web pages you are monitoring.\nAs we have seen in the RUM enabled website, we have added the follwoing two lines:\nconst Provider = SplunkRum.provider; var tracer=Provider.getTracer('appModuleLoader'); These lines will automatically create custom Events for every new Page, and you can also add these to pieces of custom code that are not part of a framework or an event you created so you can better understand the flow though your application. We support Event Request rate, Event Error Rates and Event Latency metrics.\nThese will help better understand the flow of your website and allows you increase conversions.\n2.4. Key Metrics Pane The Key Metrics View is the location where you will find the metrics for the number of Frontend Errors per second, Endpoint Errors per second an the Endpoint Latency. These Metrics are very useful to guide you to the location of an issue if you are experiencing problems with your site.\n2.5. Web Vitals Pane The Web Vitals view is the location where you go if you wish to get insight into the experience you are delivering to your End users based on Web Vitals metrics. Web Vitals is an initiative by Google to provide unified guidance for quality signals that are essential to delivering a great user experience on the web and focuses on three key parameters:\n Largest Contentful Paint (LCP): measures loading performance. To provide a good user experience, LCP should occur within 2.5 seconds of when the page first starts loading. First Input Delay (FID): measures interactivity. To provide a good user experience, pages should have a FID of 100 milliseconds or less. Cumulative Layout Shift (CLS): measures visual stability. To provide a good user experience, pages should maintain a CLS of 0.1. or less.  2.6. Other Metrics Pane The Other Metrics Pane is the location where you find an other set of performance metrics, with a focus on initial load time of you page or showing you task that are longer then others.\n Time To First Byte (TTFB), Time to First Byte (TTFB) measures how long it takes for a client’s browser to receive the first byte of the response from the server. The longer it takes for the server to process the request and send a response, the slower your visitors' browsers start displaying your page. Long Task Length, a performance metric that can be used help developers to understand the bad user experience on the website, or can be an indication of a problem. Long Task Count, A metric to indicate how often a long task occurs, again used for exploring user experiences or problem detection.  3. RUM Mobile Overview Splunk RUM supports Native Mobile RUM, for Apple iPhone and Android Phones. You can use this to see the End-user experience of your native Smartphone app.\nThe above screen is to show you the various metrics and data Splunk Mobile RUM can track. For example:\n Custom events, similar to the Browser version. App Errors , with App Errors \u0026 Crashes per minute. App Lifecycle Performance, with Cold Startup Time, Hot Startup Time per OS. Request/Response, similar to the Browser version.  At this point we will not go deeper into Mobile RUM, due to the need to run either a native app on a phone, or run an emulation. We can provide more information in a deep dive demo if needed.\n","categories":"","description":"","excerpt":" See RUM Metrics and Session information in the RUM UI See correlated …","ref":"/observability-workshop/v4.22/rum/docs/analyzing-metrics/","tags":"","title":"Analyzing RUM Metrics"},{"body":"TODO Note on .env being overwritten\nTODO change name of environment from YOURENV to something else.\nThe development team has started using Kubernetes for container orchestration. Switch to the provided milestone 12microservices-k8s with the instructions from “Getting Started”.\nThe Kubernetes manifests are located in the k8s folder. Add auto-instrumentation to the public_api microservice deployment by configuring the [Splunk distribution of OpenTelemetry Python][splunk-otel-python]. The Dockerfile has already been prepared.\nInstall the OpenTelemetry Collector to the environment using Splunk’s helm chart and use the provided values.yaml:\nShell Command   helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart helm install my-splunk-otel-collector --set=\"splunkObservability.realm=${SPLUNK_REALM},splunkObservability.accessToken=${SPLUNK_ACCESS_TOKEN},clusterName=${CLUSTER_NAME}\" splunk-otel-collector-chart/splunk-otel-collector -f values.yaml  Rebuild the container images for the private registry:\nShell Command   docker-compose build  Push the images to the private registry:\nShell Command   docker-compose push  Deploy to the cluster with\nShell Command   kubectl apply -f k8s  Test the service with\nShell Command   ENDPOINT=$(kubectl get service/public-api -o jsonpath='{.spec.clusterIP}') curl http://$ENDPOINT:8000/api -F text=@hamlet.txt  The milestone for this task is 12microservices-k8s-autoi. It has auto-instrumentation applied for all microservices.\n","categories":"","description":"","excerpt":"TODO Note on .env being overwritten\nTODO change name of environment …","ref":"/observability-workshop/v4.22/bootcamp/docs/apm/k8s/","tags":"","title":"Instrumentation in Kubernetes"},{"body":"1 Creating a new chart Let’s now create a new chart and save it in our dashboard!\nSelect the plus icon (top right of the UI) and from the drop down, choose the option Chart. Or click on the + New Chart    Button to create a new chart.\nYou will now see a chart template like the following.\nLet’s enter a metric to plot. We are still going to use the metric demo.trans.latency.\nIn the Plot Editor tab under Signal enter demo.trans.latency.\nYou should now have a familiar line chart. Please switch the time to 15 mins.\n2. Filtering and Analytics Let’s now select the Paris datacenter to do some analytics - for that we will use a filter.\nLet’s go back to the Plot Editor tab and click on Add Filter    , wait until it automatically populates, choose demo_datacenter, and then Paris.\nIn the F(x) column, add the analytic function Percentile:Aggregation, and leave the value to 95 (click outside to confirm).\nFor info on the Percentile function and the other functions see Chart Analytics.\n 3. Using Timeshift analytical function Let’s now compare with older metrics. Click on ... and then on Clone in the dropdown to clone Signal A.\nYou will see a new row identical to A, called B, both visible and plotted.\nFor Signal B, in the F(x) column add the analytic function Timeshift and enter 1w (or 7d for 7 days), and click outside to confirm.\nClick on the cog on the far right, and choose a Plot Color e.g. pink, to change color for the plot of B.\nClick on Close.\nWe now see plots for Signal A (the past 15 minutes) as a blue plot, and the plots from a week ago in pink.\nIn order to make this clearer we can click on the Area chart icon to change the visualization.\nWe now can see when last weeks latency was higher!\nNext, click into the field next to Time on the Override bar and choose Past Hour from the dropdown.\n 4. Using Formulas Let’s now plot the difference of all metric values for a day with 7 days in between.\nClick on Enter Formula    then enter A-B (A minus B) and hide (deselect) all Signals using the eye, except C.\nWe now see only the difference of all metric values of A and B being plotted. We see that we have some negative values on the plot because a metric value of B has some times larger value than the metric value of A at that time.\nLets look at the Signalflow that drives our Charts and Detectors!\n","categories":"","description":"","excerpt":"1 Creating a new chart Let’s now create a new chart and save it in our …","ref":"/observability-workshop/v4.22/imt/docs/dashboards/filtering/","tags":"","title":"Using Filters \u0026 Formulas"},{"body":"1 新しいチャートの作成 それでは、新しいチャートを作成し、ダッシュボードに保存してみましょう。\nUIの右上にある + アイコンを選択し、ドロップダウンから Chart を選択します。 または、 + New Chart    ボタンをクリックすると、新しいチャートが作成されます。\nこれで、以下のようなチャートのテンプレートが表示されます。\nプロットするメトリックを入力してみましょう。ここでは、demo.trans.latency というメトリックを使用します。\nPlot Editor タブの Signal に demo.trans.latency を入力します。\nすると、折れ線グラフが表示されるはずです。時間を15分に切り替えてみてください。\n2. フィルタリングと分析 次に、Paris データセンターを選択して分析を行ってみましょう。そのためにはフィルタを使用します。\nPlot Editor タブに戻り、 Add Filter    をクリックして、入力補助として選択肢が出てくるので、そこから demo_datacenter を選択し、Paris を選択します。\nF(x) 欄に分析関数 Percentile:Aggregation を追加し、値を 95 にします（枠外をクリックすると設定が反映されます）。\nPercentile 関数やその他の関数の情報は、Chart Analytics を参照してください。\n 3. タイムシフト分析を追加 それでは、以前のメトリックと比較してみましょう。... をクリックして、ドロップダウンから Clone をクリックし、Signal A をクローンします。\nA と同じような新しい行が B という名前で表示され、プロットされているのがわかります。\nシグナル B に対して、F(x) 列に分析関数 Timeshift を追加し、1w（もしくは 7d でも同じ意味です）と入力し、外側をクリックして反映させます。\n右端の歯車をクリックして、Plot Color を選択（例：ピンク）すると、 B のプロットの色を変更できます。\nClose をクリックして、設定を終えます。\nシグナル A （過去15分）のプロットが青、1週間前のプロットがピンクで表示されています。\nより見やすくするために、Area chart アイコンをクリックして表示方法を変更してみましょう。\nこれで、前週にレイテンシーが高かった時期を確認することができます。\n次に、Override バーの Time の隣にあるフィールドをクリックし、ドロップダウンから **Past Hour**を 選択してみましょう。\n 4. 計算式を使う ここでは、1日と7日の間のすべてのメトリック値の差をプロットしてみましょう。\n Enter Formula    をクリックして、A-B （AからBを引いた値）を入力し、C を除くすべてのシグナルを隠します（目アイコンの選択を解除します）。\nこれで、 A と B のすべてのメトリック値の差だけがプロットされているのがわかります。B のメトリック値が、その時点での A のメトリック値よりも何倍か大きな値を持っているためです。\n次のモジュールで、チャートとディテクターを動かすための SignalFlow を見てみましょう。\n","categories":"","description":"","excerpt":"1 新しいチャートの作成 それでは、新しいチャートを作成し、ダッシュボードに保存してみましょう。\nUIの右上にある + アイコンを選択し、ド …","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/filtering/","tags":"","title":"フィルタと数式の使い方"},{"body":"You will need an access token for Splunk Observability Cloud. Set them up as environment variables:\nexport SPLUNK_ACCESS_TOKEN=YOURTOKEN export SPLUNK_REALM=YOURREALM Start with the default configuration for the OpenTelemetry Collector and name it collector.yaml in the src directory.\nYou can also start with a blank configuration, which is what the milestone does for clarity.\nThen run OpenTelemetry Collector with this configuration in a docker container:\nShell Command   docker run --rm \\  -e SPLUNK_ACCESS_TOKEN=${SPLUNK_ACCESS_TOKEN} \\  -e SPLUNK_REALM=${SPLUNK_REALM} \\  -e SPLUNK_CONFIG=/etc/collector.yaml \\  -p 13133:13133 -p 14250:14250 -p 14268:14268 -p 4317:4317 \\  -p 6060:6060 -p 8888:8888 -p 9080:9080 -p 9411:9411 -p 9943:9943 \\  -v \"${PWD}/collector.yaml\":/etc/collector.yaml:ro \\  --name otelcol quay.io/signalfx/splunk-otel-collector:0.41.1  The milestone for this task is 03service-metrics-otel.\n","categories":"","description":"","excerpt":"You will need an access token for Splunk Observability Cloud. Set them …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/otel/","tags":"","title":"Deploy OpenTelemetry in Docker"},{"body":"1. Introduction Let’s take a look at SignalFlow - the analytics language of Observability Cloud that can be used to setup monitoring as code.\nThe heart of Splunk Infrastructure Monitoring is the SignalFlow analytics engine that runs computations written in a Python-like language. SignalFlow programs accept streaming input and produce output in real time. SignalFlow provides built-in analytical functions that take metric time series (MTS) as input, perform computations, and output a resulting MTS.\n Comparisons with historical norms, e.g. on a week-over-week basis Population overviews using a distributed percentile chart Detecting if the rate of change (or other metric expressed as a ratio, such as a service level objective) has exceeded a critical threshold Finding correlated dimensions, e.g. to determine which service is most correlated with alerts for low disk space  Infrastructure Monitoring creates these computations in the Chart Builder user interface, which lets you specify the input MTS to use and the analytical functions you want to apply to them. You can also run SignalFlow programs directly by using the SignalFlow API.\nSignalFlow includes a large library of built-in analytical functions that take a metric time series as an input, performs computations on its datapoints, and outputs time series that are the result of the computation.\nInfo\nFor more information on SignalFlow see Analyze incoming data using SignalFlow.   2. View SignalFlow In the chart builder, click on View SignalFlow.\nYou will see the SignalFlow code that composes the chart we were working on. You can now edit the SignalFlow directly within the UI. Our documentation has the full list of SignalFlow functions and methods.\nAlso, you can copy the SignalFlow and use it when interacting with the API or with Terraform to enable Monitoring as Code\nSignalFlow   A = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).publish(label='A', enable=False) B = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).timeshift('1w').publish(label='B', enable=False) C = (A-B).publish(label='C')  Click on View Builder to go back to the Chart Builder UI.\nLet’s save this new chart to our Dashboard!\n","categories":"","description":"","excerpt":"1. Introduction Let’s take a look at SignalFlow - the analytics …","ref":"/observability-workshop/v4.22/imt/docs/dashboards/signalflow/","tags":"","title":"SignalFlow"},{"body":"1. はじめに ここでは、Observability Cloud の分析言語であり、Monitoring as Codeを実現するために利用する SignalFlow について見てみましょう。\nSplunk Infrastructure Monitoring の中心となるのは、Python ライクな、計算を実行する SignalFlow 分析エンジンです。SignalFlow のプログラムは、ストリーミング入力を受け取り、リアルタイムで出力します。SignalFlow には、時系列メトリック（MTS）を入力として受け取り、計算を実行し、結果の MTS を出力する分析関数が組み込まれています。\n 過去の基準との比較する（例：前週との比較） 分布したパーセンタイルチャートを使った母集団の概要を表示する 変化率（またはサービスレベル目標など、比率で表されるその他の指標）が重要な閾値を超えたかどうか検出する 相関関係にあるディメンジョンの発見する（例：どのサービスの挙動がディスク容量不足の警告と最も相関関係にあるかの判断する）  Infrastructure Monitoring は、Chart Builder ユーザーインターフェイスでこれらの計算を行い、使用する入力 MTS とそれらに適用する分析関数を指定できます。また、SignalFlow API を使って、SignalFlow のプログラムを直接実行することもできます。\nSignalFlow には、時系列メトリックを入力とし、そのデータポイントに対して計算を行い、計算結果である時系列メトリックを出力する、分析関数の大規模なライブラリが組み込まれています。\n!!! info SignalFlow の詳細については、Analyze incoming data using SignalFlow を参照してください。\n2. SignalFlow の表示 チャートビルダーで View SignalFlow をクリックします。\n作業していたチャートを構成する SignalFlow のコードが表示されます。UI内で直接 SignalFlow を編集できます。ドキュメントには、SignalFlow の関数やメソッドの全てのリスト が掲載されています。\nまた、SignalFlow をコピーして、API や Terraform とやり取りする際に使用して、Monitoring as Code を実現することもできます。\nSignalFlow   A = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).publish(label='A', enable=False) B = data('demo.trans.latency', filter=filter('demo_datacenter', 'Paris')).percentile(pct=95).timeshift('1w').publish(label='B', enable=False) C = (A-B).publish(label='C')  View Builder をクリックすると、Chart Builder の UI に戻ります。\nこの新しいチャートをダッシュボードに保存してみましょう!\n","categories":"","description":"","excerpt":"1. はじめに ここでは、Observability Cloud の分析言語であり、Monitoring as Codeを実現するために利用 …","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/signalflow/","tags":"","title":"SignalFlow"},{"body":"The business teams want to add the service version, the customer profile that is defined by a color and the name of the analyzed file.\nSwitch to the provided milestone 12microservices-k8s-autoi with the instructions from “Getting Started”.\nImplement the requested changes to the public-api microservice.\nThe Kubernetes manifests are located in the k8s folder. Add the service version by configuring the OpenTelemetry resource attributes.\nThe customer profile and the file name vary by request. Create attributes and assign them to the current span with the OpenTelemetry Python API.\nYou can use a random function to generate the customer profile (e.g. red, blue, green) with this snippet:\ncolor = random.choice(['red', 'blue', 'green']) Note 1: Do not use a temporary variable to retrieve the current span. Use the trace directly.\nNote 2: Make sure to import modules.\nRebuild the container images for the private registry:\nShell Command   docker-compose build  Push the images to the private registry:\nShell Command   docker-compose push  Delete the public-api deployment:\nShell Command   kubectl delete deploy public-api  Redeploy to the cluster with\nShell Command   kubectl apply -f k8s  Test the service with\nShell Command   ENDPOINT=$(kubectl get service/public-api -o jsonpath='{.spec.clusterIP}') curl http://$ENDPOINT:8000/api -F text=@hamlet.txt  Verify in Splunk APM that traces contain the desired informations: TODO screenshot\n[Create a new indexed span tag][index-span-tag] so that the business team is able to breakdown performance per customer profile.\nThe milestone for this task is 13custom-instr. It adds the described custom instrumentation.\n","categories":"","description":"","excerpt":"The business teams want to add the service version, the customer …","ref":"/observability-workshop/v4.22/bootcamp/docs/apm/api/","tags":"","title":"Using OpenTelementry in Instrumentation"},{"body":" Look into the Metrics views for the various endpoints and use the Tags sent via the Tag spotlight for deeper analysis   1. Find an url for the Cart endpoint From the RUM Overview page, please select the url for the Cart endpoint to dive deeper into the information available for this endpoint.\nOnce you have selected and clicked on the blue url, you will find yourself in the Tag Spotlight overview\nHere you will see all of the tags that have been sent to Splunk RUM as part of the RUM traces. The tags displayed will be relevant to the overview that you have selected. These are generic Tags created automatically when the Trace was sent, and additional Tags you have added to the trace as part of the configuration of your website.\n!!! Additional Tags We are already sending two additional tags, you have seen them defined in the Beacon url that was added to your website: app: “[nodename]-rum-app”, environment: “[nodename]-rum-env” in the first section of this workshop! You can add additional tag in a similar way.\nIn our example we have selected the Document Load Latency view as shown here:\nYou can select any of the following Tag views, each focused on a specific metric.\n 2. Explore the information in the Tag Spotlight view The Tag spotlight is designed to help you identify problems, either through the chart view,, where you may quickly identify outliers or via the TAGs.\nIn the Document Load Latency view, if you look at the Browser, Browser Version \u0026 OS Name Tag views,you can see the various browser types and versions, as well as for the underlying OS.\nThis makes it easy to identify problems related to specific browser or OS versions, as they would be highlighted.\nIn the above example you can see that Firefox had the slowest response, various Browser versions ( Chrome) that have different response times and the slow response of the Android devices.\nA further example are the regional Tags that you can use to identify problems related to ISP or locations etc. Here you should be able to find the location you have been using to access the Online Boutique. Drill down by selecting the town or country you are accessing the Online Boutique from by clicking on the name as shown below (City of Amsterdam):\nThis will select only the traces relevant to the city selected as shown below:\nBy selecting the various Tag you build up a filter, you can see the current selection below\nTo clear the filter and see every trace click on Clear All at the top right of the page.\nIf the overview page is empty or shows , no traces have been received in the selected timeslot. You need to increase the time window at the top left. You can start with the Last 12 hours for example.\nYou can then use your mouse to select the time slot you want like show in the view below and activate that time filter by clicking on the little spyglass icon.\n","categories":"","description":"","excerpt":" Look into the Metrics views for the various endpoints and use the …","ref":"/observability-workshop/v4.22/rum/docs/tag-spotlight/","tags":"","title":"Analyzing RUM Tags in the Tag Spotlight view"},{"body":"Event Situation: Users are calling in to say the application is running slow\nDiscussion See if you can find out what is going on in the system that is causing the issue.\nQuestions to consider  Do we have all of the information we need to troubleshoot the issue? Is this information be alertable? Is alerting on this data the best way to deal with the problem? The users contacted us. How else could we find out about application slowness before a user calls us? How much time did it take before we found out about this issue? What could be the cost of those delays?  STOP: Don’t move past here until you have discussed the situation.\nGo to Event 1 - Wrapup\n","categories":"","description":"","excerpt":"Event Situation: Users are calling in to say the application is …","ref":"/observability-workshop/v4.22/realworld/docs/event1/","tags":"","title":"Event 1"},{"body":"Let’s now save our chart.\n 1. Save to existing dashboard Check that you have YOUR_NAME-Dashboard: YOUR_NAME-Dashboard in the top left corner.\nThis means you chart will be saved in this Dashboard.\nName the Chart Latency History and add a Chart Description if you wish.\nClick on Save And Close   . This returns you to your dashboard that now has two charts!\nNow let’s quickly add another Chart based on the previous one.\n 2. Copy and Paste a chart Click on the three dots ... on the Latency History chart in your dashboard and then on Copy.\nYou see the chart being copied, and you should now have a red circle with a white 1 next to the + on the top left of the page.\nClick on the at the top of the page, and then in the menu on Paste Charts (There should also be a red dot with a 1 visible at the end of the line).\nThis will place a copy of the previous chart in your dashboard.\n 3. Edit the pasted chart Click on the three dots ... on one of the Latency History charts in your dashboard and then on Open (or you can click on the name of the chart which here is Latency History).\nThis will bring you to the editor environment again.\nFirst set the time for the chart to -1 hour in the Time box at the top right of the chart. Then to make this a different chart, click on the eye icon in front of signal “A” to make it visible again, and then hide signal “C” via the eye icon and change the name for Latency history to Latency vs Load.\nClick on the Add Metric Or Event    button. This will bring up the box for a new signal. Type and select demo.trans.count for Signal D.\nThis will add a new Signal D to your chart, It shows the number of active requests. Add the filter for the demo_datacenter:Paris, then change the Rollup type by clicking on the Configure Plot button and changing the roll-up from Auto (Delta) to Rate/sec. Change the name from demo.trans.count to Latency vs Load.\nFinally press the Save And Close    button. This returns you to your dashboard that now has three different charts!\nLet’s add an “instruction” note and arrange the charts!\n","categories":"","description":"","excerpt":"Let’s now save our chart.\n 1. Save to existing dashboard Check that …","ref":"/observability-workshop/v4.22/imt/docs/dashboards/adding-charts/","tags":"","title":"Adding charts to dashboards"},{"body":"Add a prometheus receiver to the OpenTelemetry Collector configuration so that it captures the metrics introduced in Task 2 from the application.\nHint: The hostname host.docker.internal allows you to access the host from within a docker container. Add\n--add-host=host.docker.internal:host-gateway to the docker run command for the OpenTelemetry collector. TODO test instructions\nValidate that you are getting data for the custom metric characters_recv_total introduced in Task 2.\nThe milestone for this task is 04service-metrics-prom.\n","categories":"","description":"","excerpt":"Add a prometheus receiver to the OpenTelemetry Collector configuration …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/otel-prometheus/","tags":"","title":"Capture Prometheus metrics"},{"body":" Dive into RUM Session information in the RUM UI Identify Javascript errors in the Span of an user interaction   1. Again select the cart URL After you have focussed the time slot with the time selector, you need to reselect the cart url from Url Name view, as shown below:\nIn the example above we selected http://34.246.124.162:81/cart\n2. Drill down in the Sessions After you have analyzed the information and drilled down via the Tag Spotlight to a subset of the traces, you can view the actual session as it was run by the end-user’s browser.\nYou do this by clicking on the link Example Sessions as shown below:\nThis will give you a list of sessions that matched both the time filter and the subset selected in the Tag Profile.\nSelect one by clicking on the session ID, It is a good idea to select one that has the longest duration (preferably over 700 ms).\nOnce you have selected the session, you will be taken to the session details page. As you are selecting a specific action that is part of the session, you will likely arrive somewhere in the middle of the session, at the moment of the interaction.\nYou can see the URL http://34.246.124.162:81/cart, the one you selected earlier, is where we are focusing on in the session stream.\nScroll down a little bit on the page, so you see the end of the operation as shown below.\nYou can see that we have received a few Javascript Console errors that may not have been detected or visible to the end users. To examine these in more detail click on the middle one that says: *Cannot read properties of undefined (reading ‘Prcie’)\nThis will cause the page to expand and show the Span detail for this interaction, It will contain a detailed error.stack you can pass on the developer to solve the issue. You may have noticed when buying in the Online Boutique that the final total always was $0.00.\n","categories":"","description":"","excerpt":" Dive into RUM Session information in the RUM UI Identify Javascript …","ref":"/observability-workshop/v4.22/rum/docs/rum-sessions/","tags":"","title":"Analyzing RUM Sessions"},{"body":"Review Questions Questions to consider  Do we have all of the information we need to troubleshoot the issue?  Yes, we were able to find in Kubernetes Navigator that we ran low on disk.   Is this information be alertable? Is alerting on this data the best way to deal with the problem?  Yes, we can create a “Resources Running Out” alert on disk, for example.   The users contacted us. How else could we find out about application slowness before a user calls us?  One way is to use Synthetics, which can test the application and identify latency. Another way is to use Real User Monitoring, to identify slowness that our users experience.   How much time did it take before we found out about this issue? What could be the cost of those delays?  Not every user that is impacted will report the issue. So often there is a long delay between when an issue is reported and that information gets to a person that can solve the issue. In short: TOO long.    Go to Event 2\n","categories":"","description":"","excerpt":"Review Questions Questions to consider  Do we have all of the …","ref":"/observability-workshop/v4.22/realworld/docs/event1-wrapup/","tags":"","title":"Event 1 - Wrapup"},{"body":"それでは、チャートを保存してみましょう。\n 1. 既存のダッシュボードに保存する 右上に YOUR_NAME-Dashboard と表示されていることを確認しましょう\nこれは、あなたのチャートがこのダッシュボードに保存されることを意味します。\nチャートの名前を Latency History とし、必要に応じてチャートの説明を追加します。\n Save And Close    をクリックします。これで、ダッシュボードに戻ると2つのチャートが表示されているはずです！\nでは、先ほどのチャートを元に、もう一つのチャートをさくっと追加してみましょう。\n 2. チャートのコピー＆ペースト ダッシュボードの Latency History チャート上の3つのドット ... をクリックし、 Copy をクリックします。\nページ左上の + の横に赤い円と白い1が表示されていれば、チャートがコピーされているということになります。\nページ上部の をクリックし、メニューの Paste Charts をクリックしてください (また、右側に 1 が見える赤い点があるはずです)。\nこれにより、先程のチャートのコピーがダッシュボードに配置されます。\n 3. 貼り付けたチャートを編集する ダッシュボードの Latency History チャートの3つの点 ... をクリックし、Open をクリックします（または、チャートの名前（ここでは Latency History）をクリックすることもできます）。\nすると、再び編集できる環境になります。\nまず、チャートの右上にあるタイムボックスで、チャートの時間を -1h（1時間前から現在まで） に設定します。そして、シグナル「A」の前にある目のアイコンをクリックして再び表示させ、「C」 を非表示にし、Latency history の名前を Latency vs Load に変更します。\n Add Metric Or Event    ボタンをクリックします。これにより、新しいシグナルのボックスが表示されます。シグナル D に demo.trans.count と入力・選択します。\nこれにより、チャートに新しいシグナル D が追加され、アクティブなリクエストの数が表示されます。demo_datacenter:Paris のフィルタを追加してから、Delta Rollup をクリック（または歯車のアイコンをクリック）し、ロールアップタイプを変更します。\nビジュアライゼーションのパネルが開いたら、Rollup ドロップダウンを Rollup:Rate/sec に変更し、左上の名前フィールドをクリックして Latency vs load に変更し、 Save And Close    ボタンを押します。これでダッシュボードに戻り、3つの異なるチャートが表示されます。\n次のモジュールでは、「説明」のメモを追加して、チャートを並べてみましょう!\n","categories":"","description":"","excerpt":"それでは、チャートを保存してみましょう。\n 1. 既存のダッシュボードに保存する 右上に YOUR_NAME-Dashboard と表示され …","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/adding-charts/","tags":"","title":"ダッシュボードにチャートを追加する"},{"body":" Continue with the RUM Session information in the RUM UI See correlated APM traces and logs in the APM \u0026 Log Observer UI   1. Finding backend service issues Click on the to close the Span view. Now continue to scroll down and find the POST /cart/checkout line.\nClick on the blue link, this should pop up a dialog showing information on the backend services that were part of the checkout action taken by the end user.\nIn this popup, there are multiple sections available, providing you with a quick insight in the behavior of your backend services. For example the Performance Summary section will tell you where the time was spent during the backend call.\nIn the above example you can see that more than 77,9% was spent in external services.\nIf you scroll down to the bottom of the dialog, you can see the complete Trace and Services section like shown below:\nin the Services map, you can see two services flagged red, the Checkout Service and the Payment Service in both in dark red. Light red means it received an error and dark red means an error originated from that service.\nSo already it is obvious there is a problem in the back end services.\nLet’s investigate!\n2. Follow the Trace to the Backend service You can now click on the Trace Id link:\nThis will bring you to the Waterfall APM view that will show you what occurred in detail in a call to the backend services. On the right you see the Trace Id: and again the Performance Summary, as we saw before. In the waterfall, you can identify the various backend services that were part of this call from the frontend.\nAs you can see there are red error indicators before the Checkout Service and the Payment Service.\nClick on the after the paymentservice: grpc.hipstershop.PaymentService/Charge line.\nThis will open the span detail page to show you the detailed information about this service call. You wil see that the call returned a 401 error code or Invalid Request.\n3. Use the Related Content - Logs As the Splunk Observability cloud suite correlates trace metrics and logs automatically, the system will show you in the related content bar at the bottom of the page, the corresponding logs for this trace.\nClick on the Log link to see the logs.\nWhen the logs are shown, notice that the filter at the top of the page contains the logs for the trace. Next select one of the lines indicating an error for the payment service. This should open the log message on the right.\nIt clearly shows the reason why the payment service fails: we are using an invalid token towards the service:\n*Failed payment processing through ButtercupPayments: Invalid API Token (test-20e26e90-356b-432e-a2c6-956fc03f5609)\n4. Conclusion In the workshop, you have seen how to add RUM functionality to you website. We investigate the performance of your Website using RUM Metrics. Using the Tag profile, you have searched for your own session, and with the session waterfall, you identified two problems:\n A Java script error that caused your price calculation to be zero. An issue in the payment backend service that caused payments to fail.  Using the ability to correlate RUM traces with the Backend APM trace and Logs, you have found the reason for the payment failure.\nThis concludes the RUM workshop.\n","categories":"","description":"","excerpt":" Continue with the RUM Session information in the RUM UI See …","ref":"/observability-workshop/v4.22/rum/docs/apm-correlation/","tags":"","title":"Correlate between Splunk RUM and APM backend services"},{"body":"Dockerize the service. Use this Dockerfile as a skeleton:\nARG APP_IMAGE=python:3FROM$APP_IMAGE as baseFROMbase as builderWORKDIR/appRUN python -m venv .venv \u0026\u0026 .venv/bin/pip install --no-cache-dir -U pip setuptoolsCOPY requirements.txt .RUN .venv/bin/pip install -r requirements.txt --no-cache-dir -r requirements.txtFROMbaseWORKDIR/appCOPY --from=builder /app /appCOPY app.py .ENV PATH=\"/app/.venv/bin:$PATH\"Add the appropriate CMD at the end to launch the app.\nStop other instances of the app if you had any running.\nThen build and run the image:\nShell Command   docker build . -t wordcount docker run -p 5000:5000 wordcount:latest  Test the service in another shell:\nShell Command   curl -X POST http://127.0.0.1:5000/wordcount -F text=@hamlet.txt  The milestone for this task is 05docker.\n","categories":"","description":"","excerpt":"Dockerize the service. Use this Dockerfile as a skeleton:\nARG …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/docker/","tags":"","title":"Dockerize the Service"},{"body":"1. Adding Notes Often on dashboards it makes sense to place a short “instruction” pane that helps users of a dashboard.\nLets add one now by clicking on the New Text Note    Button.\nThis will open the notes editor.\nTo allow you to add more then just text to you notes, Splunk is allowing you to use Markdown in these notes/panes. Markdown is a lightweight markup language for creating formatted text using plain-text often used in Webpages.\nThis includes (but not limited to):\n Headers. (in various sizes) Emphasis styles. Lists and Tables. Links. These can be external webpages (for documentation for example) or directly to other Splunk IMT Dashboards  Below is an example of above Markdown options you can use in your note.\nSample Markdown text   # h1 Big headings  ###### h6 To small headings  ##### Emphasis  **This is bold text**, *This is italic text* , ~~Strikethrough~~ ##### Lists  Unordered + Create a list by starting a line with `+`, `-`, or `*` - Sub-lists are made by indenting 2 spaces: - Marker character change forces new list start: * Ac tristique libero volutpat at + Facilisis in pretium nisl aliquet * Very easy! Ordered 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa ##### Tables  | Option | Description | | ------ | ----------- | | chart | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | #### Links  [link to webpage](https://www.splunk.com) [link to dashboard with title](https://app.eu0.signalfx.com/#/dashboard/EaJHrbPAEAA?groupId=EaJHgrsAIAA\u0026configId=EaJHsHzAEAA \"Link to the Sample chart Dashboard!\")  Copy the above by using the copy button and paste it in the Edit box. the preview will show you how it will look.\n 2. Saving our chart Give the Note chart a name, in our example we used Example text chart, then press the Save And Close    Button.\nThis will bring you back to you Dashboard, that now includes the note.\n 3. Ordering \u0026 sizing of charts If you do not like the default order and sizes of your charts you can simply use window dragging technique to move and size them to the desired location.\nGrab the top border of a chart and you should see the mouse pointer change to a drag icon (see picture below).\nNow drag the Latency vs Load chart to sit between the Latency History Chart and the Example text chart.\nYou can also resize windows by dragging from the left, right and bottom edges.\nAs a last exercise reduce the width of the note chart to about a third of the other charts. The chart will automatically snap to one of the sizes it supports. Widen the 3 other charts to about a third of the Dashboard. Drag the notes to the right of the others and resize it to match it to the 3 others.\nSet the time to -1 h hour and you should have the following dashboard!\nOn to Detectors!\n","categories":"","description":"","excerpt":"1. Adding Notes Often on dashboards it makes sense to place a short …","ref":"/observability-workshop/v4.22/imt/docs/dashboards/dashboarding/","tags":"","title":"Adding Notes and Dashboard Layout"},{"body":"1. メモの追加 ダッシュボードには、ダッシュボードの利用者を支援するための短い「説明」ペインを配置することがよくあります。\nここでは、 New Text Note    ボタンをクリックして、ノートを追加してみましょう。\nすると、ノートエディターが開きます。\nノートに単なるテキスト以外のものを追加できるように、Splunk ではこれらのノート/ペインで Markdown を使用できるようにしています。 Markdown は、ウェブページでよく使われるプレーンテキストを使ってフォーマットされたテキストを作成するための軽量なマークアップ言語です。\nたとえば、以下のようなことができます (もちろん、それ以外にもいろいろあります)。\n ヘッダー (様々なサイズで) 強調スタイル リストとテーブル リンク: 外部の Web ページ (ドキュメントなど) や他の Splunk IMT ダッシュボードへの直接リンクできます  以下は、ノートで使用できる上記のMarkdownオプションの例です。\nSample Markdown text   # h1 Big headings  ###### h6 To small headings  ##### Emphasis  **This is bold text**, *This is italic text* , ~~Strikethrough~~ ##### Lists  Unordered + Create a list by starting a line with `+`, `-`, or `*` - Sub-lists are made by indenting 2 spaces: - Marker character change forces new list start: * Ac tristique libero volutpat at + Facilisis in pretium nisl aliquet * Very easy! Ordered 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa ##### Tables  | Option | Description | | ------ | ----------- | | chart | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | #### Links  [link to webpage](https://www.splunk.com) [link to dashboard with title](https://app.eu0.signalfx.com/#/dashboard/EaJHrbPAEAA?groupId=EaJHgrsAIAA\u0026configId=EaJHsHzAEAA \"Link to the Sample chart Dashboard!\")  上記をコピーボタンでコピーして、Edit ボックスにペーストしてみてください。 プレビューで、どのように表示されるか確認できます。\n 2. チャートの保存 ノートチャートに名前を付けます。この例では、Example text chart としました。そして、 Save And Close    ボタンを押します。\nこれでダッシュボードに戻ると、メモが追加されました。\n 3. チャートの順序や大きさを変更 デフォルトのチャートの順番やサイズを変更したい場合は、ウィンドウをドラッグして、チャートを好きな場所に移動したり、サイズを変更したりすることができます。\nチャートの 上側の枠 にマウスポインタを移動すると、マウスポインタがドラッグアイコンに変わります。これで、チャートを任意の場所にドラッグすることができます。\nここでは、Latency History　チャートを　Latency vs Load　チャートの下に移動してください。\nチャートのサイズを変更するには、側面または底面をドラッグします。\n最後の練習として、ノートチャートの幅を他のチャートの3分の1程度にしてみましょう。チャートは自動的に、サポートしているサイズの1つにスナップします。他の3つのチャートの幅を、ダッシュボードの約3分の1にします。ノートを他のチャートの左側にドラッグして、他の23個のチャートに合わせてサイズを変更します。\n最後に、時間を -1h に設定すると、以下のようなダッシュボードになります。\n次は、ディテクターの登場です！\n","categories":"","description":"","excerpt":"1. メモの追加 ダッシュボードには、ダッシュボードの利用者を支援するための短い「説明」ペインを配置することがよくあります。\nここでは、 …","ref":"/observability-workshop/v4.22/ja/imt/docs/dashboards/dashboarding/","tags":"","title":"ノートの追加とダッシュボードのレイアウト"},{"body":" Use RUM Metrics to set up Alerts to be warned in case of an issue Create a Custom Chart based on RUM Metrics   1. Overview The fact that Splunk’s RUM is designed as a full fidelity solution, and thus can take 100% of your traces, allows it to detect and alert you to any change to the behavior of your website. It also give you the ability to to give you accurate insight in how your website is behaving by allowing you to creating custom Chart and Dashboards. This allows you to combine data from your Website, Backend service and underlying Infrastructure. Allowing you to observe the complete stack that makes up your application/solution.\nCreating charts or alerts for RUM Metrics are done in the same way as we do for Infrastructure Metrics. In this section we will create a simple chart, detector and alert.\nIf you previously done the Splunk IMT Part of the Workshop, you will find this section very familiar. If you have not done the Splunk IMT workshop before, it is recommended that you run though the Dashboards and Detectors modules after completing the RUM workshop to get a better understanding of the capabilities.\n2. Create an alert on one of the RUM Metrics From the top left hamburger menu icon click Alerts in the menu and then select Detectors.\n3. Create a Chart based on Rum Metrics 3.1 Overview Creating charts or alerts for RUM Metrics are done in the same way as we do for Infrastructure Metrics. In this section we will create a simple chart, detector and alert If you previously done the Splunk IMT Part of the Workshop, you will find this section very familiar.\nu have added to the trace as part of the configuration of your website.\nAddtional Tags\nWe are already sending two additional tags, you have seen them defined in the Beacon url that was added to your website in the first section of this workshop! You can add additional tag in a similar way.\napp: \"[nodename]-rum-app\", environment: \"[nodename]-rum-env\"    ","categories":"","description":"","excerpt":" Use RUM Metrics to set up Alerts to be warned in case of an issue …","ref":"/observability-workshop/v4.22/rum/docs/alerting/","tags":"","title":"Custom Charts and Alerts based on RUM Metrics"},{"body":"The development team wants to use a containerized redis cache to improve performance of the service.\nStop any other running containers from this app or the OpenTelemetry Collector.\nAdd a docker-compose.yaml file for the python app to prepare us for running multiple containers.\nA skeleton to run the service on port 8000 might look like this. What port do you need to map 8000 to for the service to work?\nversion: '3'services: yourservicename: build: . expose: - \"8000\" ports: - \"8000:XXXX\"Build the service:\nShell Command   docker-compose build  Then run the whole stack:\nShell Command   docker-compose up  Test the service with curl by hitting the exposed port.\nThe milestone for this task is 06docker-compose.\n","categories":"","description":"","excerpt":"The development team wants to use a containerized redis cache to …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/docker-compose/","tags":"","title":"Container Orchestration"},{"body":"Add the OpenTelemetry Collector service definition to the docker-compose setup.\nRebuild the docker-compose stack and run it.\nThe milestone for this task is 07docker-compose-otel.\n","categories":"","description":"","excerpt":"Add the OpenTelemetry Collector service definition to the …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/otel-docker-compose/","tags":"","title":"Deploy OpenTelemetry in Docker Compose"},{"body":"The development team has started using other containerized services with docker compose. Switch to the provided milestone 08docker-compose-redis with the instructions from “Getting Started”.\nAdd the redis monitor to the OpenTelemetry Collector configuration in collector.yaml to get metrics from the [redis cache].\nRebuild the docker-compose stack and run it.\nCheck that you are getting data in the Redis dashboard:\nThe milestone for this task is 08docker-compose-redis-otel.\n","categories":"","description":"","excerpt":"The development team has started using other containerized services …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/otel-container-svc/","tags":"","title":"Monitor Containerized Services"},{"body":"Event Situation: XXX\nDiscussion See if you can find out what is going on in the system that is causing the issue.\nQuestions to consider  Do we have all of the information we need to troubleshoot the issue? Is this information be alertable? Is alerting on this data the best way to deal with the problem? The users contacted us. How else could we find out about application slowness before a user calls us? How much time did it take before we found out about this issue? What could be the cost of those delays?  STOP: Don’t move past here until you have discussed the situation.\nGo to Event 2 - Wrapup\n","categories":"","description":"","excerpt":"Event Situation: XXX\nDiscussion See if you can find out what is going …","ref":"/observability-workshop/v4.22/realworld/docs/event2/","tags":"","title":"Event 2"},{"body":"Review Questions Questions to consider: *\nGo to Event 3\n","categories":"","description":"","excerpt":"Review Questions Questions to consider: *\nGo to Event 3\n","ref":"/observability-workshop/v4.22/realworld/docs/event2-wrapup/","tags":"","title":"Event 2 - Wrapup"},{"body":"The development team has started using Kubernetes for container orchestration. Switch to the provided milestone 09k8s with the instructions from “Getting Started”.\nRebuild the container images for the private registry:\nShell Command   docker-compose build  Push the images to the private registry:\nShell Command   docker-compose push  Then deploy the services into the cluster:\nShell Command   kubectl apply -f k8s  Test the service with\nShell Command   ENDPOINT=$(kubectl get service/wordcount -o jsonpath='{.spec.clusterIP}') curl http://$ENDPOINT:8000/wordcount -F text=@hamlet.txt  Configure and install an OpenTelemetry Collector using Splunk's helm chart:\n  Review the configuration how-to and the advanced configuration to create a values.yaml that adds the required receivers for redis and prometheus.\n  Use the environment variables for realm,token and cluster name and pass them to helm as arguments.\n  The milestone for this task is 09k8s-otel.\n","categories":"","description":"","excerpt":"The development team has started using Kubernetes for container …","ref":"/observability-workshop/v4.22/bootcamp/docs/gdi/k8s/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/realworld/docs/event3/","tags":"","title":"Event 3"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/realworld/docs/event4/","tags":"","title":"Event 4"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/otelw/apm_ecs_demos/","tags":"","title":"ECS Demos"},{"body":" 組織におけるObservability Cloudの利用状況を把握する Billing and Usage（課金と使用量）インターフェースを使って、使用量を追跡する チームを作成する チームへの通知ルールを管理する 使用量をコントロールする   1. 利用状況を把握する 組織内のObservability Cloudのエンゲージメントを完全に把握するには、左下 » を開き、Settings → Organization Overview を選択すると、Observability Cloud の組織がどのように使用されているかを示す以下のダッシュボードが表示されます。\n左側のメニューには、メンバーのリストが表示され、右側には、ユーザー数、チーム数、チャート数、ダッシュボード数、ダッシュボードグループの作成数、様々な成長傾向を示すチャートが表示されます。\n現在お使いのワークショップ組織では、ワークショップごとにデータが消去されるため、作業できるデータが少ないかもしれません。\nこのワークショップインスタンスの Organization Overview にある様々なチャートをじっくりとご覧ください。\n2. Billing and Usage 契約に対する使用量を確認したい場合は、 Billing and Usage を選択します。\nこの画面では、使用量を計算して取り込むため、読み込みに数秒かかることがあります。\n3. 使用量を理解する 下図のような画面が表示され、現在の使用量、平均使用量、およびホスト、コンテナ、カスタムメトリクス、高解像度メトリクスの各カテゴリごとの権利の概要が表示されます。\nこれらのカテゴリの詳細については、Billing and Usage information を参照してください。\n 4. 使用状況を詳しく調べる 一番上のチャートには、カテゴリーごとの現在のサブスクリプションレベルが表示されます（下のスクリーンショットでは、上部の赤い矢印で表示されています）。\nまた、4つのカテゴリーの現在の使用状況も表示されます（チャート下部の赤い線で示されています）。\nこの例では、「ホスト」が25個、「コンテナ」が0個、「カスタムメトリクス」が100個、「高解像度メトリクス」が0個であることがわかります。\n下のグラフでは、現在の期間のカテゴリごとの使用量が表示されています（グラフの右上のドロップダウンボックスに表示されています）。\nAverage Usage と書かれた青い線は、Observability Cloudが現在の請求期間の平均使用量を計算するために使用するものを示しています。\nInfo\nスクリーンショットからわかるように、Observability Cloudはコスト計算には最大値や95パーセンタイル値ではなく、実際の平均時間使用量を使用しています。これにより、超過料金のリスクなしに、パフォーマンステストやBlue/Greenスタイルのデプロイメントなどを行うことができます。   オプションを確認するには、左の Usage Metric ドロップダウンから異なるオプションを選択して表示するメトリックを変更するか、右のドロップダウンで Billing Period を変更します。\nまた、右側のドロップダウンで請求期間を変更することもできます。\n最後に、右側のペインには、お客様のサブスクリプションに関する情報が表示されます。\n","categories":"","description":"","excerpt":" 組織におけるObservability Cloudの利用状況を把握する Billing and Usage（課金と使用量）インターフェース …","ref":"/observability-workshop/v4.22/ja/imt/docs/servicebureau/billing-and-usage/","tags":"","title":"Billing and Usage"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/realworld/docs/event5/","tags":"","title":"Event 5"},{"body":" How to keep track of the usage of Observability Cloud in your organization Learn how to keep track of spend by exploring the Subscription Usage interface Creating Teams Adding notification rules to Teams Controlling usage   1. Understanding engagement To fully understand Observability Cloud engagement inside your organization, click on the » bottom left and select the Settings → Organization Overview, this will provide you with the following dashboards that shows you how your Observability Cloud organization is being used:\nYou will see various dashboards such as Throttling, System Limits, Entitlements \u0026 Engagement. The workshop organization you’re using now may have less data to work with as this is cleared down after each workshop.\nTake a minute to explore the various dashboards and charts in the Organization Overview of this workshop instance.\n2. Subscription Usage If you want to see what your usage is against your subscription you can select Subscription Usage.\nThis screen may take a few seconds to load whilst it calculates and pulls in the usage.\n3. Understanding usage You will see a screen similar to the one below that will give you an overview of the current usage, the average usage and your entitlement per category: Hosts, Containers, Custom Metrics and High Resolution Metrics.\nFor more information about these categories please refer to Monitor Splunk Infrastructure Monitoring subscription usage.\n 4. Examine usage in detail The top chart shows you the current subscription levels per category (shown by the red arrows at the top in the screenshot below).\nAlso, your current usage of the four catagories is displayed (shown at the red lines at the bottom of the chart).\nIn this example you can see that there are 25 Hosts, 0 Containers, 100 Custom Metrics and 0 High Resolution Metrics.\nIn the bottom chart, you can see the usage per category for the current period (shown in the drop-down box on the top right of the chart).\nThe blue line marked Average Usage indicates what Observability Cloud will use to calculate your average usage for the current Subscription Usage Period.\nInfo\nAs you can see from the screenshot, Observability Cloud does not use High Watermark or P95% for cost calculation but the actual average hourly usage, allowing you to do performance testing or Blue/Green style deployments etc. without the risk of overage charges.   To get a feel for the options you can change the metric displayed by selecting the different options from the Usage Metric drop down on the left, or change the Subscription Usage Period with the drop down on the right.\nPlease take a minute to explore the different time periods \u0026 categories and their views.\nFinally, the pane on the right shows you information about your Subscription.\n","categories":"","description":"","excerpt":" How to keep track of the usage of Observability Cloud in your …","ref":"/observability-workshop/v4.22/imt/docs/servicebureau/billing-and-usage/","tags":"","title":"Subscription Usage"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/realworld/docs/event6/","tags":"","title":"Event 6"},{"body":" Introduction to Teams Create a Team and add members to Team   1. Introduction to Teams To make sure that users see the dashboards and alerts that are relevant to them when using Observability Cloud, most organizations will use Observability Cloud’s Teams feature to assign a member to one or more Teams.\nIdeally, this matches work related roles, for example, members of a Dev-Ops or Product Management group would be assigned to the corresponding Teams in Observability Cloud.\nWhen a user logs into Observability Cloud, they can choose which Team Dashboard will be their home page and they will typically select the page for their primary role.\nIn the example below, the user is a member of the Development, Operations and Product Management Teams, and is currently viewing the Dashboard for the Operations Team.\nThis Dashboard has specific Dashboard Groups for Usage, SaaS and APM Business Workflows assigned but any Dashboard Group can be linked to a Teams Dashboard.\nThey can use the menu along the top left to quickly navigate between their allocated teams, or they can use the ALL TEAMS dropdown on the right to select specific Team Dashboards, as well as quickly accessing ALL Dashboards using the adjacent link.\nAlerts can be linked to specific Teams so the Team can monitor only the Alerts they are interested in, and in the above example they currently have 1 active Critical Alert.\nThe Description for the Team Dashboard can be customized and can include links to team specific resources (using Markdown).\n 2. Creating a new Team To work with to Splunk’s Team UI click on the hamburger icon top left and select the Organizations Settings → Teams.\nWhen the Team UI is selected you will be presented with the list of current Teams.\nTo add a new Team click on the Create New Team    button. This will present you with the Create New Team dialog.\nCreate your own team by naming it [YOUR-INITIALS]-Team and add yourself by searching for your name and selecting the Add link next to your name. This should result in a dialog similar to the one below:\nYou can remove selected users by pressing Remove or the small x.\nMake sure you have your group created with your initials and with yourself added as a member, then click Done   \nThis will bring you back to the Teams list that will now show your Team and the one’s created by others.\nNote\nThe Teams(s) you are a member of have a grey Member icon in front of it.   If no members are assigned to your Team, you should see a blue Add Members link instead of the member count, clicking on that link will get you to the Edit Team dialog where you can add yourself.\nThis is the same dialog you get when pressing the 3 dots … at the end of the line with your Team and selecting Edit Team\nThe … menu gives you the option to Edit, Join, Leave or Delete a Team (leave and join will depend on if you are currently a member).\n 3. Adding Notification Rules You can set up specific Notification rules per team, click on the Notification Policy tab, this will open the notification edit menu.\nBy default the system offers you the ability to set up a general notification rule for your team.\nNote\nThe Email all team members option means all members of this Team will receive an email with the Alert information, regardless of the alert type.   3.1 Adding recipients You can add other recipients, by clicking Add Recipient   . These recipients do not need to be Observability Cloud users.\nHowever if you click on the link Configure separate notification tiers for different severity alerts you can configure every alert level independently.\nDifferent alert rules for the different alert levels can be configured, as shown in the above image.\nCritical and Major are using Splunk's On-Call Incident Management solution. For the Minor alerts we send it to the Teams Slack channel and for Warning and Info we send an email.\n3.2 Notification Integrations In addition to sending alert notifications via email, you can configure Observability Cloud to send alert notifications to the services shown below.\nTake a moment to create some notification rules for you Team.\n","categories":"","description":"","excerpt":" Introduction to Teams Create a Team and add members to Team   1. …","ref":"/observability-workshop/v4.22/imt/docs/servicebureau/teams/","tags":"","title":"Teams"},{"body":" チームの管理 チームの作成とメンバーの追加   1. チームの管理 Observability Cloudを使用する際に、ユーザーに関連するダッシュボードやアラートが表示されるようにするために、ほとんどの組織ではObservability Cloudのチーム機能を使用して、メンバーを1つまたは複数のチームに割り当てます。\nこれは、仕事に関連した役割と一致するのが理想的で、たとえば、DevOpsグループやプロダクトマネジメントグループのメンバーは、Observability Cloudの対応するチームに割り当てられます。\nユーザーがObservability Cloudにログインすると、どのチームダッシュボードをホームページにするかを選択することができ、通常は自分の主な役割に応じたページを選択します。\n以下の例では、ユーザーは開発、運用、プロダクトマネジメントの各チームのメンバーであり、現在は運用チームのダッシュボードを表示しています。\nこのダッシュボードには、NGINX、Infra、K8s用の特定のダッシュボード・グループが割り当てられていますが、どのダッシュボード・グループもチーム・ダッシュボードにリンクすることができます。\n左上のメニューを使って割り当てられたチーム間を素早く移動したり、右側の ALL TEAMS ドロップダウンを使って特定のチームのダッシュボードを選択したり、隣のリンクを使って ALL Dashboards に素早くアクセスしたりすることができます。\nアラートを特定のチームにリンクすることで、チームは関心のあるアラートだけをモニターすることができます。上記の例では、現在1つのアクティブなクリティカルアラートがあります。\nチームダッシュボードの説明文はカスタマイズ可能で、チーム固有のリソースへのリンクを含むことができます（Markdownを使用します）。\n 2. 新しいチームの作成 Splunk のチーム UI を使用するには、左下の » を開き、 Settings → Teams を選択します。\nTeam を選択すると、現在のチームのリストが表示されます。\n新しいチームを追加するには、 Create New Team    ボタンをクリックします。これにより、Create New Team ダイアログが表示されます。\n独自のチームを作ってみましょう。チーム名を [あなたのイニシャル]-Team のように入力し、あなた自身のユーザー選んで、Add リンクからチームに追加してみましょう。上手くいくと、次のような表示になるはずです。\n選択したユーザーを削除するには、Remove または x を押します。\n自分のイニシャルでグループを作成し、自分がメンバーとして追加されていることを確認して、 Done    をクリックします。\nこれでチームリストに戻り、自分のチームと他の人が作成したチームが表示されます。\nNote\n自分がメンバーになっているチームには、グレーの Member アイコンが前に表示されています。   自分のチームにメンバーが割り当てられていない場合は、メンバー数の代わりに青い Add Members のリンクが表示されます。このリンクをクリックすると、Edit Team ダイアログが表示され、自分を追加することができます。\n自分のチームの行末にある3つのドット … を押しても、Edit Team と同じダイアログが表示されます。\n… メニューでは、チームの編集、参加、離脱、削除を行うことができます（離脱と参加は、あなたが現在メンバーであるかどうかによって異なります）。\n 3. 通知ルールの追加 チームごとに特定の通知ルールを設定することができます。Notification Policy タブをクリックすると、通知編集メニューが表示されます。\nデフォルトでは、システムはチームの一般的な通知ルールを設定する機能を提供します。\nNote\nEmail all team members オプションは、アラートの種類に関わらず、このチームのすべてのメンバーにアラート情報のメールが送信されることを意味します。   3.1 受信者の追加  Add Recipient    をクリックすると、他の受信者を追加することができます。これらの受信者は Observability Cloud のユーザーである必要はありません。\nConfigure separate notification tiers for different severity alerts をクリックすると、各アラートレベルを個別に設定できます。\n上の画像のように、異なるアラートレベルに対して異なるアラートルールを設定することができます。\nCritical と Major は Splunk On-Call インシデント管理ソリューションを使用しています。Minor のアラートはチームの Slack チャンネルに送信し、Warning と Info はメールで送信する、という管理もできるようになります。\n3.2 通知機能の統合 Observability Cloud では、アラート通知をメールで送信するだけでなく、以下のようなサービスにアラート通知を送信するように設定することができます。\nチームの事情に合わせて、通知ルールを作成してください。\n","categories":"","description":"","excerpt":" チームの管理 チームの作成とメンバーの追加   1. チームの管理 Observability Cloudを使用する際に、ユーザーに関連す …","ref":"/observability-workshop/v4.22/ja/imt/docs/servicebureau/teams/","tags":"","title":"チーム"},{"body":" Discover how you can restrict usage by creating separate Access Tokens and set limits.  1. Access Tokens If you wish to control the consumption of Hosts, Containers, Custom Metrics and High Resolution Metrics, you can create multiple Access Tokens and allocate them to different parts of your organization.\nIn the UI click on the » bottom left and select the Settings → Access Tokens under General Settings.\nThe Access Tokens Interface provides an overview of your allotments in the form of a list of Access Tokens that have been generated. Every Organization will have a Default token generated when they are first setup, but there will typically be multiple Tokens configured.\nEach Token is unique and can be assigned limits for the amount of Hosts, Containers, Custom Metrics and High Resolution Metrics it can consume.\nThe Usage Status Column quickly shows if a token is above or below its assigned limits.\n2. Creating a new token Let create a new token by clicking on the New Token    button. This will provide you with the Name Your Access Token dialog.\nEnter the new name of the new Token by using your Initials e.g. RWC-Token and make sure to tick both Ingest Token and API Token checkboxes!\nAfter you press OK    you will be taken back to the Access Token UI. Here your new token should be present, among the ones created by others.\nIf you have made an error in your naming, want to disable/enable a token or set a Token limit, click on the ellipsis (…) menu button behind a token limit to open the manage token menu.\nIf you made a typo you can use the Rename Token option to correct the name of your token.\n3. Disabling a token If you need to make sure a token cannot be used to send Metrics in you can disable a token.\nClick on Disable to disable the token, this means the token cannot be used for sending in data to Splunk Observability Cloud.\nThe line with your token should have become greyed out to indicate that is has been disabled as you can see in the screenshot below.\nGo ahead and click on the ellipsis (…) menu button to Disable and Enable your token.\n4. Manage token usage limits Now Lets start limiting usage by clicking on Manage Token Limit in the 3 … menu.\nThis will show the Manage Token Limit Dialog:\nIn this Dialog you can set the limits per category.\nPlease go ahead and specify the limits as follows for each usage metric:\n   Limit Value     Host Limit 5   Container Limit 15   Custom Metric Limit 20   High Resolution Metric Limit 0    For our lab use your own email address, and double check that you have the correct numbers in your dialog box as shown in the table above.\nToken limits are used to trigger an alert that notify one or more recipients when the usage has been above 90% of the limit for 5 minutes.\nTo specify the recipients, click Add Recipient   , then select the recipient or notification method you want to use (specifying recipients is optional but highly recommended).\nThe severity for token alerts is always Critical.\nClick on Update    to save your Access Tokens limits and The Alert Settings.\nNote: Going above token limit\nWhen a token is at or above its limit in a usage category, new metrics for that usage category will not be stored and processed by Observability Cloud. This will make sure you there will be no unexpected cost due to a team sending in data without restriction.   Note: Advanced alerting\nIf you wish to get alerts before you hit 90%, you can create additional detectors using whatever values you want. These detectors could target the Teams consuming the specific Access Tokens so they can take action before the admins need to get involved.   In your company you would distribute these new Access Tokens to various teams, controlling how much information/data they can send to Observability Cloud.\nThis will allow you to fine tune the way you consume your Observability Cloud allotment and prevent overages from happening.\nCongratulations! You have now have completed the Service Bureau module.\n","categories":"","description":"","excerpt":" Discover how you can restrict usage by creating separate Access …","ref":"/observability-workshop/v4.22/imt/docs/servicebureau/tokens/","tags":"","title":"Controlling Usage"},{"body":" 個別のアクセストークンを作成し、制限を設けることで利用を制限する方法を紹介します  1. アクセストークン ホスト、コンテナ、カスタムメトリクス、高解像度メトリクスの使用量をコントロールしたい場合は、複数のアクセストークンを作成し、組織内の異なる部分に割り当てることができます。\n左下の » を開いて、Settings → Access Tokens を選択します。\nAccess Tokens インターフェースでは、生成されたアクセストークンのリストで概要が表示されます。すべての組織では、最初のセットアップ時に Default のトークンが生成され、その他のトークンを追加・削除できるようになっています。\n各トークンは一意であり、消費できるホスト、コンテナ、カスタムメトリクス、高解像度メトリクスの量に制限を設けることができます。\nUsage Status 欄は、トークンが割り当てられた制限値を上回っているか下回っているかを素早く表示します。\n2. 新しいトークンの作成  New Token    ボタンをクリックして、新しいトークンを作成しましょう。Name Your Access Token ダイアログが表示されます。\nここでは、Ingest Token と API Token の両方のチェックボックスにチェックを入れてください。\n OK    を押すと、Access Token のUIに戻ります。ここでは、既存のトークンの中に、あなたの新しいトークンが表示されているはずです。\n名前を間違えたり、トークンを無効/有効にしたり、トークンの制限を設定したい場合は、トークンの制限の後ろにある省略記号(…)のメニューボタンをクリックして、トークンの管理メニューを開きます。\nタイプミスがあった場合は、Rename Token オプションを使用して、トークンの名前を修正することができます。\n3. トークンの無効化 トークンがメトリクスの送信に使用できないようにする必要がある場合は、トークンを無効にすることができます。\nDisable ボタンを押すことで、トークンを無効化できます。これにより、トークンがSplunk Observability Cloudへのデータ送信に使用できなくなります。\n以下のスクリーンショットのように、トークンの行がグレーになり、無効化されたことを示しています。\n省略記号(…)のメニューボタンをクリックして、トークンの無効化と有効化を行ってください。\n4. トークンの使用制限の管理 … メニューの Manage Token Limit をクリックして、使用量を制限してみましょう。\nトークン制限管理ダイアログが表示されます。\nこのダイアログでは、カテゴリごとに制限を設定することができます。\n先に進み、各使用指標に対して以下のように制限を指定してください。\n   リミット 値     ホストの制限 5   コンテナ制限 15   カスタムメトリクスの制限 20   高解像度メトリックの制限 0    また、上の表に示すように、ダイアログボックスに正しい数字が表示されていることを再確認してください。\nトークンリミットは、5分間の使用量がリミットの90％を超えたときに、1人または複数の受信者に通知するアラートのトリガーとして使用されます。\n受信者を指定するには、 Add Recipient    をクリックして、使用する受信者または通知方法を選択します（受信者の指定は任意ですが、強くお勧めします）。\nトークンアラートの重要度は常に「Critical」です。\n Update    をクリックすると、アクセストークンの制限とアラートの設定が保存されます。\nNote: トークンの上限を超えると、何が起こるのか\nトークンが使用カテゴリの上限に達したとき、または上限を超えたとき、その使用カテゴリの新しいメトリクスはObservability Cloudに保存されず、処理されません。これにより、チームが無制限にデータを送信することによる予期せぬコストが発生しないようになります。   Note: 高度なアラート通知\n90%に達する前にアラートを取得したい場合は、必要な値を使用して追加のディテクターを作成できます。これらのディテクターは、特定のアクセストークンを消費しているチームをターゲットにすることができ、管理者が関与する必要がある前に行動を起こすことができます。   これらの新しいアクセストークンを様々なチームに配布し、Observability Cloudに送信できる情報やデータの量をコントロールできるようになります。\nこれにより、Observability Cloudの使用量を調整することができ、過剰な使用を防ぐことができます。\nおめでとうございます！ これで、管理機能のモジュールは終わりです！\n","categories":"","description":"","excerpt":" 個別のアクセストークンを作成し、制限を設けることで利用を制限する方法を紹介します  1. アクセストークン ホスト、コンテナ、カスタムメト …","ref":"/observability-workshop/v4.22/ja/imt/docs/servicebureau/tokens/","tags":"","title":"使用量を管理する"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/otelw/appendix/","tags":"","title":"Appendix"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/realworld/docs/summary/","tags":"","title":"Summary"},{"body":"OTEL COLLECTOR and SMART AGENT COMMANDS   The snippets assume a Splunk Observability Cloud Access Token is set as an environment variable named ACCESS_TOKEN.\n  To run checks in a Kubernetes cluster, we recommend you use a “swiss-army knife” container like netshoot:\nkubectl run tshoot -i --rm --restart=Never --image nicolaka/netshoot -- ACTUAL COMMAND HERE e.g. for a Trace connectivity check:\nkubectl run tshoot -i --rm --restart=Never --image nicolaka/netshoot -- curl -v -d'[]' -H'Content-Type:application/json' https://ingest.us1.signalfx.com/v2/trace If you already have Splunk OpenTelemetry collector installed, you can use its container and the bundled curl:\nkubectl exec -it $(kubectl get po -A -l 'app=splunk-otel-collector,!component' -o name) --container otel-collector -- /usr/lib/splunk-otel-collector/agent-bundle/bin/curl -v -d'[]' -H'Content-Type:application/json' https://ingest.us1.signalfx.com/v2/trace   CONNECTIVITY CHECK curl https://ingest.us1.signalfx.com/healthz INGEST CHECK Metrics curl -qs -H\"X-SF-Token:$ACCESS_TOKEN\" https://ingest.us1.signalfx.com/v2/datapoint -X POST -v -d '{}' -H \"Content-Type: application/json\" Traces curl -H \"Content-Type: application/json\" -H \"X-SF-Token: $ACCESS_TOKEN\" -d '[]' -i https://ingest.eu0.signalfx.com/v2/trace SMART AGENT CHECK AGENT STATUS sudo signalfx-agent status service signalfx-agent status systemctl signalfx-agent status SMART AGENT CHECK AGENT LOGS journalctl -u signalfx-agent | tail -f tail -f /var/log/signalfx-agent.log SPLK-OTEL-COLL CHECK AGENT LOGS journalctl -u splunk-otel-collector -f tail -100 /var/log/messages SMART AGENT START/STOP/RESTART sudo systemctl restart signalfx-agent sudo systemctl start signalfx-agent sudo systemctl stop signalfx-agent SPLK-OTEL-COLL START/STOP/RESTART sudo systemctl restart splunk-otel-collector sudo systemctl start splunk-otel-collector sudo systemctl stop splunk-otel-collector SPLK-OTEL-COLL FLUENTD START/STOP/RESTART sudo systemctl restart td-agent sudo systemctl start td-agent sudo systemctl stop td-agent SMART AGENT DEFAULT CONFIG /etc/signalfx/agent.yaml SPLK-OTEL-COLL DEFAULT CONFIG /etc/otel/collector/agent_config.yaml SPLK-OTEL-COLL ENVIRONMENT FILE WITH REQUIRED VARIABLES/VALUES FOR SERVICE /etc/otel/collector/splunk-otel-collector.conf SPLK-OTEL-COLL FLUENTD CONFIG /etc/otel/collector/fluentd/fluent.conf SPLK-OTEL-COLL FLUENTD CONFIG DIRECTORY FOR ADDING FILES WITH .CONF EXT /etc/otel/collector/fluentd/conf.d SMART AGENT TAIL METRIC DATAPOINTS BEING SENT signalfx-agent tap-dps -h signalfx-agent tap-dps -metric 'jenkins_*’ SMART AGENT ENDPOINTS SET signalfx-agent status endpoints SELINUX SETTING chcon -t bin_t /usr/lib/signalfx-agent/bin/signalfx-agent SMART AGENT STANDARD PORTS ARE 9080 and 8095\nKUBERNETES SMART AGENT CHECK AGENT STATUS kubectl get pods kubectl exec \u003csignalfx-agent-PODNAME\u003e -- signalfx-agent status OTEL AGENT CHECK AGENT STATUS kubectl exec -it YOURAGENTPODHERE -- curl localhost:55679/debug/tracez | lynx -stdin kubectl exec -it splunk-otel-collector-agent-f4gwg -- curl localhost:55679/debug/tracez | lynx -stdin OTEL INITIAL CONFIG kubectl exec -it my-splunk-otel-collector-agent-hg4gk -- curl http://localhost:55554/debug/configz/initial OTEL effective CONFIG kubectl exec -it my-splunk-otel-collector-agent-hg4gk -- curl http://localhost:55554/debug/effective SMART AGENT CHECK AGENT LOGS kubectl logs -l app=signalfx-agent -f SPLK-OTEL-COLL CHECK AGENT LOGS sudo kubectl logs -l app=splunk-otel-collector -f sudo kubectl logs -l app=splunk-otel-collector -f -c otel-collector MODIFY EITHER AGENT CONFIGMAP kubectl get configmap kubectl edit cm splunk-otel-collector-otel-agent sudo kubectl create configmap \u003cnginxconfig\u003e --from-file=workshop/k3s/nginx/nginx.conf SMART AGENT CONFIGMAP REFERENCE https://github.com/signalfx/signalfx-agent/blob/main/deployments/k8s/configmap.yaml\nMODIFY EITHER AGENT DAEMONSET kubectl get ds kubectl edit ds splunk-otel-collector-agent SMART AGENT DAEMONSET REFERENCE https://github.com/signalfx/signalfx-agent/blob/main/deployments/k8s/daemonset.yaml\nSMART AGENT HELM helm repo add signalfx https://dl.signalfx.com/helm-repo \u0026\u0026 helm repo update helm delete signalfx-agent helm install \\ --set signalFxAccessToken=$ACCESS_TOKEN \\ --set clusterName=\u003cMY-CLUSTER\u003e \\ --set kubeletAPI.url=https://localhost:10250 \\ --set signalFxRealm=$REALM \\ --set traceEndpointUrl=https://ingest.$REALM.signalfx.com/v2/trace \\ --set gatherDockerMetrics=false \\ signalfx-agent signalfx/signalfx-agent \\ -f ~/workshop/k3s/values.yaml SPLK-OTEL-COLL HELM helm repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart \u0026\u0026 helm repo update helm delete splunk-otel-collector helm uninstall splunk-otel-collector helm install splunk-otel-collector \\ --set=\"splunkRealm=$REALM\" \\ --set=\"splunkAccessToken=$ACCESS_TOKEN\" \\ --set=\"clusterName=\u003cMY-CLUSTER\u003e\" \\ --set=\"logsEnabled=false\" \\ --set=\"environment=$\u003cMY-ENV\u003e\" \\ splunk-otel-collector-chart/splunk-otel-collector \\ -f ~/workshop/k3s/otel-collector.yaml kubectl get pods kubectl get pods -n kube-system kubectl get svc kubectl describe pod signalfx-agent-86zg4 kubectl delete pod,service baz foo CREATE/DELETE DEPLOYMENT FROM FILE sudo kubectl create -f nginx-deployment.yaml sudo kubectl delete -f nginx-deployment.yaml CHECK SYSTEM CONFIGS kubectl describe -n kube-system pod \u003cmetrics-server-6d684c7b5-gm778\u003e SHOW KUBECONFIG SETTINGS kubectl config view SAVE NAMESPACE FOR ALL SUBSEQUENT KUBECTL COMMANDS IN CONTEXT kubectl config set-context --current --namespace=ggckad-s2 USE MULTIPLE KUBECONFIG FILES KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 get the password for the e2e user kubectl config view -o jsonpath='{.users[?(@.name == \"e2e\")].user.password}' kubectl config view -o jsonpath='{.users[].name}' # display the first user kubectl config view -o jsonpath='{.users[*].name}' # get a list of users kubectl config get-contexts # display list of contexts kubectl config current-context # display the current-context kubectl config use-context my-cluster-name # set the default context to my-cluster-name add a new user to your kubeconf that supports basic auth kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword set a context utilizing a specific username and namespace kubectl config set-context gce --user=cluster-admin --namespace=foo \\  \u0026\u0026 kubectl config use-context gce Return snapshot logs from pod nginx with only one container kubectl logs nginx Return snapshot logs from pod nginx with multi containers kubectl logs nginx --all-containers=true Return snapshot logs from all containers in pods defined by label app=nginx kubectl logs -l app=nginx --all-containers=true Return snapshot of previous terminated ruby container logs from pod web-1 kubectl logs -p -c ruby web-1 Begin streaming the logs of the ruby container in pod web-1 kubectl logs -f -c ruby web-1 Begin streaming the logs from all containers in pods defined by label app=nginx kubectl logs -f -lapp=nginx --all-containers=true Display only the most recent 20 lines of output in pod nginx kubectl logs --tail=20 nginx Show all logs from pod nginx written in the last hour kubectl logs --since=1h nginx Show logs from a kubelet with an expired serving certificate kubectl logs --insecure-skip-tls-verify-backend nginx Return snapshot logs from first container of a job named hello kubectl logs job/hello Return snapshot logs from container nginx-1 of a deployment named nginx kubectl logs deployment/nginx -c nginx-1 # LOGS ## log ingest test ```bash curl -v -X POST -H \"Content-Type: application/json\" -H \"Authorization: Splunk ${ACCESS_TOKEN}\" https://ingest.us1.signalfx.com/v1/log -d '{\"event\": \"hello world\", \"fields\": {\"foo\": \"bar\"}}' curl -H \"Authorization: Splunk ${ACCESS_TOKEN}\" -H \"Content-Type: application/json\" https://ingest.eu0.signalfx.com/v1/log -d '{\"sourcetype\": \"iracing\", \"event\": \"Cory into the Pits, lap 12\"}' ","categories":"","description":"","excerpt":"OTEL COLLECTOR and SMART AGENT COMMANDS   The snippets assume a Splunk …","ref":"/observability-workshop/v4.22/bootcamp/cheatsheet/","tags":"","title":""},{"body":"APM with AWS Lambda (Developer Focused) !!! important “Enabling APM”\n**If you recently signed up for the 14 day free trial then this section of the workshop cannot be completed!** An Organization needs to be pre-provisioned as a APM entitlement is required for the purposes of this module. Please contact someone from Splunk's Observability team to get a trial instance with APM enabled if you don’t have one already. To check if you have an Organization with APM enabled, just login to Splunk's observability suite and check that you have the APM tab on the top navbar next to Dashboards.   1. AWS Lambda exercise \u0026 APM overview This workshop section is focused on developer’s of serverless/Lambda application/functions. This workshop is going to guide them through the steps to add Tracing to Python and Node-Js Lambda Functions, and see traces flow from an on-prem Java application though the various Python and Node-Js Lambda Functions in Splunk APM.\nSplunk APM captures end-to-end distributed transactions from your applications, including serverless apps (Lambda’s) with trace spans sent directly to Splunk or via an optional OpenTelemetry Collector that act as a central aggregation point prior to sending trace spans to Splunk. (recommended, and show in the workshop).\nIn addition to proxying spans and infrastructure metrics, the OpenTelemetry Collector can also perform other functions, such as redacting sensitive tags prior to spans leaving your environment.\nThe following illustration shows the recommended deployment model: Splunk’s auto-instrumentation libraries send spans to the OpenTelemetry Collector; the OpenTelemetry Collector forwards the spans to Observability Cloud.\n![Lambda-Architecture](/images/lambda/Lambda Architecture.png)\n 2. AWS Lambda exercise requirements flow During this workshop you will perform the following activities:\n Perform a test run of the Splunk Mobile Phone Web shop and its services Enable Tracing on local SpringBoot App Enable Tracing on First Python \u0026 Node-js Lambda Functions Enable Tracing on the other functions Enrich the spans Look at configuring options for the OpenTelemetry Collector   3. AWS Lambda exercise requirements This workshop section assumes that you have access to the following features as they are required for the workshop:\n AWS account: Access to EC2 Instances Ability to create/run AWS Lambda’s  ","categories":"","description":"","excerpt":"APM with AWS Lambda (Developer Focused) !!! important “Enabling APM” …","ref":"/observability-workshop/v4.22/lambda/","tags":"","title":""},{"body":"Initial Setup Aim This module is simply to ensure you have access to the Splunk On-Call UI (formerly known as VictorOps), Splunk Infrastructure Monitoring UI (formerly known as SignalFx) and the EC2 Instance which has been allocated to you.\nOnce you have access to each platform, keep them open for the duration of the workshop as you will be switching between them and the workshop instructions.\n1. Activate your Splunk On-Call Login You should have received an invitation to Activate your Splunk On-Call account via e-mail, if you have not already done so, click the Activate Account link and follow the prompts.\nIf you did not receive an invitation it is probably because you already have a Splunk On-Call login, linked to a different organisation.\nIf so login to that Org, then use the organisation dropdown next to your username in the top left to switch to the Observability Workshop Org.\n{: .center}\n!!! Note If you do not see the Organisation dropdown menu item next to your name with Observability Workshop EMEA that is OK, it simply means you only have access to a single Org so that menu is not visible to you.\nIf you have forgotten your password go to the [sign-in](https://portal.victorops.com/membership/#/) page and use the forgotten password link to reset your password. ![Reset Pwd](..//images/oncall/reset-password.png){: .center}   2. Activate your Splunk Infrastructure Monitoring Login You should have received an invitation to join the Splunk Infrastructure Monitoring - Observability Workshop. If you have not already done so click the JOIN NOW{: .label-button .sfx-ui-button-black} button and follow the prompts to set a password and activate your login.\n3. Access your EC2 Instance Splunk has provided you with a dedicated EC2 Instance which you can use during this workshop for triggering Incidents the same way the instructor did during the introductory demo. This VM has Splunk Infrastructure Monitoring deployed and has an associated Detector configured. The Detector will pass Alerts to Splunk On-Call which will then create Incidents and page the on-call user.\nThe welcome e-mail you received providing you all the details for this Workshop contain the instructions for accessing your allocated EC2 Instance.\nSSH (Mac OS/Linux) Most attendees will be able to connect to the workshop by using SSH from their Mac or Linux device.\nTo use SSH, open a terminal on your system and type ssh ubuntu@x.x.x.x (replacing x.x.x.x with the IP address found in your welcome e-mail).\nWhen prompted Are you sure you want to continue connecting (yes/no/[fingerprint])? please type yes.\nEnter the password provided in the welcome e-mail.\nUpon successful login you will be presented with the Splunk logo and the Linux prompt.\nAt this point you are ready to continue with the workshop when instructed to do so by the instructor\n Putty (Windows users only) If you do not have ssh preinstalled or if you are on a Windows system, the best option is to install putty, you can find the downloads here.\n!!! important If you cannot install Putty, please go to Web Browser (All).\nOpen Putty and in the Host Name (or IP address) field enter the IP address provided in the welcome e-mail.\nYou can optionally save your settings by providing a name and pressing Save.\nTo then login to your instance click on the Open button as shown above.\nIf this is the first time connecting to your EC2 instance, you will be presented with a security dialog, please click Yes.\nOnce connected, login in as ubuntu using the password provided in the welcome e-mail.\nOnce you are connected successfully you should see a screen similar to the one below:\nAt this point you are ready to continue with the workshop when instructed to do so by the instructor\n Web Browser (All) If you are blocked from using SSH (Port 22) or unable to install Putty you may be able to connect to the workshop instance by using a web browser.\n!!! note This assumes that access to port 6501 is not restricted by your company’s firewall.\nOpen your web browser and type http://X.X.X.X:6501 (where X.X.X.X is the IP address from the welcome e-mail).\nOnce connected, login in as ubuntu and the password is the one provided in the welcome e-mail.\nOnce you are connected successfully you should see a screen similar to the one below:\n Copy \u0026 Paste in browser Unlike when you are using regular SSH, copy and paste does require a few extra steps to complete when using a browser session. This is due to cross browser restrictions.\nWhen the workshop asks you to copy instructions into your terminal, please do the following:\nCopy the instruction as normal, but when ready to paste it in the web terminal, choose Paste from browser as show below:\nThis will open a dialog box asking for the text to be pasted into the web terminal:\nPaste the text in the text box as show, then press OK to complete the copy and paste process.\n!!! note Unlike regular SSH connection, the web browser has a 60 second time out, and you will be disconnected, and a Connect button will be shown in the center of the web terminal.\nSimply click the **Connect** button and you will be reconnected and will be able to continue.  At this point you are ready to continue with the workshop when instructed to do so by the instructor\n","categories":"","description":"","excerpt":"Initial Setup Aim This module is simply to ensure you have access to …","ref":"/observability-workshop/v4.22/oncall/getting_started/","tags":"","title":""},{"body":"UI Overview Aim The aim of this module is for you to get more familiar with the Timeline Tab and the filtering features.\n 1. Timeline The aim of Splunk On-Call is to “Make On Call Suck Less”, and it does this by getting the critical data, to the right people, at the right time.\nThe key to making it work for you is to centralize all your alerting sources, sending them all to the Splunk On-Call platform, then you have a single pane of glass in which to manage all of your alerting.\nLogin to the Splunk On-Call UI and select the Timeline tab on the main menu bar, you should have a screen similar to the following image:\n2. People On the left we have the People section with the Teams and Users sub tabs.\nOn the Teams tab, click on All Teams then expand [Your Teamname].\nUsers with the Splunk On-Call Logo against their name are currently on call.\nHere you can see who is on call within a particular Team, or across all Teams via Users → On-Call.\nIf you click into one of the currently on call users, you can see their status.\nIt shows which Rotation they are on call for, when their current Shift ends and their next Shift starts (times are displayed in your timezone), what contact methods they have and which Teams they belong to (dummy users such as Hank do not have Contact Methods configured).\n{: .center}\n3. Timeline In the centre Timeline section you get a realtime view of what is happening within your environment with the newest messages at the top.\nHere you can quickly post update messages to make your colleagues aware of important developments etc.\nYou can filter the view using the buttons on the top toolbar showing only update messages, GitHub integrations, or apply more advanced filters.\nLets change the Filters settings to streamline your view. Click the Filters button then within the Routing Keys tab change the Show setting from all routing keys to selected routing keys.\nChange the My Keys value to all and the Other Keys value to selected and deselect all keys under the Other Keys section.\nClick anywhere outside of the dialogue box to close it.\nYou will probably now have a much simpler view as you will not currently have Incidents created using your Routing Keys, so you are left with the other types of messages that the Timeline can display.\nClick on Filters again, but this time switch to the Message Types tab.\nHere you control the types of messages that are displayed.\nFor example, deselect On-call Changes and Escalations, this will reduce the amount of messages displayed.\n4. Incidents On the right we have the Incidents section.\nHere we get a list of all the incidents within the platform, or we can view a more specific list such as incidents you are specifically assigned to, or for any of the Teams you are a member of.\nSelect the Team Incidents tab you should find that the Triggered, Acknowledged \u0026 Resolved tabs are currently all empty as you have had no incidents logged.\nLet’s change that by generating your first incident!\n Continue with the Create Incidents module.\n","categories":"","description":"","excerpt":"UI Overview Aim The aim of this module is for you to get more familiar …","ref":"/observability-workshop/v4.22/oncall/incident_lifecycle/","tags":"","title":""},{"body":"Create a SignalFx Detector Aim We need to create a new Detector within SignalFx which will use VictorOps as the target to send alerts to.\nWe will use Terraform installed within the VM to create the Detector, but first we need to obtain some values required for Terraform to run.\n 1. Preparation The presenter will typically share these values with you at the start of the module to save time, but the following instructions explain how to get them for yourself.\n1.1 Create a variables document We suggest you create a variables document using your preferred text editor as you will be gathering three different values in the next few steps which you need to use in the last step of this module.\nAdd the following lines to your variables document, then as you gather the values you can add them to the appropriate lines:\n=== “variables.txt”\n``` text export ACCESS_TOKEN= export REALM= export SFXVOPSID= ```  1.2 Obtain SignalFx Access Token In the Splunk UI you can find your Access Token by clicking on the Settings icon on the top right of the Splunk UI, select Organization Settings → Access Tokens, expand the Default token, then click on Show Token to expose your token.\nClick the Copy{: .label-button .sfx-ui-button-blue} button to copy it you your clipboard, then paste it into the ACCESS_TOKEN line of your variables document.\n=== “variables.txt”\n```text export ACCESS_TOKEN={==xxxx==} export REALM= export SFXVOPSID= ```  1.3 Obtain SignalFx Realm Still in the Splunk UI, click on the Settings icon again, but this time select My Profile.\nThe Realm can be found in the middle of the page within the Organizations section. In this example it is us1, but yours may be eu0 or one of the many other SignalFx Realms.\nCopy it to the REALM line of your variables document.\n=== “variables.txt”\n```text export ACCESS_TOKEN={==xxxx==} export REALM={==xxxx==} export SFXVOPSID= ```  1.4. Obtain VictorOps Integration ID In Splunk UI navigate to Integrations and use the search feature to find the VictorOps Integration.\nExpand the VictorOps-xxxx configuration; if there are more than one you will be informed which one to copy by the presenter.\nCopy it to the SFXVOPSID line of your variables document.\n=== “variables.txt”\n```text export ACCESS_TOKEN={==xxxx==} export REALM={==xxxx==} export SFXVOPSID={==xxxx==} ```   2. Create environment variables 2.1 Copy variables to VM With all the required values now safely copied into your variables document you can use them to compile the commands which we will run in your VM in the next step.\n=== “Example”\n```text export SFXVOPSID=EYierbGA4AA export ACCESS_TOKEN=by78voyt7b..... export REALM=us1 ```  Switch back to your shell session connected to the VM you created in the Getting Started/Create a Test Environment module, all of the following commands will be executed within this instance:\nPast the three commands from your variables document into the shell session of your VM.\n3. Initialize and apply Terraform Still within your VM, switch to the victorops folder where the Terraform config files are located (you should still be logged in as Ubuntu and should not have elevated to root)\n=== “Change Directory”\n```text cd ~/workshop/victorops ```  Now we can initialize Terraform:\n=== “Shell Command”\n```text terraform init -upgrade ```  === “Example Output”\n```text Initializing the backend... Initializing provider plugins... - Checking for available provider plugins... - Downloading plugin for provider \"signalfx\" (terraform-providers/signalfx) 4.21.0... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.signalfx: version = \"~\u003e 4.21\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. ```  You can now copy and the paste the following code block to run Terraform using the Variables you created in the VM. Check the plan output for errors before typing yes to commit the apply.\n=== “Shell Command”\n```text terraform apply \\ -var=\"access_token=$ACCESS_TOKEN\" \\ -var=\"realm=$REALM\" \\ -var=\"sfx_prefix=${HOSTNAME}\" \\ -var=\"sfx_vo_id=$SFXVOPSID\" \\ -var=\"routing_key=${HOSTNAME}_PRI\" ```  === “Example Output”\n``` An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # signalfx_detector.cpu_greater_90 will be created + resource \"signalfx_detector\" \"cpu_greater_90\" { + description = \"Alerts when CPU usage is greater than 90%\" + id = (known after apply) + max_delay = 0 + name = \"vmpe CPU greater than 90%\" + program_text = \u003c\u003c~EOT from signalfx.detectors.against_recent import against_recent A = data('cpu.utilization', filter=filter('host', 'vmpe*')).publish(label='A') detect(when(A \u003e threshold(90))).publish('CPU utilization is greater than 90%') EOT + show_data_markers = true + time_range = 3600 + url = (known after apply) + rule { + detect_label = \"CPU utilization is greater than 90%\" + disabled = false + notifications = [ + \"VictorOps,xxx,vmpe_pri\", ] + parameterized_body = \u003c\u003c~EOT {{#if anomalous}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" triggered at {{timestamp}}. {{else}} Rule \"{{{ruleName}}}\" in detector \"{{{detectorName}}}\" cleared at {{timestamp}}. {{/if}} {{#if anomalous}} Triggering condition: {{{readableRule}}} {{/if}} {{#if anomalous}} Signal value: {{inputs.A.value}} {{else}} Current signal value: {{inputs.A.value}} {{/if}} {{#notEmpty dimensions}} Signal details: {{{dimensions}}} {{/notEmpty}} {{#if anomalous}} {{#if runbookUrl}} Runbook: {{{runbookUrl}}} {{/if}} {{#if tip}} Tip: {{{tip}}} {{/if}} {{/if}} EOT + parameterized_subject = \"{{ruleSeverity}} Alert: {{{ruleName}}} ({{{detectorName}}})\" + severity = \"Critical\" } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions in workspace \"Workshop\"? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes signalfx_detector.cpu_greater_90: Creating... signalfx_detector.cpu_greater_90: Creation complete after 2s [id=EWHU-YAAAAA] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. ```   4. Summary By running Terraform within the VM you have just created a new Detector within SignalFx which will send alerts to VictorOps if the CPU utilization of your specific VM goes above 90%.\nIn the Splunk UI go to Alerts → Detectors to show all the Detectors and find the one matching your INSTANCE value (the first four letters of the name of your VM).\nOptionally - Click on CPU Utilization is greater than 90% to open the Alert Rule Editor to view its settings.\nA filter has been used to specifically monitor your VM using the 1st 4 characters of its name, which were randomly assigned when you created the VM.\nA Recipient has been configured using the VictorOps Integration and your Routing Key has been specified. This is how a monitoring system such as SignalFx knows to route Alerts into VictorOps, and ensure they get routed to the correct team.\n You have now configured the Integrations between VictorOps and SignalFx!\nThe final part of this module is to test the flow of alerts from SignalFx into VictorOps and see how you can manage the incident with both the VictorOps UI and Mobile App.\n","categories":"","description":"","excerpt":"Create a SignalFx Detector Aim We need to create a new Detector within …","ref":"/observability-workshop/v4.22/oncall/optional/detector/","tags":"","title":""},{"body":"Creating a Test VM Using Multipass Aim The aim of this module is to guide you through the process of creating a VM locally using Multipass.\nOnce the configuration of VictorOps is complete you will use this VM to trigger an Alert from SignalFx which in turn will create an Incident within VictorOps, resulting in you getting paged.\n 1. Install Multipass If you do not already have Multipass installed you can download the installer from here.\nUsers running macOS can install it using Homebrew by running:\n=== “Shell Command”\n```text brew cask install multipass ```   2. Create VM using Multipass 2.1 Cloud-init The first step is to pull down the cloud-init file to launch a pre-configured VM.\n=== “Shell Command”\n```text WSVERSION=2.42 curl -s \\ https://raw.githubusercontent.com/signalfx/observability-workshop/v$WSVERSION/workshop/cloud-init/victorops.yaml \\ -o victorops.yaml ```  2.2 Launch VM Remaining in the same directory where you downloaded victorops.yaml, run the following commands to create your VM.\nThe first command will generate a random unique 4 character string. This will prevent clashes in the Splunk UI.\n=== “Shell Command”\n```text export INSTANCE=$(cat /dev/urandom | base64 | tr -dc 'a-z' | head -c4) multipass launch \\ --name ${INSTANCE} \\ --cloud-init victorops.yaml ```  === “Example Output”\n```text Launched: zevn ```  Make a note of your VMs Hostname as you will need it in later steps.\n2.3 Connect to VM Once the VM has deployed successfully, in a new shell session connect to the VM using the following command.\n=== “Shell Command”\n```text multipass shell ${INSTANCE} ```  === “Example Input”\n```text multipass shell zevn ```  === “Example Output”\n```text Last login: Tue Jun 9 15:10:19 2020 from 192.168.64.1 ubuntu@zevn:~$ ```   3. Install SignalFx Agent An easy way to install the SignalFx Agent into your VM is to copy the install commands from the Splunk UI, then run them directly within your VM.\n3.1 Splunk UI Navigate to the Integrations tab within the Splunk UI, where you will find the SignalFx SmartAgent tile on the top row.\nClick on the SmartAgent tile to open it…\n…then select the Setup tab…\n…then scroll down to ‘Step 1’ where you will find the commands for installing the agent for both Linux and Windows. You need to copy the commands for Linux, so click the top copy{: .label-button .sfx-ui-button-blue} button to place these commands on your clipboard ready for the next step.\n3.2 Install Agent Now paste the linux install commands into your VM Shell, the SignalFx Agent will install and after approx 1 min you should have the following result.\n=== “Example Output”\n```text The SignalFx Agent has been successfully installed. Make sure that your system's time is relatively accurate or else datapoints may not be accepted. The agent's main configuration file is located at /etc/signalfx/agent.yaml. ```   4. Check SignalFx Agent 4.1 Agent Status Once the agent has completed installing run the following command to check the status\n=== “Shell Command”\n```text sudo signalfx-agent status ```  === “Example Output”\n```text SignalFx Agent version: 5.3.0 Agent uptime: 2m7s Observers active: host Active Monitors: 9 Configured Monitors: 9 Discovered Endpoint Count: 6 Bad Monitor Config: None Global Dimensions: {host: zevn} GlobalSpanTags: map[] Datapoints sent (last minute): 237 Datapoints failed (last minute): 0 Datapoints overwritten (total): 0 Events Sent (last minute): 18 Trace Spans Sent (last minute): 0 Trace Spans overwritten (total): 0 Additional status commands: signalfx-agent status config - show resolved config in use by agent signalfx-agent status endpoints - show discovered endpoints signalfx-agent status monitors - show active monitors signalfx-agent status all - show everything ```  4.2 Check the Splunk UI Navigate to the Splunk UI and click on the Infrastructure tab. The click on Hosts (Smart Agent / collectd) under the Hosts section.\nFind your VM and confirm it is reporting in correctly; allow a few minutes for it to appear.\nIf it fails to appear after 3 mins, please let the Splunk Team know so they can help troubleshoot.\n Because you are using a Multipass VM instead of a Splunk Provided EC2 Instance, you will also need to complete the optional module “Create a Detector”. This will ensure you receive incident notifications for this VM.\n","categories":"","description":"","excerpt":"Creating a Test VM Using Multipass Aim The aim of this module is to …","ref":"/observability-workshop/v4.22/oncall/optional/multipass/","tags":"","title":""},{"body":"VictorOps Integrations - Lab Summary This module covers configuring the Integrations between SignalFx and VictorOps. Whilst the detailed steps below walk you through the process, you will find that the intergrations are already active within the Splunk systems being used for this workhop so Steps 1 \u0026 2 are for info only. You only need to complete Step 3. Copy ID\n1. VictorOps Service API Endpoint !!! warning The SignalFx Integration only needs to be enabled once per VictorOps instance, so you will probably find it has already been enabled, please DO NOT disable an already active integration when completing this lab.\nThis is for info only as the Integration has already been enabled\nIn order to integrate SignalFx with VictorOps we need to first obtain the Service API Endpoint for VictorOps. Within the VictorOps UI navigate to Integrations main tab and then use the search feature to find the SignalFx Integration.\nIf it is not already enabled, click the Enable Integration button to activate it.\nThis would be used when configuring the VictorOps Integration within the Splunk UI if it had not already been enabled.\n2. Enable VictorOps Integration within SignalFx In the Splunk UI navigate to Integrations and use the search feature to find the VictorOps integration.\n!!! danger “Do not create a new integration!” Please do not create additional VictorOps integrations if one already exists, it will not break anything but simply creates extra clean up work after the workshop has completed. The aim of this part of the lab was to show you how you would go about configuring the Integration if it was not already enabled.\nAssuming you are using the AppDev EMEA instance of VictorOps you will find the VictorOps Integration has already been configured so there is no need to create a new one.\nHowever the process of creating a new Integration is simply to click on Create New Integration like in the image below, or if there are existing integrations and you want to add another one you would click New Integration.\nEnter a descriptive Name then paste the Service_API_Endpoint value you copied in the previous step into the Post URL field, then save it.\n!!! important “Handling multiple VictorOps integrations” SignalFx can integrate with multiple VictorOps accounts so it is important when creating one to use a descriptive name and to not simply call it VictorOps. This name will be used within the Splunk UI when selecting this integration, so ensure it is unambiguous\n3. Copy ID In Splunk UI navigate to Integrations and use the search feature to find the VictorOps Integration.\nCopy the ID field and save it for use in the next steps. We suggest you create a notepad document or similar as you will be gathering some additional values in the next steps.\n","categories":"","description":"","excerpt":"VictorOps Integrations - Lab Summary This module covers configuring …","ref":"/observability-workshop/v4.22/oncall/optional/sfx_integrations/","tags":"","title":""},{"body":"Make sure to scroll down to see infra metrics for the app\n","categories":"","description":"","excerpt":"Make sure to scroll down to see infra metrics for the app\n","ref":"/observability-workshop/v4.22/otelw/dashboards/servicedashboard/","tags":"","title":""},{"body":"Log in to your Splunk Observability account to identify token/realm For individuals and groups- allow 30-45 minutes of prep time to identify account credentials and prepare a lab environment. When running as a group we recommend doing a separate prep meeting before running the workshop together.\nCheck your Splunk Observability Account (your welcome email has this link) and identify your TOKEN and REALM - these are available in the profile menu in your Splunk Observability account. Note that the realm component i.e. us1 may be different for your account based on how you signed up.\nHow to find realm:\nSplunk Observability Menu -\u003e Your Name -\u003e Account Settings\nHow to find token:\nCreate Lab Environment Splunk Observability is for server environments. This workshop uses Ubuntu Linux as the lab server environment. You can use any Ubuntu platform - bare metal, VM, or cloud VM.\nRecommended Environment For optimal learning we recommend that you use a fresh cloud VM running Ubuntu with minimum 12GB RAM and 20GB disk space.\nIf you chose your own Ubuntu machine, you can set it up with the Workshop software with this command:\nbash \u003c(curl -s https://raw.githubusercontent.com/signalfx/otelworkshop/master/setup-tools/ubuntu.sh) Local Environment If you cannot procure a cloud VM, you can create an Ubuntu Linux environment on a Mac or PC and install the necessary software components:\nMac OS Install Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Make sure Homebrew is fully upgraded:\nbrew upgrade Results should be at least 1.5:\nbrew --version We will use Multipass as a hypervisor for Mac:\nbrew cask install multipass If needed, further instructions are here. Do one final brew upgrade before spinning up VM:\nbrew upgrade Windows Follow Multipass Windows installation instructions\nLaunch Multipass Ubuntu VM Create your VM called “primary”:\nmultipass launch -n primary -d 20G -m 12G This will download Ubuntu and may take a few minutes the first time.\nBasic multipass commands:\n Shell into VM: multipass shell primary Exit VM: exit  To manage multipass VM:\n multipass stop primary stops the VM multipass delete primary deletes the VM from the hypervisor multipass purge purges created images but leaves the ubuntu template intace  Install OTel Workshop A bootstrap script will install everything needed and clone this repo.\nThis will take up to 10 minutes to execute- leave it running until complete.\nmultipass shell primary Once in your Multipass Ubuntu VM:\nbash \u003c(curl -s https://raw.githubusercontent.com/signalfx/otelworkshop/master/setup-tools/ubuntu.sh) Key OTel APM concepts Moving parts that make APM happen in OpenTelemetry:\n Application Spans: OpenTelemetry instrumentation causes spans to be emitted by your applications OpenTelmetry auto-instrumentation (no code changes) for most languages is availabile but you can use any framework/library that emits spans in formats accepted by the Otel Collector i.e zipkin, OpenTracing, or OpenTelemetry. The spans are received by the OpenTelemetry Collector which both doubles as an infrastructure metrics collection agent and a telemetry processor. The Collector then forwards all telemetry (metrics/traces/logs) to Splunk Observability Cloud. Instructructure metrics: Infrastructure metrics are collected by your OpenTelemetry Collector which is observing the application’s host or container cluster. The infrastructure agent is lightweight, open source, real-time, and designed for microservices, containers, and cloud as well as on premise servers or cloud virtual machines. Application spans will be sent to the OpenTelemetry Collector running on a host or k8s pod to correlate APM with host/cluster metrics. The Collector then relays the spans to Splunk Observability Cloud APM where they will be assembled into traces. The APM spans flow in real time and there is no sampling. Pre-made default Service Dashboards with application metrics for each app will appear once spans are received by Splunk APM. The APM view has directed troubleshooting. Environment variables control the setup of APM. These names vary based on instrumentation but they always include:\n- Endpoint: destination to send spans\n- Service name: the name of the application as you want it to appear in a service map\n- Environment: a value for segmenting betwen dev/prod etc. Can be set with instrumentation and not necessarily as part of an ENV variable.  ","categories":"","description":"","excerpt":"Log in to your Splunk Observability account to identify token/realm …","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_k8s/prep/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/bootcamp/docs/","tags":"","title":"Bootcamp Labs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/ja/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"How to create both Real Browser and API checks in Splunk Synthetics \n","excerpt":"How to create both Real Browser and API checks in Splunk Synthetics \n","ref":"/observability-workshop/v4.22/synthetics/docs/","tags":"","title":"Creating checks"},{"body":"","categories":"","description":"How to create both Real Browser and API checks in Splunk Synthetics \n","excerpt":"How to create both Real Browser and API checks in Splunk Synthetics \n","ref":"/observability-workshop/v4.22/ja/synthetics/docs/","tags":"","title":"Creating checks"},{"body":"Splunk APM Trace Generator Demo For AWS ECS EC2 This repo demonstrates a reference implemenation for a single AWS ECS EC2 task example of Splunk APM that will send spans directly to Splunk Observability Cloud.\nECS works very simply: just add the environment variables required by the Otel APM Instrumentation and the Instrumentation will do the rest.\nTo deploy this example, you must have an ECS environment ready to go with VPC, task roles for logs, etc. Instructions are below:\n  Install ECS CLI: ECS CLI Setup\n  Pay critical attention to setting up VPC in advance: Task Definition Guide\n  Set up log environment here: Cloudwatch\n   Setup Configure ECS CLI Profile:\necs-cli \\ configure profile \\ --access-key YOURAWSKEYHERE \\ --secret-key YOURAWSSECRETKEYHERE \\ --profile-name ecs-ec2-profile Configure ECS EC2 Cluster:\nconfigure ECS Cluster Config: ecs-cli configure \\ --cluster test-cluster \\ --config-name test-cluster \\ --region YOURREGIONHEREi.e.:us-east-1 Deploy ECS EC2 Cluster:\necs-cli up \\ --cluster test-cluster \\ --region YOURREGIONHEREi.e.:us-east-1\\ --size 1 \\ --capability-iam \\ --instance-type t2.xlarge \\ --launch-type EC2 \\ --ecs-profile test-profile \\ --force  Deploy Task Deploy with the following commands- you must change the variables in caps in these task .json files to suit your environment:\naws ecs register-task-definition \\ --cli-input-json file://tracegen-java-otel-ecs-ec2.json Note that the task definition will increment each time you try it- from 1 to 2 etc. To check which version is current use:\naws ecs list-task-definitions Deploy trace generator task to cluster:\naws ecs create-service \\ --cluster test-cluster \\ --launch-type EC2 \\ --scheduling-strategy DAEMON \\ --service-name tracegen-java-otel-ecs-ec2 \\ --task-definition tracegen-java-otel-ecs-ec2:VERSIONHEREi.e.1 After a few seconds check Splunk APM to see the trace generator service.\n Cleanup aws ecs delete-service --cluster test-cluster --service tracegen-java-otel-ecs-ec2 --force ecs-cli down \\ --cluster test-cluster \\ --region us-east-1 aws ecs delete-cluster --cluster test-cluster  Extras The ecs-cli-commands.md file offers helpful commands for ECS Fargate management for the AWS CLI.\n","categories":"","description":"","excerpt":"Splunk APM Trace Generator Demo For AWS ECS EC2 This repo demonstrates …","ref":"/observability-workshop/v4.22/otelw/apm_ecs_demos/index-ec2/","tags":"","title":"ECS-EC2"},{"body":"Splunk APM Trace Generator Demo For AWS ECS Fargate This repo demonstrates reference implemenations for a single AWS ECS Fargate task example of Splunk APM that will send spans to a sidecar OpenTelemetry container.\nECS works very simply: just add the environment variables required by the Otel APM Instrumentation and the Instrumentation will do the rest.\nTo deploy this example, you must have an ECS environment ready to go with VPC, task roles for logs, etc. Instructions are below:\n  Install ECS CLI: ECS CLI Setup\n  Pay critical attention to setting up VPC in advance: Task Definition Guide\n  Set up log environment here: Cloudwatch\n   Setup Configure ECS CLI Profile:\necs-cli \\ configure profile \\ --access-key YOURAWSKEYHERE \\ --secret-key YOURAWSSECRETKEYHERE \\ --profile-name ecs-ec2-profile Configure ECS Cluster:\nconfigure ECS Cluster Config: ecs-cli configure \\ --cluster test-cluster \\ --config-name test-cluster \\ --region YOURREGIONHEREi.e.:us-east-1 Create ECS Cluster:\naws ecs create-cluster --cluster-name test-cluster  Deploy Task The task spins up two ECS Fargate containers:\n splunk-otel-collector - sidecar to observe ECS host metrics and relay application traces to Splunk APM tracegen-fargate - generates traces using a manually instrumented Java app and sends them to the Otel Collector  Deploy with the following commands- you must change the variables in caps in tracegen-java-otel-fargate-otelcolfargate.json to suit your environment:\nCreate cluster\naws ecs create-cluster --cluster-name test-cluster Note that the task definition will increment each time you try it- from 1 to 2 etc. To check which version is current use:\naws ecs list-task-definitions Identify your Security Group and Subnet and change them in the deploy script below and then deploy trace generator / otelcollector task to cluster:\naws ecs create-service \\ --cluster test-cluster \\ --service-name tracegen-fargate \\ --desired-count 1 \\ --launch-type \"FARGATE\" \\ --network-configuration \"awsvpcConfiguration={subnets=[subnet-YOURSUBNETHERE],securityGroups=[sg-YOURSECURITYGROUPHERE],assignPublicIp=ENABLED}\" \\ --task-definition tracegen-java-otel-fargate-otelcol:1 After a few seconds check Splunk APM and Dashboards-\u003eECS-Fargate to see the trace generator service and Otel collector\n Cleanup aws ecs delete-service --cluster test-cluster --service tracegen-java-otel-fargate-otelcol --force  Extras The ecs-cli-commands.md file offers helpful commands for ECS Fargate management for the AWS CLI.\n","categories":"","description":"","excerpt":"Splunk APM Trace Generator Demo For AWS ECS Fargate This repo …","ref":"/observability-workshop/v4.22/otelw/apm_ecs_demos/index-fargate/","tags":"","title":"ECS-Fargate"},{"body":"","categories":"","description":"How to get traces into Observability Cloud and how to use Splunk APM\n","excerpt":"How to get traces into Observability Cloud and how to use Splunk APM\n","ref":"/observability-workshop/v4.22/apm/docs/","tags":"","title":"Get Traces In, Get Data Out"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/categories/imt/","tags":"","title":"IMT"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/ja/categories/imt/","tags":"","title":"IMT"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/otelw/labs/apm_for_k8s/examples/","tags":"","title":"Instrumentation Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/itsi/docs/","tags":"","title":"ITSI Content Pack Workshop"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/tags/k3s/","tags":"","title":"k3s"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/ja/tags/k3s/","tags":"","title":"k3s"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/tags/ngnix/","tags":"","title":"NGNIX"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/ja/tags/ngnix/","tags":"","title":"NGNIX"},{"body":"","categories":"","description":"Hands-On Exercises and Discussion\n","excerpt":"Hands-On Exercises and Discussion\n","ref":"/observability-workshop/v4.22/realworld/","tags":"","title":"O11y Real World Workshop"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/oncall/","tags":"","title":"Oncalls"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/otelw/labs/optional/","tags":"","title":"Optional/Advanced"},{"body":"Docker based APM examples  Prep: Make sure you’ve stopped your previous workshop examples and stopped all instances of Otel Collector as to not confuse re-used example names.\nDocker must be installed and current for this lab.\nRepo location: https://github.com/signalfx/otelworkshop/tree/main/misc/docker\nStart in k8s directory:\ncd ~/otelworkshop/misc/docker Create environment variables with your Splunk token and realm- substitute yours for the variables in caps:\nexport SPLUNK_ACCESS_TOKEN=YOURTOKENHERE export SPLUNK_REALM=YOURREALMHERE Add initals to environment i.e. sjl-apm-workshop:\nexport SPLUNK_WORKSHOP_ENV=YOURINITIALS-apm-workshop Make sure to re-export these environment variables every time you open a terminal.\n Example 1: Python Microservice w/ Local Otel Collector A local docker network with an OpenTelemetry Collector container and a container with a Python microservice example with a redis client and server in same container.\nStep 1: Create a local docker network called otel-net\nsource setup-docker.sh Step 2: Run Otel Collector docker container in the otel-net docker bridged network:\nsource run-otelcol.sh Step 3: Run the Python Redis client w/ Redis server microservice example container:\nOpen a new terminal window. Re-export your env variables from the prep section.\nsource run-python-autgen.sh Wait a about 60 seconds and check APM Explore map to see the microservices.\nStudy the run scripts to understand how OpenTelemetry environment variables are configured, and the source code for the microservice example is here\nctrl-c in each terminal will stop things and containers can be removed via standard Docker commands.\n Example 2: Python Microservice Sending Telmetry Directly to Splunk Observability Cloud Run the direct-to-ingest docker container:\nsource run-python-autogen-direct.sh Wait a about 60 seconds and check APM Explore map to see the microservices.\n Example 3: .NET Microservice Sending Telemetry Directly to Splunk Observability Cloud Run the direct-to-ingest docker container:\nsource run-dotnet-autogen-direct.sh Wait a about 60 seconds and check APM Explore map to see the microservices.\nMisc Docker container instructions for OpenTelemetry Collector are here\nView Otel Collector trace stats (requires Lynx ascii browser):\ndocker exec -it otelcol curl localhost:55679/debug/tracez | lynx -stdin ","categories":"","description":"","excerpt":"Docker based APM examples  Prep: Make sure you’ve stopped your …","ref":"/observability-workshop/v4.22/otelw/labs/optional/docker/","tags":"","title":"OTel Collector and APM for Docker"},{"body":"Overview of the RUM Workshop The aim of this Splunk Real User Monitoring (RUM) workshop is to:\n  See how to add RUM to your website.\n  Examine the performance of your website with RUM metrics.\n  Investigate issues with your website.\n  In order to reach this goal, we will use an online boutique to order various products. Whilst shopping on the online boutique you may encounter some issues with this web site, and you will use Splunk RUM to identify the issues, so they can be resolved by the developers.\nThe workshop host will provide you with the URL for an online boutique store that has RUM enabled. This will allow us to generate live data to be analysed later.\nIf you are running this session as part of the IMT/APM workshop you will be able to compare your current online boutique store which is not RUM enabled to the RUM enabled URL that your workshop host will provide. If this an standalone RUM workshop, you will be provided with two URL, one for a shop that no RUM, and one that is RUM enabled.\n","categories":"","description":"","excerpt":"Overview of the RUM Workshop The aim of this Splunk Real User …","ref":"/observability-workshop/v4.22/rum/overview/","tags":"","title":"Overview"},{"body":"","categories":"","description":"Environment Configuration and Hands-On Exercises\n","excerpt":"Environment Configuration and Hands-On Exercises\n","ref":"/observability-workshop/v4.22/pet-clinic/","tags":"","title":"Pet Clinic Java Workshop"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/rum/docs/","tags":"","title":"RUM Workshop"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/search/","tags":"","title":"Search Results"},{"body":"  #td-cover-block-0 { background-image: url(/observability-workshop/v4.22/background_hucef8ecfb8c929753b37f58ee610ed71c_306261_960x540_fill_q75_catmullrom_top.jpeg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/observability-workshop/v4.22/background_hucef8ecfb8c929753b37f58ee610ed71c_306261_1920x1080_fill_q75_catmullrom_top.jpeg); } }  Welcome to the Splunk Observability Workshops! Get Started    Download   -- Get insights into your applications and infrastructure in real-time with the help of the monitoring, analytics and response tools of the Splunk Observability Cloud         This workshop is going to take you through the best-in-class observability platform for ingesting, monitoring, visualizing and analyzing metrics, traces and spans.       OpenTelemetry OpenTelemetry is used in this workshop to instrument, generate, collect and export telemetry data (metrics, traces and logs) to help you analyze your application and infrastructure.\n Read more …\n   Contributions welcome! You can contribute to this documentation via issues and pull requests. Please don’t hesitate to help to make the workshops better.\n Read more …\n   Follow us on Twitter! You can find information about updates and interesting reads in the Twitter channel of Splunk.\n Read more …\n    ","categories":"","description":"Learn how to build observability solutions with Splunk","excerpt":"Learn how to build observability solutions with Splunk","ref":"/observability-workshop/v4.22/","tags":"","title":"Splunk"},{"body":"  #td-cover-block-0 { background-image: url(/observability-workshop/v4.22/ja/background_hucef8ecfb8c929753b37f58ee610ed71c_306261_960x540_fill_q75_catmullrom_top.jpeg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/observability-workshop/v4.22/ja/background_hucef8ecfb8c929753b37f58ee610ed71c_306261_1920x1080_fill_q75_catmullrom_top.jpeg); } }  Splunk Observability ワークショップへようこそ！ 始める    Download   -- Splunk Observability Cloud の監視、分析、応答ツールを使用して、アプリケーションとインフラストラクチャをリアルタイムで把握することができます。         このワークショップでは、メトリクス、トレース、スパンを取り込み、監視し、可視化し、分析するためのクラス最高のオブザーバビリティプラットフォームをご紹介します。       OpenTelemetry このワークショップでは、OpenTelemetryを使用して、アプリケーションやインフラの分析に役立つテレメトリデータ（メトリクス、トレース、ログ）の計測、生成、収集、エクスポートを行います。\n 続きを読む …\n   Contributions welcome! このドキュメントには、issue や pull request で貢献することができます。より良いワークショップにするために、遠慮なくご協力ください。\n 続きを読む …\n   Follow us on Twitter! SplunkのTwitterチャンネルでは、アップデート情報や様々な読み物が紹介されています。\n 続きを読む …\n    ","categories":"","description":"Splunkのオブザーバビリティソリューションの構築方法を学びましょう","excerpt":"Splunkのオブザーバビリティソリューションの構築方法を学びましょう","ref":"/observability-workshop/v4.22/ja/","tags":"","title":"Splunk"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/observability-workshop/v4.22/ja/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"Observability Cloudへのトレース取り込み方法とSplunk APMの活用方法\n","excerpt":"Observability Cloudへのトレース取り込み方法とSplunk APMの活用方法\n","ref":"/observability-workshop/v4.22/ja/apm/docs/","tags":"","title":"トレースを取り込み、データを取り出す"}]